{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceType":"datasetVersion","sourceId":14941831,"datasetId":9562283,"databundleVersionId":15810992},{"sourceType":"datasetVersion","sourceId":14796712,"datasetId":9460614,"databundleVersionId":15651343}],"dockerImageVersionId":31287,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport torch\nfrom transformers import CLIPProcessor, CLIPModel, AutoTokenizer, AutoModel\nimport easyocr\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import Input, Dense, Dropout, Concatenate, BatchNormalization\nfrom tensorflow.keras.models import Model\nfrom sklearn.model_selection import train_test_split","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n\n\nCSV_PATH = \"/kaggle/input/datasets/akra1234/meme-csv/train_malayalam_ocr (7).csv\"\n\ndf1 = pd.read_csv(CSV_PATH)\ndf2=pd.read_excel('/kaggle/input/datasets/akra1234/sth-dravidian/STH_Darvidian_Datasets/mala/Malayalam_Train_label (1).xlsx')\ndf=pd.merge(df1,df2, on='meme_id', how='inner')\nprint(df.shape)\ndf.head()\n\nprint(df.shape)\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-24T06:37:32.140363Z","iopub.execute_input":"2026-02-24T06:37:32.140686Z","iopub.status.idle":"2026-02-24T06:37:32.241175Z","shell.execute_reply.started":"2026-02-24T06:37:32.140663Z","shell.execute_reply":"2026-02-24T06:37:32.240493Z"}},"outputs":[{"name":"stdout","text":"(500, 4)\n(500, 4)\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"   meme_id                                      ml_text_clean        Level 1  \\\n0        1  1 . ടം ആജ്‌\\nര്‍ പ പ ദ്ദ; ടി ഗ്യ ഷ്‌ കാടോ ലാ\\n...  TROLL/ OPPOSE   \n1       10  റും ര.\\n7 ല്‍ റ ന പ\\n൫9൭൭2 ന മി\\nടം ന്തര ഞന\\n1...  TROLL/ OPPOSE   \n2      101  വി\\n\\n4 ിജിിര്തിട\\n\\n(95 0790൫5 രി:\\n\\n1) [രിത...  TROLL/ OPPOSE   \n3      102  ടട (ധ്യ 7 ഉ ണ്ണ ു\\nടം; റ്റി നിം യ; ചി റി ൧\\nപം...  TROLL/ OPPOSE   \n4      104  | ആആ്ഞ്ഞ്ഞ്ഞ്തംത്ത്്തഴ് [|\\n)൭(൫70പയല്നുന്ന്ര!...  TROLL/ OPPOSE   \n\n                     Level 2  \n0               Intersection  \n1  Against individual person  \n2              Against party  \n3  Against individual person  \n4  Against individual person  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>meme_id</th>\n      <th>ml_text_clean</th>\n      <th>Level 1</th>\n      <th>Level 2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1 . ടം ആജ്‌\\nര്‍ പ പ ദ്ദ; ടി ഗ്യ ഷ്‌ കാടോ ലാ\\n...</td>\n      <td>TROLL/ OPPOSE</td>\n      <td>Intersection</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>10</td>\n      <td>റും ര.\\n7 ല്‍ റ ന പ\\n൫9൭൭2 ന മി\\nടം ന്തര ഞന\\n1...</td>\n      <td>TROLL/ OPPOSE</td>\n      <td>Against individual person</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>101</td>\n      <td>വി\\n\\n4 ിജിിര്തിട\\n\\n(95 0790൫5 രി:\\n\\n1) [രിത...</td>\n      <td>TROLL/ OPPOSE</td>\n      <td>Against party</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>102</td>\n      <td>ടട (ധ്യ 7 ഉ ണ്ണ ു\\nടം; റ്റി നിം യ; ചി റി ൧\\nപം...</td>\n      <td>TROLL/ OPPOSE</td>\n      <td>Against individual person</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>104</td>\n      <td>| ആആ്ഞ്ഞ്ഞ്ഞ്തംത്ത്്തഴ് [|\\n)൭(൫70പയല്നുന്ന്ര!...</td>\n      <td>TROLL/ OPPOSE</td>\n      <td>Against individual person</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"def Level1_MAP(label):\n    \n    if \"TROLL/ OPPOSE\" in label:\n        return 1\n    elif \"SUPPORT\" in label:\n        return 0\n\n    else :\n        return 'NaN'\n\n\ndef map_level2(label):\n    \n\n    if \"person\" in label:\n        return 0\n    elif \"party\" in label:\n        return 1\n    elif \"Intersection\" in label:\n        return 2\n    else:\n        return -1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-24T06:37:54.578662Z","iopub.execute_input":"2026-02-24T06:37:54.579387Z","iopub.status.idle":"2026-02-24T06:37:54.583606Z","shell.execute_reply.started":"2026-02-24T06:37:54.579354Z","shell.execute_reply":"2026-02-24T06:37:54.582880Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"df['label1']=df['Level 1'].map(Level1_MAP)\ndf['label2']=df[\"Level 2\"].apply(map_level2)\n\nprint(\"Train examples:\", len(df))\n\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-24T06:38:11.854739Z","iopub.execute_input":"2026-02-24T06:38:11.855114Z","iopub.status.idle":"2026-02-24T06:38:11.868086Z","shell.execute_reply.started":"2026-02-24T06:38:11.855085Z","shell.execute_reply":"2026-02-24T06:38:11.867323Z"}},"outputs":[{"name":"stdout","text":"Train examples: 500\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"   meme_id                                      ml_text_clean        Level 1  \\\n0        1  1 . ടം ആജ്‌\\nര്‍ പ പ ദ്ദ; ടി ഗ്യ ഷ്‌ കാടോ ലാ\\n...  TROLL/ OPPOSE   \n1       10  റും ര.\\n7 ല്‍ റ ന പ\\n൫9൭൭2 ന മി\\nടം ന്തര ഞന\\n1...  TROLL/ OPPOSE   \n2      101  വി\\n\\n4 ിജിിര്തിട\\n\\n(95 0790൫5 രി:\\n\\n1) [രിത...  TROLL/ OPPOSE   \n3      102  ടട (ധ്യ 7 ഉ ണ്ണ ു\\nടം; റ്റി നിം യ; ചി റി ൧\\nപം...  TROLL/ OPPOSE   \n4      104  | ആആ്ഞ്ഞ്ഞ്ഞ്തംത്ത്്തഴ് [|\\n)൭(൫70പയല്നുന്ന്ര!...  TROLL/ OPPOSE   \n\n                     Level 2  label1  label2  \n0               Intersection       1       2  \n1  Against individual person       1       0  \n2              Against party       1       1  \n3  Against individual person       1       0  \n4  Against individual person       1       0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>meme_id</th>\n      <th>ml_text_clean</th>\n      <th>Level 1</th>\n      <th>Level 2</th>\n      <th>label1</th>\n      <th>label2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1 . ടം ആജ്‌\\nര്‍ പ പ ദ്ദ; ടി ഗ്യ ഷ്‌ കാടോ ലാ\\n...</td>\n      <td>TROLL/ OPPOSE</td>\n      <td>Intersection</td>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>10</td>\n      <td>റും ര.\\n7 ല്‍ റ ന പ\\n൫9൭൭2 ന മി\\nടം ന്തര ഞന\\n1...</td>\n      <td>TROLL/ OPPOSE</td>\n      <td>Against individual person</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>101</td>\n      <td>വി\\n\\n4 ിജിിര്തിട\\n\\n(95 0790൫5 രി:\\n\\n1) [രിത...</td>\n      <td>TROLL/ OPPOSE</td>\n      <td>Against party</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>102</td>\n      <td>ടട (ധ്യ 7 ഉ ണ്ണ ു\\nടം; റ്റി നിം യ; ചി റി ൧\\nപം...</td>\n      <td>TROLL/ OPPOSE</td>\n      <td>Against individual person</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>104</td>\n      <td>| ആആ്ഞ്ഞ്ഞ്ഞ്തംത്ത്്തഴ് [|\\n)൭(൫70പയല്നുന്ന്ര!...</td>\n      <td>TROLL/ OPPOSE</td>\n      <td>Against individual person</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"df['Level 1'].value_counts()\ntest_df1=pd.read_csv('/kaggle/input/datasets/akra1234/meme-csv/test_malayalam_ocr (1) (3).csv')\ntest_df2=pd.read_excel('/kaggle/input/datasets/akra1234/sth-dravidian/STH_Darvidian_Datasets/mala/Malayalam_Test_label.xlsx')\ntest_df=pd.merge(test_df1, test_df2, on='meme_id', how='inner')\ntest_df.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-24T06:39:26.945248Z","iopub.execute_input":"2026-02-24T06:39:26.946017Z","iopub.status.idle":"2026-02-24T06:39:27.003530Z","shell.execute_reply.started":"2026-02-24T06:39:26.945985Z","shell.execute_reply":"2026-02-24T06:39:27.003015Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"   meme_id                                      ml_text_clean  Level 1  \\\n0      100  1111001000\\nകില്‍ ത്ത്‌ ഗള്‍ കം ക ്ന\\nറ ഥി വ 4...      NaN   \n1      103  ) ' 55 ഹ്ന... ടാം പി\\nടാ ടം ത്തത്‌\\n; ക ത ന ക ...      NaN   \n2      106  1.\\nവ ്ു വ ി . & യ ക\\nലി സ്സ [ പ്ര ി ബ്ല യ നടന...      NaN   \n3      107  ന 2011)0200)0/7/11001/01\\nയിട്ടു ദിനം.\\nയി ാ\\n...      NaN   \n4      122  യല്ല\\nലം ഴു്‌ ലാ ്്‌ ||). [|\\n1; റിക ഷ്‌ മി 14...      NaN   \n\n   Level 2  \n0      NaN  \n1      NaN  \n2      NaN  \n3      NaN  \n4      NaN  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>meme_id</th>\n      <th>ml_text_clean</th>\n      <th>Level 1</th>\n      <th>Level 2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>100</td>\n      <td>1111001000\\nകില്‍ ത്ത്‌ ഗള്‍ കം ക ്ന\\nറ ഥി വ 4...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>103</td>\n      <td>) ' 55 ഹ്ന... ടാം പി\\nടാ ടം ത്തത്‌\\n; ക ത ന ക ...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>106</td>\n      <td>1.\\nവ ്ു വ ി . &amp; യ ക\\nലി സ്സ [ പ്ര ി ബ്ല യ നടന...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>107</td>\n      <td>ന 2011)0200)0/7/11001/01\\nയിട്ടു ദിനം.\\nയി ാ\\n...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>122</td>\n      <td>യല്ല\\nലം ഴു്‌ ലാ ്്‌ ||). [|\\n1; റിക ഷ്‌ മി 14...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"import cv2\nimport os\nimport pandas as pd\n\ndef load_images_to_dataframe(image_dir):\n\n    data = []\n\n\n    for file_name in os.listdir(image_dir):\n        file_path = os.path.join(image_dir, file_name)\n        \n\n        if file_name.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff')):\n\n            img = cv2.imread(file_path)\n            \n\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n            \n\n            img = cv2.resize(img, (224, 224), interpolation=cv2.INTER_AREA)\n            \n\n            alpha = 1.2 \n            beta = 20   \n            img = cv2.convertScaleAbs(img, alpha=alpha, beta=beta)\n\n\n            image_id = int(os.path.splitext(file_name)[0])\n            \n            # Append image data and file name to the list\n            data.append({\n                'meme_id': image_id,\n                'Image_name': img\n            })\n\n\n    df = pd.DataFrame(data)\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-24T06:40:04.694931Z","iopub.execute_input":"2026-02-24T06:40:04.695608Z","iopub.status.idle":"2026-02-24T06:40:04.701307Z","shell.execute_reply.started":"2026-02-24T06:40:04.695579Z","shell.execute_reply":"2026-02-24T06:40:04.700570Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"df_test= load_images_to_dataframe('/kaggle/input/datasets/akra1234/sth-dravidian/STH_Darvidian_Datasets/mala/Test_images-20260210T131747Z-1-001/Test_images')\ndf_train = load_images_to_dataframe('/kaggle/input/datasets/akra1234/sth-dravidian/STH_Darvidian_Datasets/mala/Train_images-20260210T132007Z-1-001/Train_images')\ntrain = pd.merge(df, df_train, on='meme_id')\ntest = pd.merge(test_df, df_test, on='meme_id')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-24T06:41:35.038952Z","iopub.execute_input":"2026-02-24T06:41:35.039628Z","iopub.status.idle":"2026-02-24T06:41:48.183971Z","shell.execute_reply.started":"2026-02-24T06:41:35.039599Z","shell.execute_reply":"2026-02-24T06:41:48.183338Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-24T06:41:51.913729Z","iopub.execute_input":"2026-02-24T06:41:51.914029Z","iopub.status.idle":"2026-02-24T06:41:58.450502Z","shell.execute_reply.started":"2026-02-24T06:41:51.914003Z","shell.execute_reply":"2026-02-24T06:41:58.449894Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"   meme_id                                      ml_text_clean        Level 1  \\\n0        1  1 . ടം ആജ്‌\\nര്‍ പ പ ദ്ദ; ടി ഗ്യ ഷ്‌ കാടോ ലാ\\n...  TROLL/ OPPOSE   \n1       10  റും ര.\\n7 ല്‍ റ ന പ\\n൫9൭൭2 ന മി\\nടം ന്തര ഞന\\n1...  TROLL/ OPPOSE   \n2      101  വി\\n\\n4 ിജിിര്തിട\\n\\n(95 0790൫5 രി:\\n\\n1) [രിത...  TROLL/ OPPOSE   \n3      102  ടട (ധ്യ 7 ഉ ണ്ണ ു\\nടം; റ്റി നിം യ; ചി റി ൧\\nപം...  TROLL/ OPPOSE   \n4      104  | ആആ്ഞ്ഞ്ഞ്ഞ്തംത്ത്്തഴ് [|\\n)൭(൫70പയല്നുന്ന്ര!...  TROLL/ OPPOSE   \n\n                     Level 2  label1  label2  \\\n0               Intersection       1       2   \n1  Against individual person       1       0   \n2              Against party       1       1   \n3  Against individual person       1       0   \n4  Against individual person       1       0   \n\n                                          Image_name  \n0  [[[20, 20, 20], [20, 20, 20], [20, 20, 20], [2...  \n1  [[[52, 57, 49], [56, 60, 51], [62, 64, 57], [7...  \n2  [[[28, 28, 28], [28, 28, 28], [28, 28, 28], [2...  \n3  [[[20, 20, 20], [20, 20, 20], [20, 20, 20], [2...  \n4  [[[55, 55, 55], [55, 55, 55], [55, 55, 55], [5...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>meme_id</th>\n      <th>ml_text_clean</th>\n      <th>Level 1</th>\n      <th>Level 2</th>\n      <th>label1</th>\n      <th>label2</th>\n      <th>Image_name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1 . ടം ആജ്‌\\nര്‍ പ പ ദ്ദ; ടി ഗ്യ ഷ്‌ കാടോ ലാ\\n...</td>\n      <td>TROLL/ OPPOSE</td>\n      <td>Intersection</td>\n      <td>1</td>\n      <td>2</td>\n      <td>[[[20, 20, 20], [20, 20, 20], [20, 20, 20], [2...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>10</td>\n      <td>റും ര.\\n7 ല്‍ റ ന പ\\n൫9൭൭2 ന മി\\nടം ന്തര ഞന\\n1...</td>\n      <td>TROLL/ OPPOSE</td>\n      <td>Against individual person</td>\n      <td>1</td>\n      <td>0</td>\n      <td>[[[52, 57, 49], [56, 60, 51], [62, 64, 57], [7...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>101</td>\n      <td>വി\\n\\n4 ിജിിര്തിട\\n\\n(95 0790൫5 രി:\\n\\n1) [രിത...</td>\n      <td>TROLL/ OPPOSE</td>\n      <td>Against party</td>\n      <td>1</td>\n      <td>1</td>\n      <td>[[[28, 28, 28], [28, 28, 28], [28, 28, 28], [2...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>102</td>\n      <td>ടട (ധ്യ 7 ഉ ണ്ണ ു\\nടം; റ്റി നിം യ; ചി റി ൧\\nപം...</td>\n      <td>TROLL/ OPPOSE</td>\n      <td>Against individual person</td>\n      <td>1</td>\n      <td>0</td>\n      <td>[[[20, 20, 20], [20, 20, 20], [20, 20, 20], [2...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>104</td>\n      <td>| ആആ്ഞ്ഞ്ഞ്ഞ്തംത്ത്്തഴ് [|\\n)൭(൫70പയല്നുന്ന്ര!...</td>\n      <td>TROLL/ OPPOSE</td>\n      <td>Against individual person</td>\n      <td>1</td>\n      <td>0</td>\n      <td>[[[55, 55, 55], [55, 55, 55], [55, 55, 55], [5...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"!pip install git+https://github.com/indic-transliteration/indic_transliteration_py/@master","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-24T06:42:31.211565Z","iopub.execute_input":"2026-02-24T06:42:31.212193Z","iopub.status.idle":"2026-02-24T06:42:39.942485Z","shell.execute_reply.started":"2026-02-24T06:42:31.212152Z","shell.execute_reply":"2026-02-24T06:42:39.941535Z"}},"outputs":[{"name":"stdout","text":"Collecting git+https://github.com/indic-transliteration/indic_transliteration_py/@master\n  Cloning https://github.com/indic-transliteration/indic_transliteration_py/ (to revision master) to /tmp/pip-req-build-ncjvavr8\n  Running command git clone --filter=blob:none --quiet https://github.com/indic-transliteration/indic_transliteration_py/ /tmp/pip-req-build-ncjvavr8\n  Resolved https://github.com/indic-transliteration/indic_transliteration_py/ to commit 322e88533190a874743d72853e3991cbd0e1c076\n  Running command git submodule update --init --recursive -q\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting backports.functools_lru_cache (from indic_transliteration==2.3.79)\n  Downloading backports.functools_lru_cache-2.0.0-py2.py3-none-any.whl.metadata (3.5 kB)\nRequirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from indic_transliteration==2.3.79) (2025.11.3)\nRequirement already satisfied: typer in /usr/local/lib/python3.12/dist-packages (from indic_transliteration==2.3.79) (0.21.1)\nRequirement already satisfied: toml in /usr/local/lib/python3.12/dist-packages (from indic_transliteration==2.3.79) (0.10.2)\nCollecting roman (from indic_transliteration==2.3.79)\n  Downloading roman-5.2-py3-none-any.whl.metadata (4.3 kB)\nRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer->indic_transliteration==2.3.79) (8.3.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from typer->indic_transliteration==2.3.79) (4.15.0)\nRequirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer->indic_transliteration==2.3.79) (1.5.4)\nRequirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer->indic_transliteration==2.3.79) (13.9.4)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer->indic_transliteration==2.3.79) (4.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer->indic_transliteration==2.3.79) (2.19.2)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer->indic_transliteration==2.3.79) (0.1.2)\nDownloading backports.functools_lru_cache-2.0.0-py2.py3-none-any.whl (6.7 kB)\nDownloading roman-5.2-py3-none-any.whl (6.0 kB)\nBuilding wheels for collected packages: indic_transliteration\n  Building wheel for indic_transliteration (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for indic_transliteration: filename=indic_transliteration-2.3.79-py3-none-any.whl size=164285 sha256=cfa56f54d06b5837bb4d09f8350da4063f30fbc542cda568dd392a20481df1ef\n  Stored in directory: /tmp/pip-ephem-wheel-cache-awb5is9b/wheels/ef/dd/4d/c621f9f27e4ec9e07c73a5614faa96a12a4cc1e94bf05f069b\nSuccessfully built indic_transliteration\nInstalling collected packages: roman, backports.functools_lru_cache, indic_transliteration\nSuccessfully installed backports.functools_lru_cache-2.0.0 indic_transliteration-2.3.79 roman-5.2\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"def text_preprocessing(text):\n    import re\n    import string\n    from indic_transliteration import sanscript\n    from indic_transliteration.sanscript import transliterate\n\n    # Remove mentions, hashtags, slashes\n    pattern = re.compile(r'[@#/]\\S+')\n    text = pattern.sub('', text)\n\n    # Remove digits\n    text = re.sub(r'\\d+', '', text)\n\n    # Remove URLs\n    text = re.sub(r'https?://\\S+|www\\.\\S+|ftp://\\S+|mailto:\\S+', '', text)\n\n    # Remove newline and carriage returns\n    text = text.replace('\\n', ' ').replace('\\r', '')\n\n    # Remove extra spaces\n    text = re.sub(r'\\s+', ' ', text).strip()\n\n    # Remove punctuation\n    text = text.translate(str.maketrans('', '', string.punctuation))\n\n    # Remove emojis\n    emoji_pattern = re.compile(\n        r\"[\"\n        r\"\\U0001F600-\\U0001F64F\"\n        r\"\\U0001F300-\\U0001F5FF\"\n        r\"\\U0001F680-\\U0001F6FF\"\n        r\"\\U0001F1E0-\\U0001F1FF\"\n        r\"\\U00002700-\\U000027BF\"\n        r\"\\U0001F900-\\U0001F9FF\"\n        r\"\\U00002600-\\U000026FF\"\n        r\"\\U00002B50-\\U00002B55\"\n        r\"]+\",\n        flags=re.UNICODE\n    )\n\n    text = emoji_pattern.sub('', text)\n\n    # Malayalam stopwords\n    ml_stopwords = [\n        \"ഒരു\",\"എന്ന്\",\"മറ്റും\",\"ഈ\",\"ഇത്\",\"എന്ന\",\"കൊണ്ട്\",\"എന്നത്\",\"പല\",\"ആണ്\",\n        \"അല്ലെങ്കിൽ\",\"അവൻ\",\"ഞാൻ\",\"ഉള്ള\",\"ആ\",\"ഇവൻ\",\"എന്നാൽ\",\"ആദ്യം\",\"എന്ത്\",\"നിന്ന്\",\n        \"ചില\",\"എന്റെ\",\"പോലെ\",\"വേണ്ടി\",\"വന്ന്\",\"ഇതിന്റെ\",\"അത്\",\"അവൾ\",\"തന്നെ\",\"പലരും\",\n        \"എന്നും\",\"കൂടാതെ\",\"ശേഷം\",\"കொண்ட\",\"ഇരിക്കും\",\"തന്റെ\",\"ഉണ്ട്\",\"സമയം\",\"എപ്പോഴും\",\n        \"അതിന്റെ\",\"തൻ\",\"പിന്നീട്\",\"അവർ\",\"വരെ\",\"നീ\",\"ആയ\",\"ഇരുന്നു\",\"ഉണ്ടായിരുന്നു\",\n        \"വന്ന\",\"ഇരുന്ന\",\"വളരെ\",\"ഇവിടെ\",\"മേൽ\",\"ഇവ\",\"കുറിച്ച്\",\"വരും\",\n        \"മറ്റൊരു\",\"ഇരു\",\"ഇതിൽ\",\"ഇപ്പോൾ\",\"അവന്റെ\",\"മാത്രം\",\"എന്നുള്ള\",\"മുകളിൽ\",\n        \"ചേർന്ന\",\"എനിക്ക്\",\"ഇനിയും\",\"ആ ദിവസം\",\"ഒരേ\",\"വളരെയേറെ\",\"അവിടെ\",\n        \"പലവിധ\",\"വിട്ട്\",\"വലിയ\",\"അதை\",\"കുറിച്ചുള്ള\",\"നിന്റെ\",\"കൂടുതൽ\",\"പേര്\",\"ഇതിനാൽ\",\n        \"അവ\",\"അതേ\",\"എന്തുകൊണ്ട്\",\"രീതി\",\"ആർ\",\"എന്നതിനെ\",\"എല്ലാം\",\"മാത്രമേ\",\n        \"സ്ഥലം\",\"സ്ഥലത്ത്\",\"അതിൽ\",\"നാം\",\"അതിനു\",\"അതുകൊണ്ട്\",\"മറ്റു\",\"ചെറിയ\",\"വിട്ടു\",\"ഏത്\",\n        \"എന്നുവെച്ച്\",\"എന്നറിയപ്പെടുന്ന\",\"എങ്കിലും\",\"അടുത്ത\",\"ഇതിനെ\",\"എടുക്കാൻ\",\"ഇതിന്\",\n        \"ഒഴികെ\",\"പോലും\",\"കുറച്ച്\"\n    ]\n\n    # Remove stopwords\n    words = text.split()\n    filtered_words = [word for word in words if word not in ml_stopwords]\n    text = \" \".join(filtered_words)\n\n    # Transliteration\n    text = transliterate(text, sanscript.HK, sanscript.TAMIL)\n\n    return text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-24T06:45:04.086708Z","iopub.execute_input":"2026-02-24T06:45:04.087502Z","iopub.status.idle":"2026-02-24T06:45:04.099326Z","shell.execute_reply.started":"2026-02-24T06:45:04.087464Z","shell.execute_reply":"2026-02-24T06:45:04.098538Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"train=train.rename(columns={'ml_text_clean': 'text'})\ntest= test.rename(columns={'ml_text_clean' : 'text'})\ntrain['text'] = train['text'].fillna('').apply(text_preprocessing)\ntest['text'] = test['text'].fillna('').apply(text_preprocessing)\ntrain['text'] = train['text'].apply(text_preprocessing)\ntest['text'] = test['text'].apply(text_preprocessing)\ntrain.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-24T06:45:55.450287Z","iopub.execute_input":"2026-02-24T06:45:55.450919Z","iopub.status.idle":"2026-02-24T06:46:03.686776Z","shell.execute_reply.started":"2026-02-24T06:45:55.450885Z","shell.execute_reply":"2026-02-24T06:46:03.685870Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"   meme_id                                               text        Level 1  \\\n0        1  ടം ആജ്‌ ര്‍ പ പ ദ്ദ ടി ഗ്യ ഷ്‌ കാടോ ലാ പ പ ടം ...  TROLL/ OPPOSE   \n1       10  റും ര ല്‍ റ ന പ ന മി ടം ന്തര ഞന ്‌ പര്‍ ല്‌ ലി...  TROLL/ OPPOSE   \n2      101  വി ിജിിര്തിട രി രിത്തുതിള്തി സ്‌ ി കസ്യിലാണ്‌ ...  TROLL/ OPPOSE   \n3      102  ടട ധ്യ ഉ ണ്ണ ു ടം റ്റി നിം യ ചി റി പം പു ര്‍ ല...  TROLL/ OPPOSE   \n4      104  ആആ്ഞ്ഞ്ഞ്ഞ്തംത്ത്്തഴ് പയല്നുന്ന്ര ത്ഞ്ത്ൃതന്ത്...  TROLL/ OPPOSE   \n\n                     Level 2  label1  label2  \\\n0               Intersection       1       2   \n1  Against individual person       1       0   \n2              Against party       1       1   \n3  Against individual person       1       0   \n4  Against individual person       1       0   \n\n                                          Image_name  \n0  [[[20, 20, 20], [20, 20, 20], [20, 20, 20], [2...  \n1  [[[52, 57, 49], [56, 60, 51], [62, 64, 57], [7...  \n2  [[[28, 28, 28], [28, 28, 28], [28, 28, 28], [2...  \n3  [[[20, 20, 20], [20, 20, 20], [20, 20, 20], [2...  \n4  [[[55, 55, 55], [55, 55, 55], [55, 55, 55], [5...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>meme_id</th>\n      <th>text</th>\n      <th>Level 1</th>\n      <th>Level 2</th>\n      <th>label1</th>\n      <th>label2</th>\n      <th>Image_name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>ടം ആജ്‌ ര്‍ പ പ ദ്ദ ടി ഗ്യ ഷ്‌ കാടോ ലാ പ പ ടം ...</td>\n      <td>TROLL/ OPPOSE</td>\n      <td>Intersection</td>\n      <td>1</td>\n      <td>2</td>\n      <td>[[[20, 20, 20], [20, 20, 20], [20, 20, 20], [2...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>10</td>\n      <td>റും ര ല്‍ റ ന പ ന മി ടം ന്തര ഞന ്‌ പര്‍ ല്‌ ലി...</td>\n      <td>TROLL/ OPPOSE</td>\n      <td>Against individual person</td>\n      <td>1</td>\n      <td>0</td>\n      <td>[[[52, 57, 49], [56, 60, 51], [62, 64, 57], [7...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>101</td>\n      <td>വി ിജിിര്തിട രി രിത്തുതിള്തി സ്‌ ി കസ്യിലാണ്‌ ...</td>\n      <td>TROLL/ OPPOSE</td>\n      <td>Against party</td>\n      <td>1</td>\n      <td>1</td>\n      <td>[[[28, 28, 28], [28, 28, 28], [28, 28, 28], [2...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>102</td>\n      <td>ടട ധ്യ ഉ ണ്ണ ു ടം റ്റി നിം യ ചി റി പം പു ര്‍ ല...</td>\n      <td>TROLL/ OPPOSE</td>\n      <td>Against individual person</td>\n      <td>1</td>\n      <td>0</td>\n      <td>[[[20, 20, 20], [20, 20, 20], [20, 20, 20], [2...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>104</td>\n      <td>ആആ്ഞ്ഞ്ഞ്ഞ്തംത്ത്്തഴ് പയല്നുന്ന്ര ത്ഞ്ത്ൃതന്ത്...</td>\n      <td>TROLL/ OPPOSE</td>\n      <td>Against individual person</td>\n      <td>1</td>\n      <td>0</td>\n      <td>[[[55, 55, 55], [55, 55, 55], [55, 55, 55], [5...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"!pip install deep-translator","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-24T06:46:39.303531Z","iopub.execute_input":"2026-02-24T06:46:39.304301Z","iopub.status.idle":"2026-02-24T06:46:42.713712Z","shell.execute_reply.started":"2026-02-24T06:46:39.304269Z","shell.execute_reply":"2026-02-24T06:46:42.713069Z"}},"outputs":[{"name":"stdout","text":"Collecting deep-translator\n  Downloading deep_translator-1.11.4-py3-none-any.whl.metadata (30 kB)\nRequirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /usr/local/lib/python3.12/dist-packages (from deep-translator) (4.13.5)\nRequirement already satisfied: requests<3.0.0,>=2.23.0 in /usr/local/lib/python3.12/dist-packages (from deep-translator) (2.32.4)\nRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (2.8.1)\nRequirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (4.15.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2026.1.4)\nDownloading deep_translator-1.11.4-py3-none-any.whl (42 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: deep-translator\nSuccessfully installed deep-translator-1.11.4\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"print(\"Distribution for Label1:\")\nprint(train['label1'].value_counts())\n\n# Check counts for label2 (Sub-category)\nprint(\"\\nDistribution for Label2:\")\nprint(train['label2'].value_counts())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-24T06:46:53.491059Z","iopub.execute_input":"2026-02-24T06:46:53.491385Z","iopub.status.idle":"2026-02-24T06:46:53.500771Z","shell.execute_reply.started":"2026-02-24T06:46:53.491354Z","shell.execute_reply":"2026-02-24T06:46:53.500054Z"}},"outputs":[{"name":"stdout","text":"Distribution for Label1:\nlabel1\n1    477\n0     23\nName: count, dtype: int64\n\nDistribution for Label2:\nlabel2\n0    327\n1    120\n2     53\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"from PIL import Image\nfrom torchvision import transforms\nfrom deep_translator import GoogleTranslator\nimport numpy as np\nimport pandas as pd\nimport time\n\naugmented_data = []\ncnt = 0\n\n# Image augmentation pipeline\nimg_augmentations = transforms.Compose([\n    transforms.ColorJitter(brightness=0.4, contrast=0.3),\n    transforms.RandomGrayscale(p=0.1),\n    transforms.RandomRotation(10), # Small rotation for variety\n])\n\n# Iterate through the Malayalam 'train' dataframe\nfor idx, row in train.iterrows():\n    # Updated Column Names: meme_id, Image_name (pixels), text\n    image_data = row['Image_name'] \n    text_content = row['text']\n    l1 = row['label1']\n    l2 = row['label2']\n    meme_id = row['meme_id']\n\n    # --- STRATEGY: Target the extreme minorities ---\n    # We augment if Label 1 is 0 OR Label 2 is 2 (Intersection)\n    if l1 == 0 or l2 == 2:\n        cnt += 1\n        if cnt % 5 == 0: print(f\"Augmenting Malayalam Minority {cnt}...\")\n\n        image_pil = Image.fromarray(np.uint8(image_data))\n        \n        # We will create 4 variations for each minority row to bridge the gap\n        # Languages: English, Tamil, Hindi, Kannada (Dravidian neighbor)\n        target_langs = ['en', 'ta', 'hi', 'kn']\n        \n        for lang in target_langs:\n            # 1. Image Transform\n            img_aug = np.array(img_augmentations(image_pil))\n\n            # 2. Back-Translation (Malayalam -> Lang -> Malayalam)\n            try:\n                translated = GoogleTranslator(source='ml', target=lang).translate(text_content)\n                time.sleep(0.5) # Safety to avoid 429 Error\n                text_aug = GoogleTranslator(source=lang, target='ml').translate(translated)\n            except:\n                text_aug = text_content # Fallback to original text\n\n            augmented_data.append({\n                'meme_id': f\"{meme_id}_aug_{lang}\",\n                'text': text_aug,\n                'label1': l1,\n                'label2': l2,\n                'Image_name': img_aug\n            })\n\n# Combine and check\naug_df = pd.DataFrame(augmented_data)\ntrain = pd.concat([train, aug_df], ignore_index=True)\n\nprint(\"\\n--- After Augmentation Distribution ---\")\nprint(\"Label 1:\\n\", train['label1'].value_counts())\nprint(\"Label 2:\\n\", train['label2'].value_counts())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-24T06:47:21.298887Z","iopub.execute_input":"2026-02-24T06:47:21.299411Z","iopub.status.idle":"2026-02-24T06:51:00.816170Z","shell.execute_reply.started":"2026-02-24T06:47:21.299381Z","shell.execute_reply":"2026-02-24T06:51:00.815502Z"}},"outputs":[{"name":"stdout","text":"Augmenting Malayalam Minority 5...\nAugmenting Malayalam Minority 10...\nAugmenting Malayalam Minority 15...\nAugmenting Malayalam Minority 20...\nAugmenting Malayalam Minority 25...\nAugmenting Malayalam Minority 30...\nAugmenting Malayalam Minority 35...\nAugmenting Malayalam Minority 40...\nAugmenting Malayalam Minority 45...\nAugmenting Malayalam Minority 50...\nAugmenting Malayalam Minority 55...\nAugmenting Malayalam Minority 60...\nAugmenting Malayalam Minority 65...\nAugmenting Malayalam Minority 70...\nAugmenting Malayalam Minority 75...\n\n--- After Augmentation Distribution ---\nLabel 1:\n label1\n1    685\n0    115\nName: count, dtype: int64\nLabel 2:\n label2\n0    375\n2    265\n1    160\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"# 1. Isolate the \"Support\" memes that are NOT yet over-augmented\n# Focus on those where label2 is 1 or 2 to keep the whole system balanced\ntarget_rows = train[(train['label1'] == 0) & (train['label2'] != 0)].copy()\n\n# 2. If we don't have enough of those, take any label1 == 0\nif len(target_rows) < 50:\n    target_rows = train[train['label1'] == 0].copy()\n\nmore_balanced_data = []\n# Use 4 variations per row to add ~400+ samples\nfinal_langs = ['en', 'ta', 'kn', 'te'] \n\nprint(f\"Expanding {len(target_rows)} Support memes...\")\n\nfor idx, row in target_rows.iterrows():\n    img_pil = Image.fromarray(np.uint8(row['Image_name'])).convert(\"RGB\")\n    \n    for lang in final_langs:\n        try:\n            # Back-translate\n            trans = GoogleTranslator(source='ml', target=lang).translate(row['text'])\n            time.sleep(0.4)\n            text_aug = GoogleTranslator(source=lang, target='ml').translate(trans)\n            \n            more_balanced_data.append({\n                'meme_id': f\"{row['meme_id']}_final_{lang}\",\n                'text': text_aug,\n                'label1': 0,\n                'label2': row['label2'],\n                'Image_name': np.array(img_augmentations(img_pil))\n            })\n        except:\n            continue\n\n# 3. Merge and Check\ntrain = pd.concat([train, pd.DataFrame(more_balanced_data)], ignore_index=True)\n\nprint(\"\\n--- FINAL DISTRIBUTION ---\")\nprint(\"Label 1:\\n\", train['label1'].value_counts())\nprint(\"Label 2:\\n\", train['label2'].value_counts())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-24T06:53:38.187533Z","iopub.execute_input":"2026-02-24T06:53:38.188149Z","iopub.status.idle":"2026-02-24T06:55:50.479923Z","shell.execute_reply.started":"2026-02-24T06:53:38.188115Z","shell.execute_reply":"2026-02-24T06:55:50.479147Z"}},"outputs":[{"name":"stdout","text":"Expanding 55 Support memes...\n\n--- FINAL DISTRIBUTION ---\nLabel 1:\n label1\n1    685\n0    335\nName: count, dtype: int64\nLabel 2:\n label2\n0    375\n1    360\n2    285\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"# 1. Target a balanced count (around 700 per class)\ntarget_count = 700\n\n# 2. Downsample Label 1 = 0\ndf_l1_0 = train[train['label1'] == 0]\nif len(df_l1_0) > target_count:\n    df_l1_0 = df_l1_0.sample(n=target_count, random_state=42)\n\n# 3. Keep all of Label 1 = 1 (since it has 685)\ndf_l1_1 = train[train['label1'] == 1]\n\n# 4. Combine into a final balanced training set\ntrain_balanced = pd.concat([df_l1_0, df_l1_1]).sample(frac=1, random_state=42).reset_index(drop=True)\n\nprint(\"--- FINAL OPTIMIZED DISTRIBUTION ---\")\nprint(train_balanced['label1'].value_counts())\nprint(train_balanced['label2'].value_counts())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-24T06:56:50.418324Z","iopub.execute_input":"2026-02-24T06:56:50.418637Z","iopub.status.idle":"2026-02-24T06:56:50.431242Z","shell.execute_reply.started":"2026-02-24T06:56:50.418611Z","shell.execute_reply":"2026-02-24T06:56:50.430523Z"}},"outputs":[{"name":"stdout","text":"--- FINAL OPTIMIZED DISTRIBUTION ---\nlabel1\n1    685\n0    335\nName: count, dtype: int64\nlabel2\n0    375\n1    360\n2    285\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"print(\"Distribution for Label1:\")\nprint(train_balanced['label1'].value_counts())\nprint(train_balanced['label2'].value_counts())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-24T06:56:54.867936Z","iopub.execute_input":"2026-02-24T06:56:54.868746Z","iopub.status.idle":"2026-02-24T06:56:54.874693Z","shell.execute_reply.started":"2026-02-24T06:56:54.868705Z","shell.execute_reply":"2026-02-24T06:56:54.873965Z"}},"outputs":[{"name":"stdout","text":"Distribution for Label1:\nlabel1\n1    685\n0    335\nName: count, dtype: int64\nlabel2\n0    375\n1    360\n2    285\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Create a combined key for stratification to keep both labels balanced in the split\nstratify_key = train_balanced[\"label1\"].astype(str) + \"_\" + train_balanced[\"label2\"].astype(str)\n\n# Split the balanced dataframe (85% for training, 15% for validation)\ndf_train, df_val = train_test_split(\n    train_balanced, \n    test_size=0.15, \n    random_state=42, \n    stratify=stratify_key\n)\n\nprint(f\"Training samples: {len(df_train)}\")\nprint(f\"Validation samples: {len(df_val)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-24T06:56:59.236678Z","iopub.execute_input":"2026-02-24T06:56:59.237231Z","iopub.status.idle":"2026-02-24T06:57:00.121130Z","shell.execute_reply.started":"2026-02-24T06:56:59.237202Z","shell.execute_reply":"2026-02-24T06:57:00.120493Z"}},"outputs":[{"name":"stdout","text":"Training samples: 867\nValidation samples: 153\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"train_balanced=train_balanced.dropna()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-24T06:57:43.850007Z","iopub.execute_input":"2026-02-24T06:57:43.850322Z","iopub.status.idle":"2026-02-24T06:57:43.854791Z","shell.execute_reply.started":"2026-02-24T06:57:43.850296Z","shell.execute_reply":"2026-02-24T06:57:43.854150Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport torch\nfrom transformers import CLIPProcessor, CLIPModel, AutoTokenizer, AutoModel\nimport easyocr\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import Input, Dense, Dropout, Concatenate, BatchNormalization\nfrom tensorflow.keras.models import Model\nfrom sklearn.model_selection import train_test_split","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-24T07:02:37.492297Z","iopub.execute_input":"2026-02-24T07:02:37.493246Z","iopub.status.idle":"2026-02-24T07:02:58.312248Z","shell.execute_reply.started":"2026-02-24T07:02:37.493200Z","shell.execute_reply":"2026-02-24T07:02:58.311666Z"}},"outputs":[{"name":"stderr","text":"2026-02-24 07:02:39.348892: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1771916559.512098      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1771916559.555602      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1771916559.933229      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1771916559.933255      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1771916559.933258      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1771916559.933260      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"import torch\nfrom transformers import BertTokenizer, BertModel\nfrom transformers import ViTModel, AutoFeatureExtractor  # <- changed here\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\nimport torch.optim as optim\nfrom PIL import Image\nclass TextDataset(Dataset):\n    def __init__(self, df, tokenizer, max_len=128):\n        self.df = df.reset_index(drop=True)\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        text = str(self.df.loc[idx, 'text'])\n        l1 = self.df.loc[idx, 'label1']\n        l2 = self.df.loc[idx, 'label2']\n\n        encoding = self.tokenizer(\n            text,\n            padding='max_length',\n            truncation=True,\n            max_length=self.max_len,\n            return_tensors='pt'\n        )\n\n        return {\n            'input_ids': encoding['input_ids'].squeeze(0),\n            'attention_mask': encoding['attention_mask'].squeeze(0),\n            'label1': torch.tensor(l1, dtype=torch.long),\n            'label2': torch.tensor(l2, dtype=torch.long)\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-24T07:05:21.553713Z","iopub.execute_input":"2026-02-24T07:05:21.554020Z","iopub.status.idle":"2026-02-24T07:05:21.560757Z","shell.execute_reply.started":"2026-02-24T07:05:21.553995Z","shell.execute_reply":"2026-02-24T07:05:21.560049Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"import torch\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(\"Using device:\", device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-24T07:11:55.925791Z","iopub.execute_input":"2026-02-24T07:11:55.926398Z","iopub.status.idle":"2026-02-24T07:11:55.930675Z","shell.execute_reply.started":"2026-02-24T07:11:55.926364Z","shell.execute_reply":"2026-02-24T07:11:55.930042Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":43},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.utils.class_weight import compute_class_weight\nimport numpy as np\nimport torch\n\n# Device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Level 1: Troll vs Support\nclasses_l1 = np.array([0, 1])  # Must be np.ndarray\ny_l1 = df_train['label1'].values\nweights_l1 = compute_class_weight(class_weight='balanced', classes=classes_l1, y=y_l1)\nweights_l1 = torch.tensor(weights_l1, dtype=torch.float).to(device)\n\n# Level 2: Person / Party / Intersection\nclasses_l2 = np.array([0, 1, 2])\ny_l2 = df_train['label2'].values\nweights_l2 = compute_class_weight(class_weight='balanced', classes=classes_l2, y=y_l2)\nweights_l2 = torch.tensor(weights_l2, dtype=torch.float).to(device)\n\n# Use in loss\ncriterion_l1 = nn.CrossEntropyLoss(weight=weights_l1)\ncriterion_l2 = nn.CrossEntropyLoss(weight=weights_l2)\n\nprint(\"✅ Class weights computed successfully:\")\nprint(\"Level 1:\", weights_l1)\nprint(\"Level 2:\", weights_l2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-24T07:20:33.069193Z","iopub.execute_input":"2026-02-24T07:20:33.069499Z","iopub.status.idle":"2026-02-24T07:20:33.170284Z","shell.execute_reply.started":"2026-02-24T07:20:33.069472Z","shell.execute_reply":"2026-02-24T07:20:33.169668Z"}},"outputs":[{"name":"stdout","text":"✅ Class weights computed successfully:\nLevel 1: tensor([1.5264, 0.7436], device='cuda:0')\nLevel 2: tensor([0.9060, 0.9444, 1.1942], device='cuda:0')\n","output_type":"stream"}],"execution_count":49},{"cell_type":"code","source":"from torch.utils.data import DataLoader, Dataset\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Example: Using BERT Multilingual\ntokenizer = AutoTokenizer.from_pretrained('bert-base-multilingual-cased')\nmodel = AutoModel.from_pretrained('bert-base-multilingual-cased').to(device)\n\n# Minimal Dataset wrapper\nclass TextDataset(Dataset):\n    def __init__(self, df, tokenizer, max_len=128):\n        self.df = df.reset_index(drop=True)\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        text = str(self.df.loc[idx, 'text'])\n        l1 = self.df.loc[idx, 'label1']\n        l2 = self.df.loc[idx, 'label2']\n\n        enc = self.tokenizer(\n            text, padding='max_length', truncation=True, max_length=self.max_len, return_tensors='pt'\n        )\n\n        return {\n            'input_ids': enc['input_ids'].squeeze(0),\n            'attention_mask': enc['attention_mask'].squeeze(0),\n            'label1': torch.tensor(l1, dtype=torch.long),\n            'label2': torch.tensor(l2, dtype=torch.long)\n        }\n\n# Create DataLoaders\ntrain_dataset = TextDataset(df_train, tokenizer)\ndev_dataset = TextDataset(df_val, tokenizer)\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\ndev_loader = DataLoader(dev_dataset, batch_size=16)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-24T07:22:43.846347Z","iopub.execute_input":"2026-02-24T07:22:43.846906Z","iopub.status.idle":"2026-02-24T07:22:45.565931Z","shell.execute_reply.started":"2026-02-24T07:22:43.846865Z","shell.execute_reply":"2026-02-24T07:22:45.565295Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc2499f209e04d3ca3b55a05bacf51da"}},"metadata":{}},{"name":"stderr","text":"\u001b[1mBertModel LOAD REPORT\u001b[0m from: bert-base-multilingual-cased\nKey                                        | Status     |  | \n-------------------------------------------+------------+--+-\ncls.predictions.transform.LayerNorm.bias   | UNEXPECTED |  | \ncls.seq_relationship.weight                | UNEXPECTED |  | \ncls.seq_relationship.bias                  | UNEXPECTED |  | \ncls.predictions.transform.LayerNorm.weight | UNEXPECTED |  | \ncls.predictions.transform.dense.bias       | UNEXPECTED |  | \ncls.predictions.transform.dense.weight     | UNEXPECTED |  | \ncls.predictions.bias                       | UNEXPECTED |  | \n\n\u001b[3mNotes:\n- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n","output_type":"stream"}],"execution_count":52},{"cell_type":"code","source":"import torch.nn as nn\nimport torch\n\nclass TextClassifier(nn.Module):\n    def __init__(self, transformer_model, num_l1, num_l2):\n        super(TextClassifier, self).__init__()\n        self.transformer = transformer_model\n        hidden_size = transformer_model.config.hidden_size\n        self.dropout = nn.Dropout(0.3)\n        self.classifier_l1 = nn.Linear(hidden_size, num_l1)\n        self.classifier_l2 = nn.Linear(hidden_size, num_l2)\n\n    def forward(self, input_ids, attention_mask):\n        output = self.transformer(input_ids=input_ids, attention_mask=attention_mask)\n        # Use [CLS] token\n        cls_feat = output.last_hidden_state[:, 0, :]\n        cls_feat = self.dropout(cls_feat)\n        return self.classifier_l1(cls_feat), self.classifier_l2(cls_feat)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-24T07:22:49.808555Z","iopub.execute_input":"2026-02-24T07:22:49.809281Z","iopub.status.idle":"2026-02-24T07:22:49.817128Z","shell.execute_reply.started":"2026-02-24T07:22:49.809239Z","shell.execute_reply":"2026-02-24T07:22:49.816327Z"}},"outputs":[],"execution_count":53},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = TextClassifier(AutoModel.from_pretrained('bert-base-multilingual-cased'), num_l1=2, num_l2=3).to(device)\noptimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n\nepochs = 5  # You can increase\n\nfor epoch in range(epochs):\n    model.train()\n    total_loss = 0\n    for batch in train_loader:\n        ids = batch['input_ids'].to(device)\n        mask = batch['attention_mask'].to(device)\n        lab1 = batch['label1'].to(device)\n        lab2 = batch['label2'].to(device)\n\n        optimizer.zero_grad()\n        out1, out2 = model(ids, mask)\n        loss = criterion_l1(out1, lab1) + criterion_l2(out2, lab2)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n\n    print(f\"Epoch {epoch+1} | Train Loss: {total_loss/len(train_loader):.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-24T07:22:53.293188Z","iopub.execute_input":"2026-02-24T07:22:53.293476Z","iopub.status.idle":"2026-02-24T07:24:41.113935Z","shell.execute_reply.started":"2026-02-24T07:22:53.293449Z","shell.execute_reply":"2026-02-24T07:24:41.113133Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a1a212da2fe41aaa08e607799293c60"}},"metadata":{}},{"name":"stderr","text":"\u001b[1mBertModel LOAD REPORT\u001b[0m from: bert-base-multilingual-cased\nKey                                        | Status     |  | \n-------------------------------------------+------------+--+-\ncls.predictions.transform.LayerNorm.bias   | UNEXPECTED |  | \ncls.seq_relationship.weight                | UNEXPECTED |  | \ncls.seq_relationship.bias                  | UNEXPECTED |  | \ncls.predictions.transform.LayerNorm.weight | UNEXPECTED |  | \ncls.predictions.transform.dense.bias       | UNEXPECTED |  | \ncls.predictions.transform.dense.weight     | UNEXPECTED |  | \ncls.predictions.bias                       | UNEXPECTED |  | \n\n\u001b[3mNotes:\n- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 | Train Loss: 1.5224\nEpoch 2 | Train Loss: 1.2746\nEpoch 3 | Train Loss: 1.0790\nEpoch 4 | Train Loss: 0.8645\nEpoch 5 | Train Loss: 0.9027\n","output_type":"stream"}],"execution_count":54},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nval_loader = dev_loader  # just alias it\n\nmodel.eval()\nl1_true, l1_pred, l2_true, l2_pred = [], [], [], []\n\nwith torch.no_grad():\n    for batch in val_loader:\n        ids = batch['input_ids'].to(device)\n        mask = batch['attention_mask'].to(device)\n        y1 = batch['label1'].to(device)\n        y2 = batch['label2'].to(device)\n\n        out1, out2 = model(ids, mask)\n        _, p1 = torch.max(out1, 1)\n        _, p2 = torch.max(out2, 1)\n\n        l1_true.extend(y1.cpu().numpy())\n        l1_pred.extend(p1.cpu().numpy())\n        l2_true.extend(y2.cpu().numpy())\n        l2_pred.extend(p2.cpu().numpy())\n\nprint(classification_report(l1_true, l1_pred, digits=4))\nprint(classification_report(l2_true, l2_pred, digits=4))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-24T07:25:28.270761Z","iopub.execute_input":"2026-02-24T07:25:28.271536Z","iopub.status.idle":"2026-02-24T07:25:29.533533Z","shell.execute_reply.started":"2026-02-24T07:25:28.271505Z","shell.execute_reply":"2026-02-24T07:25:29.532897Z"}},"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           0     0.9787    0.9020    0.9388        51\n           1     0.9528    0.9902    0.9712       102\n\n    accuracy                         0.9608       153\n   macro avg     0.9658    0.9461    0.9550       153\nweighted avg     0.9615    0.9608    0.9604       153\n\n              precision    recall  f1-score   support\n\n           0     0.6923    0.8036    0.7438        56\n           1     1.0000    0.7037    0.8261        54\n           2     0.7000    0.8140    0.7527        43\n\n    accuracy                         0.7712       153\n   macro avg     0.7974    0.7737    0.7742       153\nweighted avg     0.8031    0.7712    0.7753       153\n\n","output_type":"stream"}],"execution_count":56},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModel\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# 1️⃣ Tokenizer & Model\nxlm_tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base')\nxlm_model = AutoModel.from_pretrained('xlm-roberta-base')\n\n# 2️⃣ Simple Text Classifier for Level 1 & Level 2\nclass TextClassifier(nn.Module):\n    def __init__(self, model, num_l1, num_l2):\n        super(TextClassifier, self).__init__()\n        self.model = model\n        hidden_size = model.config.hidden_size\n        self.classifier_l1 = nn.Linear(hidden_size, num_l1)\n        self.classifier_l2 = nn.Linear(hidden_size, num_l2)\n        self.dropout = nn.Dropout(0.3)\n\n    def forward(self, input_ids, attention_mask):\n        out = self.model(input_ids=input_ids, attention_mask=attention_mask)\n        # Take the [CLS] token representation\n        cls_features = out.last_hidden_state[:, 0, :]\n        cls_features = self.dropout(cls_features)\n        logits_l1 = self.classifier_l1(cls_features)\n        logits_l2 = self.classifier_l2(cls_features)\n        return logits_l1, logits_l2\n\nxlm_classifier = TextClassifier(xlm_model, num_l1=2, num_l2=3).to(device)\n\n# 3️⃣ Optional: define a dev_loader for evaluation\n# Make sure you have a dataset that uses only text\nclass TextDataset(Dataset):\n    def __init__(self, df, tokenizer):\n        self.df = df.reset_index(drop=True)\n        self.tokenizer = tokenizer\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        text = str(self.df.loc[idx, 'text'])\n        l1 = self.df.loc[idx, 'label1']\n        l2 = self.df.loc[idx, 'label2']\n        enc = self.tokenizer(text, padding='max_length', truncation=True, max_length=128, return_tensors='pt')\n        return {\n            'input_ids': enc['input_ids'].squeeze(0),\n            'attention_mask': enc['attention_mask'].squeeze(0),\n            'label1': torch.tensor(l1, dtype=torch.long),\n            'label2': torch.tensor(l2, dtype=torch.long)\n        }\n\ndev_loader = DataLoader(TextDataset(df_val, xlm_tokenizer), batch_size=16)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-24T07:27:57.435594Z","iopub.execute_input":"2026-02-24T07:27:57.436299Z","iopub.status.idle":"2026-02-24T07:28:01.020645Z","shell.execute_reply.started":"2026-02-24T07:27:57.436264Z","shell.execute_reply":"2026-02-24T07:28:01.019888Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d1f9f7c99f24a1eadffb153d6663a73"}},"metadata":{}},{"name":"stderr","text":"\u001b[1mXLMRobertaModel LOAD REPORT\u001b[0m from: xlm-roberta-base\nKey                       | Status     |  | \n--------------------------+------------+--+-\nlm_head.dense.bias        | UNEXPECTED |  | \nlm_head.layer_norm.weight | UNEXPECTED |  | \nlm_head.bias              | UNEXPECTED |  | \nlm_head.layer_norm.bias   | UNEXPECTED |  | \nlm_head.dense.weight      | UNEXPECTED |  | \n\n\u001b[3mNotes:\n- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n","output_type":"stream"}],"execution_count":58},{"cell_type":"code","source":"# ============================\n# 1. Imports\n# ============================\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoTokenizer, AutoModel\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import classification_report\nimport numpy as np\nimport pandas as pd\n\n# ============================\n# 2. Device\n# ============================\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n# ============================\n# 3. Dataset Class\n# ============================\nclass TextDataset(Dataset):\n    def __init__(self, df, tokenizer, max_len=128):\n        self.df = df.reset_index(drop=True)\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        text = str(self.df.loc[idx, 'text'])\n        l1 = self.df.loc[idx, 'label1']  # Troll/Support\n        l2 = self.df.loc[idx, 'label2']  # Person/Party/Intersection\n\n        encoding = self.tokenizer(\n            text,\n            padding='max_length',\n            truncation=True,\n            max_length=self.max_len,\n            return_tensors='pt'\n        )\n\n        return {\n            'input_ids': encoding['input_ids'].squeeze(0),\n            'attention_mask': encoding['attention_mask'].squeeze(0),\n            'label1': torch.tensor(l1, dtype=torch.long),\n            'label2': torch.tensor(l2, dtype=torch.long)\n        }\n\n# ============================\n# 4. Model Definition\n# ============================\nclass XLMRClassifier(nn.Module):\n    def __init__(self, num_classes_l1, num_classes_l2):\n        super(XLMRClassifier, self).__init__()\n        self.model = AutoModel.from_pretrained('xlm-roberta-base')\n        hidden_size = self.model.config.hidden_size  # 768\n        self.dropout = nn.Dropout(0.3)\n        self.classifier_l1 = nn.Linear(hidden_size, num_classes_l1)\n        self.classifier_l2 = nn.Linear(hidden_size, num_classes_l2)\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n        # Use [CLS] token (first token)\n        pooled_output = outputs.last_hidden_state[:, 0, :]\n        pooled_output = self.dropout(pooled_output)\n        out1 = self.classifier_l1(pooled_output)\n        out2 = self.classifier_l2(pooled_output)\n        return out1, out2\n\n# ============================\n# 5. Load Data (Replace with your df_train/df_val)\n# ============================\n# Example format:\n# df_train = pd.DataFrame({'text':[...], 'label1':[...], 'label2':[...])\n# df_val = pd.DataFrame({'text':[...], 'label1':[...], 'label2':[...])\n\n# Tokenizer\ntokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base')\n\n# Datasets & Loaders\ntrain_dataset = TextDataset(df_train, tokenizer)\nval_dataset = TextDataset(df_val, tokenizer)\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=16)\n\n# ============================\n# 6. Compute Class Weights for LEVEL 1 (Troll/Support)\n# ============================\ny_train = df_train['label1'].values\nclasses_l1 = np.array([0, 1])\nweights_l1 = compute_class_weight('balanced', classes=classes_l1, y=y_train)\nclass_weights_l1 = torch.tensor(weights_l1, dtype=torch.float).to(device)\nprint(\"Class weights for Level1:\", class_weights_l1)\n\n# LEVEL 2 weights optional\ny_train_l2 = df_train['label2'].values\nclasses_l2 = np.array([0,1,2])\nweights_l2 = compute_class_weight('balanced', classes=classes_l2, y=y_train_l2)\nclass_weights_l2 = torch.tensor(weights_l2, dtype=torch.float).to(device)\nprint(\"Class weights for Level2:\", class_weights_l2)\n\n# ============================\n# 7. Initialize Model, Loss, Optimizer\n# ============================\nnum_l1 = 2  # Troll/Support\nnum_l2 = 3  # Person/Party/Intersection\n\nmodel = XLMRClassifier(num_l1, num_l2).to(device)\ncriterion_l1 = nn.CrossEntropyLoss(weight=class_weights_l1)\ncriterion_l2 = nn.CrossEntropyLoss(weight=class_weights_l2)\noptimizer = optim.AdamW(model.parameters(), lr=2e-5)\n\n# ============================\n# 8. Training Loop\n# ============================\nepochs = 3  # adjust as needed\n\nfor epoch in range(epochs):\n    model.train()\n    total_loss = 0\n    for batch in train_loader:\n        ids = batch['input_ids'].to(device)\n        mask = batch['attention_mask'].to(device)\n        lab1 = batch['label1'].to(device)\n        lab2 = batch['label2'].to(device)\n\n        optimizer.zero_grad()\n        out1, out2 = model(ids, mask)\n        loss = criterion_l1(out1, lab1) + criterion_l2(out2, lab2)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n\n    print(f\"Epoch {epoch+1}/{epochs} | Loss: {total_loss/len(train_loader):.4f}\")\n\n# ============================\n# 9. Evaluation\n# ============================\nmodel.eval()\nl1_true, l1_pred = [], []\nl2_true, l2_pred = [], []\n\nwith torch.no_grad():\n    for batch in val_loader:\n        ids = batch['input_ids'].to(device)\n        mask = batch['attention_mask'].to(device)\n        lab1 = batch['label1'].to(device)\n        lab2 = batch['label2'].to(device)\n\n        out1, out2 = model(ids, mask)\n        _, p1 = torch.max(out1, 1)\n        _, p2 = torch.max(out2, 1)\n\n        l1_true.extend(lab1.cpu().numpy())\n        l1_pred.extend(p1.cpu().numpy())\n        l2_true.extend(lab2.cpu().numpy())\n        l2_pred.extend(p2.cpu().numpy())\n\n# ============================\n# 10. Print Reports\n# ============================\ntarget_names_l1 = ['SUPPORT', 'TROLL/OPPOSE']\ntarget_names_l2 = ['PERSON','PARTY','INTERSECTION']\n\nprint(\"\\n🚀 LEVEL 1: Sentiment (Troll vs Support)\")\nprint(classification_report(l1_true, l1_pred, target_names=target_names_l1, digits=4))\n\nprint(\"\\n🚀 LEVEL 2: Target (Person/Party/Intersection)\")\nprint(classification_report(l2_true, l2_pred, target_names=target_names_l2, digits=4))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-24T07:30:47.293581Z","iopub.execute_input":"2026-02-24T07:30:47.293947Z","iopub.status.idle":"2026-02-24T07:32:02.872841Z","shell.execute_reply.started":"2026-02-24T07:30:47.293917Z","shell.execute_reply":"2026-02-24T07:32:02.872041Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nClass weights for Level1: tensor([1.5264, 0.7436], device='cuda:0')\nClass weights for Level2: tensor([0.9060, 0.9444, 1.1942], device='cuda:0')\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f864d6fdcb94f158a58938202814440"}},"metadata":{}},{"name":"stderr","text":"\u001b[1mXLMRobertaModel LOAD REPORT\u001b[0m from: xlm-roberta-base\nKey                       | Status     |  | \n--------------------------+------------+--+-\nlm_head.dense.bias        | UNEXPECTED |  | \nlm_head.layer_norm.weight | UNEXPECTED |  | \nlm_head.bias              | UNEXPECTED |  | \nlm_head.layer_norm.bias   | UNEXPECTED |  | \nlm_head.dense.weight      | UNEXPECTED |  | \n\n\u001b[3mNotes:\n- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3 | Loss: 1.6318\nEpoch 2/3 | Loss: 1.4903\nEpoch 3/3 | Loss: 1.3808\n\n🚀 LEVEL 1: Sentiment (Troll vs Support)\n              precision    recall  f1-score   support\n\n     SUPPORT     0.8974    0.6863    0.7778        51\nTROLL/OPPOSE     0.8596    0.9608    0.9074       102\n\n    accuracy                         0.8693       153\n   macro avg     0.8785    0.8235    0.8426       153\nweighted avg     0.8722    0.8693    0.8642       153\n\n\n🚀 LEVEL 2: Target (Person/Party/Intersection)\n              precision    recall  f1-score   support\n\n      PERSON     0.6316    0.8571    0.7273        56\n       PARTY     1.0000    0.0741    0.1379        54\nINTERSECTION     0.4247    0.7209    0.5345        43\n\n    accuracy                         0.5425       153\n   macro avg     0.6854    0.5507    0.4666       153\nweighted avg     0.7035    0.5425    0.4651       153\n\n","output_type":"stream"}],"execution_count":60},{"cell_type":"code","source":"import torch\nfrom torch import nn, optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import models, transforms\nfrom PIL import Image\nfrom sklearn.utils.class_weight import compute_class_weight\nimport numpy as np\n\n# -------------------------\n# Device\n# -------------------------\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# -------------------------\n# Image Transforms\n# -------------------------\nimage_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                         std=[0.229, 0.224, 0.225]),\n])\n\n# -------------------------\n# Dataset\n# -------------------------\nclass ImageDataset(Dataset):\n    def __init__(self, df, transform):\n        self.df = df.reset_index(drop=True)\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        image_arr = self.df.loc[idx, 'Image_name']\n        label1 = self.df.loc[idx, 'label1']\n        label2 = self.df.loc[idx, 'label2']\n\n        # Convert to PIL image\n        image = Image.fromarray(image_arr.astype('uint8')).convert(\"RGB\")\n        image = self.transform(image)\n\n        return {\n            'pixel_values': image,\n            'label1': torch.tensor(label1, dtype=torch.long),\n            'label2': torch.tensor(label2, dtype=torch.long)\n        }\n\n# -------------------------\n# Model Classes\n# -------------------------\nclass ImageClassifier(nn.Module):\n    def __init__(self, base_model_name='resnet50', num_l1=2, num_l2=3):\n        super().__init__()\n        if base_model_name == 'resnet50':\n            self.model = models.resnet50(pretrained=True)\n            self.model.fc = nn.Identity()  # remove classifier\n            feature_dim = 2048\n        elif base_model_name == 'vit':\n            from transformers import ViTModel\n            self.model = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n            feature_dim = 768\n        elif base_model_name == 'clip':\n            from transformers import CLIPModel\n            self.model = CLIPModel.from_pretrained('openai/clip-vit-base-patch32')\n            self.model = self.model.vision_model\n            feature_dim = 768\n        else:\n            raise ValueError(\"Choose 'resnet50', 'vit', or 'clip'.\")\n\n        # Final classifiers\n        self.dropout = nn.Dropout(0.3)\n        self.classifier_l1 = nn.Linear(feature_dim, num_l1)\n        self.classifier_l2 = nn.Linear(feature_dim, num_l2)\n        self.base_model_name = base_model_name\n\n    def forward(self, pixel_values):\n        if self.base_model_name in ['resnet50']:\n            features = self.model(pixel_values)\n        else:  # ViT / CLIP\n            features = self.model(pixel_values).pooler_output\n\n        features = self.dropout(features)\n        return self.classifier_l1(features), self.classifier_l2(features)\n\n# -------------------------\n# Dataloaders\n# -------------------------\ntrain_loader = DataLoader(ImageDataset(df_train, image_transform), batch_size=16, shuffle=True)\nval_loader   = DataLoader(ImageDataset(df_val, image_transform), batch_size=16)\n\n# -------------------------\n# Initialize Model\n# -------------------------\nmodel_name = 'resnet50'  # change to 'vit' or 'clip'\nmodel = ImageClassifier(base_model_name=model_name, num_l1=2, num_l2=3).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.AdamW(model.parameters(), lr=2e-5)\n\n# -------------------------\n# Training Loop\n# -------------------------\nepochs = 3\nfor epoch in range(epochs):\n    model.train()\n    total_loss = 0\n    for batch in train_loader:\n        pix = batch['pixel_values'].to(device)\n        lab1 = batch['label1'].to(device)\n        lab2 = batch['label2'].to(device)\n\n        optimizer.zero_grad()\n        out1, out2 = model(pix)\n        loss = criterion(out1, lab1) + criterion(out2, lab2)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n\n    print(f\"Epoch {epoch+1}/{epochs} | Loss: {total_loss/len(train_loader):.4f}\")\n\n# -------------------------\n# Evaluation\n# -------------------------\nfrom sklearn.metrics import classification_report\n\nmodel.eval()\nl1_true, l1_pred = [], []\nl2_true, l2_pred = [], []\n\nwith torch.no_grad():\n    for batch in val_loader:\n        pix = batch['pixel_values'].to(device)\n        lab1 = batch['label1'].to(device)\n        lab2 = batch['label2'].to(device)\n        out1, out2 = model(pix)\n\n        _, p1 = torch.max(out1, 1)\n        _, p2 = torch.max(out2, 1)\n\n        l1_true.extend(lab1.cpu().numpy())\n        l1_pred.extend(p1.cpu().numpy())\n        l2_true.extend(lab2.cpu().numpy())\n        l2_pred.extend(p2.cpu().numpy())\n\nprint(\"📊 LEVEL 1: Sentiment (Troll vs Support)\")\nprint(classification_report(l1_true, l1_pred, digits=4))\nprint(\"\\n📊 LEVEL 2: Target (Person/Party/Intersection)\")\nprint(classification_report(l2_true, l2_pred, digits=4))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-24T07:37:42.687327Z","iopub.execute_input":"2026-02-24T07:37:42.687994Z","iopub.status.idle":"2026-02-24T07:38:13.568594Z","shell.execute_reply.started":"2026-02-24T07:37:42.687960Z","shell.execute_reply":"2026-02-24T07:38:13.568007Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n","output_type":"stream"},{"name":"stdout","text":"Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 97.8M/97.8M [00:00<00:00, 193MB/s] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3 | Loss: 1.4285\nEpoch 2/3 | Loss: 0.7835\nEpoch 3/3 | Loss: 0.4659\n📊 LEVEL 1: Sentiment (Troll vs Support)\n              precision    recall  f1-score   support\n\n           0     1.0000    0.9216    0.9592        51\n           1     0.9623    1.0000    0.9808       102\n\n    accuracy                         0.9739       153\n   macro avg     0.9811    0.9608    0.9700       153\nweighted avg     0.9748    0.9739    0.9736       153\n\n\n📊 LEVEL 2: Target (Person/Party/Intersection)\n              precision    recall  f1-score   support\n\n           0     0.7500    0.9107    0.8226        56\n           1     0.9535    0.7593    0.8454        54\n           2     0.9286    0.9070    0.9176        43\n\n    accuracy                         0.8562       153\n   macro avg     0.8774    0.8590    0.8619       153\nweighted avg     0.8720    0.8562    0.8573       153\n\n","output_type":"stream"}],"execution_count":61},{"cell_type":"code","source":"import torch\nfrom torch import nn, optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import models, transforms\nfrom PIL import Image\nfrom sklearn.utils.class_weight import compute_class_weight\nimport numpy as np\n\n# -------------------------\n# Device\n# -------------------------\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# -------------------------\n# Image Transforms\n# -------------------------\nimage_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                         std=[0.229, 0.224, 0.225]),\n])\n\n# -------------------------\n# Dataset\n# -------------------------\nclass ImageDataset(Dataset):\n    def __init__(self, df, transform):\n        self.df = df.reset_index(drop=True)\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        image_arr = self.df.loc[idx, 'Image_name']\n        label1 = self.df.loc[idx, 'label1']\n        label2 = self.df.loc[idx, 'label2']\n\n        # Convert to PIL image\n        image = Image.fromarray(image_arr.astype('uint8')).convert(\"RGB\")\n        image = self.transform(image)\n\n        return {\n            'pixel_values': image,\n            'label1': torch.tensor(label1, dtype=torch.long),\n            'label2': torch.tensor(label2, dtype=torch.long)\n        }\n\n# -------------------------\n# Model Classes\n# -------------------------\nclass ImageClassifier(nn.Module):\n    def __init__(self, base_model_name='resnet50', num_l1=2, num_l2=3):\n        super().__init__()\n        if base_model_name == 'resnet50':\n            self.model = models.resnet50(pretrained=True)\n            self.model.fc = nn.Identity()  # remove classifier\n            feature_dim = 2048\n        elif base_model_name == 'vit':\n            from transformers import ViTModel\n            self.model = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n            feature_dim = 768\n        elif base_model_name == 'clip':\n            from transformers import CLIPModel\n            self.model = CLIPModel.from_pretrained('openai/clip-vit-base-patch32')\n            self.model = self.model.vision_model\n            feature_dim = 768\n        else:\n            raise ValueError(\"Choose 'resnet50', 'vit', or 'clip'.\")\n\n        # Final classifiers\n        self.dropout = nn.Dropout(0.3)\n        self.classifier_l1 = nn.Linear(feature_dim, num_l1)\n        self.classifier_l2 = nn.Linear(feature_dim, num_l2)\n        self.base_model_name = base_model_name\n\n    def forward(self, pixel_values):\n        if self.base_model_name in ['resnet50']:\n            features = self.model(pixel_values)\n        else:  # ViT / CLIP\n            features = self.model(pixel_values).pooler_output\n\n        features = self.dropout(features)\n        return self.classifier_l1(features), self.classifier_l2(features)\n\n# -------------------------\n# Dataloaders\n# -------------------------\ntrain_loader = DataLoader(ImageDataset(df_train, image_transform), batch_size=16, shuffle=True)\nval_loader   = DataLoader(ImageDataset(df_val, image_transform), batch_size=16)\n\n# -------------------------\n# Initialize Model\n# -------------------------\nmodel_name = 'vit'  # change to 'vit' or 'clip'\nmodel = ImageClassifier(base_model_name=model_name, num_l1=2, num_l2=3).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.AdamW(model.parameters(), lr=2e-5)\n\n# -------------------------\n# Training Loop\n# -------------------------\nepochs = 3\nfor epoch in range(epochs):\n    model.train()\n    total_loss = 0\n    for batch in train_loader:\n        pix = batch['pixel_values'].to(device)\n        lab1 = batch['label1'].to(device)\n        lab2 = batch['label2'].to(device)\n\n        optimizer.zero_grad()\n        out1, out2 = model(pix)\n        loss = criterion(out1, lab1) + criterion(out2, lab2)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n\n    print(f\"Epoch {epoch+1}/{epochs} | Loss: {total_loss/len(train_loader):.4f}\")\n\n# -------------------------\n# Evaluation\n# -------------------------\nfrom sklearn.metrics import classification_report\n\nmodel.eval()\nl1_true, l1_pred = [], []\nl2_true, l2_pred = [], []\n\nwith torch.no_grad():\n    for batch in val_loader:\n        pix = batch['pixel_values'].to(device)\n        lab1 = batch['label1'].to(device)\n        lab2 = batch['label2'].to(device)\n        out1, out2 = model(pix)\n\n        _, p1 = torch.max(out1, 1)\n        _, p2 = torch.max(out2, 1)\n\n        l1_true.extend(lab1.cpu().numpy())\n        l1_pred.extend(p1.cpu().numpy())\n        l2_true.extend(lab2.cpu().numpy())\n        l2_pred.extend(p2.cpu().numpy())\n\nprint(\"📊 LEVEL 1: Sentiment (Troll vs Support)\")\nprint(classification_report(l1_true, l1_pred, digits=4))\nprint(\"\\n📊 LEVEL 2: Target (Person/Party/Intersection)\")\nprint(classification_report(l2_true, l2_pred, digits=4))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-24T07:39:31.599556Z","iopub.execute_input":"2026-02-24T07:39:31.600153Z","iopub.status.idle":"2026-02-24T07:41:05.702094Z","shell.execute_reply.started":"2026-02-24T07:39:31.600119Z","shell.execute_reply":"2026-02-24T07:41:05.701330Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/502 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2931dea831734dd8a6c61abd8027f30d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6422e62cd4a543d182cb0fcfdc7d8022"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading weights:   0%|          | 0/200 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b68dffc3b8e147c1b4c462b93852e81a"}},"metadata":{}},{"name":"stdout","text":"Epoch 1/3 | Loss: 1.4957\nEpoch 2/3 | Loss: 1.0591\nEpoch 3/3 | Loss: 0.7031\n📊 LEVEL 1: Sentiment (Troll vs Support)\n              precision    recall  f1-score   support\n\n           0     1.0000    0.8627    0.9263        51\n           1     0.9358    1.0000    0.9668       102\n\n    accuracy                         0.9542       153\n   macro avg     0.9679    0.9314    0.9466       153\nweighted avg     0.9572    0.9542    0.9533       153\n\n\n📊 LEVEL 2: Target (Person/Party/Intersection)\n              precision    recall  f1-score   support\n\n           0     0.6714    0.8393    0.7460        56\n           1     0.9750    0.7222    0.8298        54\n           2     0.7442    0.7442    0.7442        43\n\n    accuracy                         0.7712       153\n   macro avg     0.7969    0.7686    0.7733       153\nweighted avg     0.7990    0.7712    0.7751       153\n\n","output_type":"stream"}],"execution_count":62},{"cell_type":"code","source":"# ===============================\n# 1. Imports\n# ===============================\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import CLIPProcessor, CLIPModel\nfrom PIL import Image\nimport pandas as pd\nfrom sklearn.metrics import classification_report\n\n# ===============================\n# 2. Device\n# ===============================\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# ===============================\n# 3. Dataset\n# ===============================\nclass ClipImageDataset(Dataset):\n    def __init__(self, df, clip_processor):\n        self.df = df.reset_index(drop=True)\n        self.processor = clip_processor\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        img_arr = self.df.loc[idx, 'Image_name']  # numpy array\n        label1 = self.df.loc[idx, 'label1']\n        label2 = self.df.loc[idx, 'label2']\n\n        image = Image.fromarray(img_arr.astype('uint8')).convert(\"RGB\")\n        pixel_values = self.processor(images=image, return_tensors=\"pt\")['pixel_values'].squeeze(0)\n\n        return {\n            'pixel_values': pixel_values,\n            'label1': torch.tensor(label1, dtype=torch.long),\n            'label2': torch.tensor(label2, dtype=torch.long)\n        }\n\n# ===============================\n# 4. CLIP Vision Model\n# ===============================\nclass ClipUnimodal(nn.Module):\n    def __init__(self, clip_model, num_l1=2, num_l2=3):\n        super(ClipUnimodal, self).__init__()\n        self.clip_vision = clip_model.vision_model  # Only use vision\n        self.dropout = nn.Dropout(0.3)\n        self.classifier_l1 = nn.Linear(768, num_l1)  # CLIP ViT-B/32 -> 768 features\n        self.classifier_l2 = nn.Linear(768, num_l2)\n\n    def forward(self, pixel_values):\n        features = self.clip_vision(pixel_values=pixel_values).pooler_output\n        features = self.dropout(features)\n        out1 = self.classifier_l1(features)\n        out2 = self.classifier_l2(features)\n        return out1, out2\n\n# ===============================\n# 5. Load CLIP Processor and Model\n# ===============================\nclip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\nclip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n\n# ===============================\n# 6. Data Loaders\n# ===============================\ntrain_dataset = ClipImageDataset(df_train, clip_processor)\nval_dataset = ClipImageDataset(df_val, clip_processor)\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=16)\n\n# ===============================\n# 7. Initialize Model, Loss, Optimizer\n# ===============================\nmodel = ClipUnimodal(clip_model, num_l1=2, num_l2=3).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.AdamW(model.parameters(), lr=2e-5)\n\n# ===============================\n# 8. Training Loop\n# ===============================\nepochs = 3\nfor epoch in range(epochs):\n    model.train()\n    total_loss = 0\n    for batch in train_loader:\n        pix = batch['pixel_values'].to(device)\n        lab1 = batch['label1'].to(device)\n        lab2 = batch['label2'].to(device)\n\n        optimizer.zero_grad()\n        out1, out2 = model(pix)\n\n        loss = criterion(out1, lab1) + criterion(out2, lab2)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n\n    print(f\"Epoch {epoch+1}/{epochs} | Loss: {total_loss/len(train_loader):.4f}\")\n\n# ===============================\n# 9. Evaluation\n# ===============================\nmodel.eval()\nl1_true, l1_pred = [], []\nl2_true, l2_pred = [], []\n\nwith torch.no_grad():\n    for batch in val_loader:\n        pix = batch['pixel_values'].to(device)\n        lab1 = batch['label1'].to(device)\n        lab2 = batch['label2'].to(device)\n\n        out1, out2 = model(pix)\n        _, p1 = torch.max(out1, dim=1)\n        _, p2 = torch.max(out2, dim=1)\n\n        l1_true.extend(lab1.cpu().numpy())\n        l1_pred.extend(p1.cpu().numpy())\n        l2_true.extend(lab2.cpu().numpy())\n        l2_pred.extend(p2.cpu().numpy())\n\n# Level 1 Report\ntarget_names_l1 = ['SUPPORT', 'TROLL/OPPOSE']\nprint(\"📊 LEVEL 1: Sentiment (Troll vs Support)\")\nprint(classification_report(l1_true, l1_pred, target_names=target_names_l1, digits=4))\n\n# Level 2 Report\ntarget_names_l2 = ['PERSON', 'PARTY', 'INTERSECTION']\nprint(\"📊 LEVEL 2: Target (Person/Party/Intersection)\")\nprint(classification_report(l2_true, l2_pred, target_names=target_names_l2, digits=4))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-24T07:47:10.325134Z","iopub.execute_input":"2026-02-24T07:47:10.326051Z","iopub.status.idle":"2026-02-24T07:47:49.782341Z","shell.execute_reply.started":"2026-02-24T07:47:10.326012Z","shell.execute_reply":"2026-02-24T07:47:49.781496Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1723d57dfdb4788aeebd11ea87332ad"}},"metadata":{}},{"name":"stderr","text":"The image processor of type `CLIPImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. \n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/592 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"93e7d9115b09405daa74441351737260"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f437b6db969143a0be01140631a3ad6d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68c3f6314a96426d92617dea0f408256"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d8e206394704cbfaa2332f19ed12730"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6fe04eddb684b16aa96ac7f0e49b02e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading weights:   0%|          | 0/398 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"79be219b493448bfab652935e1b6501a"}},"metadata":{}},{"name":"stderr","text":"\u001b[1mCLIPModel LOAD REPORT\u001b[0m from: openai/clip-vit-base-patch32\nKey                                  | Status     |  | \n-------------------------------------+------------+--+-\ntext_model.embeddings.position_ids   | UNEXPECTED |  | \nvision_model.embeddings.position_ids | UNEXPECTED |  | \n\n\u001b[3mNotes:\n- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3 | Loss: 0.9923\nEpoch 2/3 | Loss: 0.2508\nEpoch 3/3 | Loss: 0.0437\n📊 LEVEL 1: Sentiment (Troll vs Support)\n              precision    recall  f1-score   support\n\n     SUPPORT     1.0000    0.9608    0.9800        51\nTROLL/OPPOSE     0.9808    1.0000    0.9903       102\n\n    accuracy                         0.9869       153\n   macro avg     0.9904    0.9804    0.9851       153\nweighted avg     0.9872    0.9869    0.9869       153\n\n📊 LEVEL 2: Target (Person/Party/Intersection)\n              precision    recall  f1-score   support\n\n      PERSON     0.8627    0.7857    0.8224        56\n       PARTY     0.8033    0.9074    0.8522        54\nINTERSECTION     1.0000    0.9535    0.9762        43\n\n    accuracy                         0.8758       153\n   macro avg     0.8887    0.8822    0.8836       153\nweighted avg     0.8803    0.8758    0.8761       153\n\n","output_type":"stream"}],"execution_count":64}]}