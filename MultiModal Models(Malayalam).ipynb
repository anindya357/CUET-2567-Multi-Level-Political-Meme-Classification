{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceType":"datasetVersion","sourceId":14799155,"datasetId":9462504,"databundleVersionId":15654048},{"sourceType":"datasetVersion","sourceId":14993618,"datasetId":9597663,"databundleVersionId":15868037},{"sourceType":"datasetVersion","sourceId":14799017,"datasetId":9462384,"databundleVersionId":15653891},{"sourceType":"datasetVersion","sourceId":14798673,"datasetId":9462109,"databundleVersionId":15653518},{"sourceType":"datasetVersion","sourceId":14798600,"datasetId":9462053,"databundleVersionId":15653432},{"sourceType":"datasetVersion","sourceId":14798990,"datasetId":9462360,"databundleVersionId":15653864},{"sourceType":"datasetVersion","sourceId":14799042,"datasetId":9462405,"databundleVersionId":15653922}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport torch\nfrom transformers import CLIPProcessor, CLIPModel, AutoTokenizer, AutoModel\nimport easyocr\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import Input, Dense, Dropout, Concatenate, BatchNormalization\nfrom tensorflow.keras.models import Model\nfrom sklearn.model_selection import train_test_split","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-28T13:13:17.795069Z","iopub.execute_input":"2026-02-28T13:13:17.795340Z","iopub.status.idle":"2026-02-28T13:13:42.793832Z","shell.execute_reply.started":"2026-02-28T13:13:17.795316Z","shell.execute_reply":"2026-02-28T13:13:42.793040Z"}},"outputs":[{"name":"stderr","text":"2026-02-28 13:13:27.880548: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1772284408.043527      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1772284408.088817      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1772284408.446709      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1772284408.446737      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1772284408.446740      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1772284408.446743      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\n\n\n\nCSV_PATH = \"/kaggle/input/sth-malyalam-datasets/train_malayalam_ocr.csv\"\n\ndf1 = pd.read_csv(CSV_PATH)\ndf2=pd.read_excel('/kaggle/input/malyalam-trainwithlabel/Malayalam_Train_label.xlsx')\ndf=pd.merge(df1,df2, on='meme_id', how='inner')\nprint(df.shape)\ndf.head()\n\nprint(df.shape)\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-28T13:13:42.794973Z","iopub.execute_input":"2026-02-28T13:13:42.795492Z","iopub.status.idle":"2026-02-28T13:13:43.330113Z","shell.execute_reply.started":"2026-02-28T13:13:42.795465Z","shell.execute_reply":"2026-02-28T13:13:43.329521Z"}},"outputs":[{"name":"stdout","text":"(500, 4)\n(500, 4)\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"   meme_id                                      ml_text_clean        Level 1  \\\n0        1  1 . ടം ആജ്‌\\nര്‍ പ പ ദ്ദ; ടി ഗ്യ ഷ്‌ കാടോ ലാ\\n...  TROLL/ OPPOSE   \n1       10  റും ര.\\n7 ല്‍ റ ന പ\\n൫9൭൭2 ന മി\\nടം ന്തര ഞന\\n1...  TROLL/ OPPOSE   \n2      101  വി\\n\\n4 ിജിിര്തിട\\n\\n(95 0790൫5 രി:\\n\\n1) [രിത...  TROLL/ OPPOSE   \n3      102  ടട (ധ്യ 7 ഉ ണ്ണ ു\\nടം; റ്റി നിം യ; ചി റി ൧\\nപം...  TROLL/ OPPOSE   \n4      104  | ആആ്ഞ്ഞ്ഞ്ഞ്തംത്ത്്തഴ് [|\\n)൭(൫70പയല്നുന്ന്ര!...  TROLL/ OPPOSE   \n\n                     Level 2  \n0               Intersection  \n1  Against individual person  \n2              Against party  \n3  Against individual person  \n4  Against individual person  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>meme_id</th>\n      <th>ml_text_clean</th>\n      <th>Level 1</th>\n      <th>Level 2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1 . ടം ആജ്‌\\nര്‍ പ പ ദ്ദ; ടി ഗ്യ ഷ്‌ കാടോ ലാ\\n...</td>\n      <td>TROLL/ OPPOSE</td>\n      <td>Intersection</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>10</td>\n      <td>റും ര.\\n7 ല്‍ റ ന പ\\n൫9൭൭2 ന മി\\nടം ന്തര ഞന\\n1...</td>\n      <td>TROLL/ OPPOSE</td>\n      <td>Against individual person</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>101</td>\n      <td>വി\\n\\n4 ിജിിര്തിട\\n\\n(95 0790൫5 രി:\\n\\n1) [രിത...</td>\n      <td>TROLL/ OPPOSE</td>\n      <td>Against party</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>102</td>\n      <td>ടട (ധ്യ 7 ഉ ണ്ണ ു\\nടം; റ്റി നിം യ; ചി റി ൧\\nപം...</td>\n      <td>TROLL/ OPPOSE</td>\n      <td>Against individual person</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>104</td>\n      <td>| ആആ്ഞ്ഞ്ഞ്ഞ്തംത്ത്്തഴ് [|\\n)൭(൫70പയല്നുന്ന്ര!...</td>\n      <td>TROLL/ OPPOSE</td>\n      <td>Against individual person</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"def Level1_MAP(label):\n    \n    if \"TROLL/ OPPOSE\" in label:\n        return 1\n    elif \"SUPPORT\" in label:\n        return 0\n\n    else :\n        return 'NaN'\n\n\ndef map_level2(label):\n    \n\n    if \"person\" in label:\n        return 0\n    elif \"party\" in label:\n        return 1\n    elif \"Intersection\" in label:\n        return 2\n    else:\n        return -1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-28T13:13:43.330881Z","iopub.execute_input":"2026-02-28T13:13:43.331279Z","iopub.status.idle":"2026-02-28T13:13:43.336023Z","shell.execute_reply.started":"2026-02-28T13:13:43.331255Z","shell.execute_reply":"2026-02-28T13:13:43.335314Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"df['label1']=df['Level 1'].map(Level1_MAP)\ndf['label2']=df[\"Level 2\"].apply(map_level2)\n\nprint(\"Train examples:\", len(df))\n\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-28T13:13:43.337297Z","iopub.execute_input":"2026-02-28T13:13:43.337821Z","iopub.status.idle":"2026-02-28T13:13:43.353794Z","shell.execute_reply.started":"2026-02-28T13:13:43.337797Z","shell.execute_reply":"2026-02-28T13:13:43.353181Z"}},"outputs":[{"name":"stdout","text":"Train examples: 500\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"   meme_id                                      ml_text_clean        Level 1  \\\n0        1  1 . ടം ആജ്‌\\nര്‍ പ പ ദ്ദ; ടി ഗ്യ ഷ്‌ കാടോ ലാ\\n...  TROLL/ OPPOSE   \n1       10  റും ര.\\n7 ല്‍ റ ന പ\\n൫9൭൭2 ന മി\\nടം ന്തര ഞന\\n1...  TROLL/ OPPOSE   \n2      101  വി\\n\\n4 ിജിിര്തിട\\n\\n(95 0790൫5 രി:\\n\\n1) [രിത...  TROLL/ OPPOSE   \n3      102  ടട (ധ്യ 7 ഉ ണ്ണ ു\\nടം; റ്റി നിം യ; ചി റി ൧\\nപം...  TROLL/ OPPOSE   \n4      104  | ആആ്ഞ്ഞ്ഞ്ഞ്തംത്ത്്തഴ് [|\\n)൭(൫70പയല്നുന്ന്ര!...  TROLL/ OPPOSE   \n\n                     Level 2  label1  label2  \n0               Intersection       1       2  \n1  Against individual person       1       0  \n2              Against party       1       1  \n3  Against individual person       1       0  \n4  Against individual person       1       0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>meme_id</th>\n      <th>ml_text_clean</th>\n      <th>Level 1</th>\n      <th>Level 2</th>\n      <th>label1</th>\n      <th>label2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1 . ടം ആജ്‌\\nര്‍ പ പ ദ്ദ; ടി ഗ്യ ഷ്‌ കാടോ ലാ\\n...</td>\n      <td>TROLL/ OPPOSE</td>\n      <td>Intersection</td>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>10</td>\n      <td>റും ര.\\n7 ല്‍ റ ന പ\\n൫9൭൭2 ന മി\\nടം ന്തര ഞന\\n1...</td>\n      <td>TROLL/ OPPOSE</td>\n      <td>Against individual person</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>101</td>\n      <td>വി\\n\\n4 ിജിിര്തിട\\n\\n(95 0790൫5 രി:\\n\\n1) [രിത...</td>\n      <td>TROLL/ OPPOSE</td>\n      <td>Against party</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>102</td>\n      <td>ടട (ധ്യ 7 ഉ ണ്ണ ു\\nടം; റ്റി നിം യ; ചി റി ൧\\nപം...</td>\n      <td>TROLL/ OPPOSE</td>\n      <td>Against individual person</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>104</td>\n      <td>| ആആ്ഞ്ഞ്ഞ്ഞ്തംത്ത്്തഴ് [|\\n)൭(൫70പയല്നുന്ന്ര!...</td>\n      <td>TROLL/ OPPOSE</td>\n      <td>Against individual person</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"df['Level 1'].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-28T13:13:51.467660Z","iopub.execute_input":"2026-02-28T13:13:51.467955Z","iopub.status.idle":"2026-02-28T13:13:51.479056Z","shell.execute_reply.started":"2026-02-28T13:13:51.467929Z","shell.execute_reply":"2026-02-28T13:13:51.478331Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"Level 1\nTROLL/ OPPOSE    477\nSUPPORT           23\nName: count, dtype: int64"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"df['Level 2'].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-28T13:13:57.956864Z","iopub.execute_input":"2026-02-28T13:13:57.957169Z","iopub.status.idle":"2026-02-28T13:13:57.963146Z","shell.execute_reply.started":"2026-02-28T13:13:57.957142Z","shell.execute_reply":"2026-02-28T13:13:57.962453Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"Level 2\nAgainst individual person        315\nAgainst party                    110\nIntersection                      53\nSupport for individual person     12\nSupport for party                 10\nName: count, dtype: int64"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"test_df1=pd.read_csv('/kaggle/input/test-malaylam/test_malayalam_ocr (1).csv')\ntest_df2=pd.read_excel('/kaggle/input/malyalam-test-csv/Malayalam_Test_label.xlsx')\ntest_df=pd.merge(test_df1, test_df2, on='meme_id', how='inner')\ntest_df.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-28T13:14:02.498967Z","iopub.execute_input":"2026-02-28T13:14:02.499689Z","iopub.status.idle":"2026-02-28T13:14:02.554732Z","shell.execute_reply.started":"2026-02-28T13:14:02.499629Z","shell.execute_reply":"2026-02-28T13:14:02.554098Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"   meme_id                                      ml_text_clean  Level 1  \\\n0      100  1111001000\\nകില്‍ ത്ത്‌ ഗള്‍ കം ക ്ന\\nറ ഥി വ 4...      NaN   \n1      103  ) ' 55 ഹ്ന... ടാം പി\\nടാ ടം ത്തത്‌\\n; ക ത ന ക ...      NaN   \n2      106  1.\\nവ ്ു വ ി . & യ ക\\nലി സ്സ [ പ്ര ി ബ്ല യ നടന...      NaN   \n3      107  ന 2011)0200)0/7/11001/01\\nയിട്ടു ദിനം.\\nയി ാ\\n...      NaN   \n4      122  യല്ല\\nലം ഴു്‌ ലാ ്്‌ ||). [|\\n1; റിക ഷ്‌ മി 14...      NaN   \n\n   Level 2  \n0      NaN  \n1      NaN  \n2      NaN  \n3      NaN  \n4      NaN  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>meme_id</th>\n      <th>ml_text_clean</th>\n      <th>Level 1</th>\n      <th>Level 2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>100</td>\n      <td>1111001000\\nകില്‍ ത്ത്‌ ഗള്‍ കം ക ്ന\\nറ ഥി വ 4...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>103</td>\n      <td>) ' 55 ഹ്ന... ടാം പി\\nടാ ടം ത്തത്‌\\n; ക ത ന ക ...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>106</td>\n      <td>1.\\nവ ്ു വ ി . &amp; യ ക\\nലി സ്സ [ പ്ര ി ബ്ല യ നടന...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>107</td>\n      <td>ന 2011)0200)0/7/11001/01\\nയിട്ടു ദിനം.\\nയി ാ\\n...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>122</td>\n      <td>യല്ല\\nലം ഴു്‌ ലാ ്്‌ ||). [|\\n1; റിക ഷ്‌ മി 14...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"import cv2\nimport os\nimport pandas as pd\n\ndef load_images_to_dataframe(image_dir):\n\n    data = []\n\n\n    for file_name in os.listdir(image_dir):\n        file_path = os.path.join(image_dir, file_name)\n        \n\n        if file_name.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff')):\n\n            img = cv2.imread(file_path)\n            \n\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n            \n\n            img = cv2.resize(img, (224, 224), interpolation=cv2.INTER_AREA)\n            \n\n            alpha = 1.2 \n            beta = 20   \n            img = cv2.convertScaleAbs(img, alpha=alpha, beta=beta)\n\n\n            image_id = int(os.path.splitext(file_name)[0])\n            \n            # Append image data and file name to the list\n            data.append({\n                'meme_id': image_id,\n                'Image_name': img\n            })\n\n\n    df = pd.DataFrame(data)\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-28T13:14:07.022827Z","iopub.execute_input":"2026-02-28T13:14:07.023124Z","iopub.status.idle":"2026-02-28T13:14:07.029045Z","shell.execute_reply.started":"2026-02-28T13:14:07.023098Z","shell.execute_reply":"2026-02-28T13:14:07.028328Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"df_train= load_images_to_dataframe('/kaggle/input/full-train-malyalam/Train/Train_images')\ndf_test = load_images_to_dataframe('/kaggle/input/test-images/Test_images')\ntrain = pd.merge(df, df_train, on='meme_id')\ntest = pd.merge(test_df, df_test, on='meme_id')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-28T13:14:13.833065Z","iopub.execute_input":"2026-02-28T13:14:13.833358Z","iopub.status.idle":"2026-02-28T13:14:29.349402Z","shell.execute_reply.started":"2026-02-28T13:14:13.833332Z","shell.execute_reply":"2026-02-28T13:14:29.348324Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-28T13:14:29.350859Z","iopub.execute_input":"2026-02-28T13:14:29.351504Z","iopub.status.idle":"2026-02-28T13:14:35.648139Z","shell.execute_reply.started":"2026-02-28T13:14:29.351464Z","shell.execute_reply":"2026-02-28T13:14:35.647224Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"   meme_id                                      ml_text_clean        Level 1  \\\n0        1  1 . ടം ആജ്‌\\nര്‍ പ പ ദ്ദ; ടി ഗ്യ ഷ്‌ കാടോ ലാ\\n...  TROLL/ OPPOSE   \n1       10  റും ര.\\n7 ല്‍ റ ന പ\\n൫9൭൭2 ന മി\\nടം ന്തര ഞന\\n1...  TROLL/ OPPOSE   \n2      101  വി\\n\\n4 ിജിിര്തിട\\n\\n(95 0790൫5 രി:\\n\\n1) [രിത...  TROLL/ OPPOSE   \n3      102  ടട (ധ്യ 7 ഉ ണ്ണ ു\\nടം; റ്റി നിം യ; ചി റി ൧\\nപം...  TROLL/ OPPOSE   \n4      104  | ആആ്ഞ്ഞ്ഞ്ഞ്തംത്ത്്തഴ് [|\\n)൭(൫70പയല്നുന്ന്ര!...  TROLL/ OPPOSE   \n\n                     Level 2  label1  label2  \\\n0               Intersection       1       2   \n1  Against individual person       1       0   \n2              Against party       1       1   \n3  Against individual person       1       0   \n4  Against individual person       1       0   \n\n                                          Image_name  \n0  [[[20, 20, 20], [20, 20, 20], [20, 20, 20], [2...  \n1  [[[52, 57, 49], [56, 60, 51], [62, 64, 57], [7...  \n2  [[[28, 28, 28], [28, 28, 28], [28, 28, 28], [2...  \n3  [[[20, 20, 20], [20, 20, 20], [20, 20, 20], [2...  \n4  [[[55, 55, 55], [55, 55, 55], [55, 55, 55], [5...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>meme_id</th>\n      <th>ml_text_clean</th>\n      <th>Level 1</th>\n      <th>Level 2</th>\n      <th>label1</th>\n      <th>label2</th>\n      <th>Image_name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1 . ടം ആജ്‌\\nര്‍ പ പ ദ്ദ; ടി ഗ്യ ഷ്‌ കാടോ ലാ\\n...</td>\n      <td>TROLL/ OPPOSE</td>\n      <td>Intersection</td>\n      <td>1</td>\n      <td>2</td>\n      <td>[[[20, 20, 20], [20, 20, 20], [20, 20, 20], [2...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>10</td>\n      <td>റും ര.\\n7 ല്‍ റ ന പ\\n൫9൭൭2 ന മി\\nടം ന്തര ഞന\\n1...</td>\n      <td>TROLL/ OPPOSE</td>\n      <td>Against individual person</td>\n      <td>1</td>\n      <td>0</td>\n      <td>[[[52, 57, 49], [56, 60, 51], [62, 64, 57], [7...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>101</td>\n      <td>വി\\n\\n4 ിജിിര്തിട\\n\\n(95 0790൫5 രി:\\n\\n1) [രിത...</td>\n      <td>TROLL/ OPPOSE</td>\n      <td>Against party</td>\n      <td>1</td>\n      <td>1</td>\n      <td>[[[28, 28, 28], [28, 28, 28], [28, 28, 28], [2...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>102</td>\n      <td>ടട (ധ്യ 7 ഉ ണ്ണ ു\\nടം; റ്റി നിം യ; ചി റി ൧\\nപം...</td>\n      <td>TROLL/ OPPOSE</td>\n      <td>Against individual person</td>\n      <td>1</td>\n      <td>0</td>\n      <td>[[[20, 20, 20], [20, 20, 20], [20, 20, 20], [2...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>104</td>\n      <td>| ആആ്ഞ്ഞ്ഞ്ഞ്തംത്ത്്തഴ് [|\\n)൭(൫70പയല്നുന്ന്ര!...</td>\n      <td>TROLL/ OPPOSE</td>\n      <td>Against individual person</td>\n      <td>1</td>\n      <td>0</td>\n      <td>[[[55, 55, 55], [55, 55, 55], [55, 55, 55], [5...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"!pip install git+https://github.com/indic-transliteration/indic_transliteration_py/@master","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-28T13:14:52.515374Z","iopub.execute_input":"2026-02-28T13:14:52.516094Z","iopub.status.idle":"2026-02-28T13:15:01.173410Z","shell.execute_reply.started":"2026-02-28T13:14:52.516051Z","shell.execute_reply":"2026-02-28T13:15:01.172388Z"}},"outputs":[{"name":"stdout","text":"Collecting git+https://github.com/indic-transliteration/indic_transliteration_py/@master\n  Cloning https://github.com/indic-transliteration/indic_transliteration_py/ (to revision master) to /tmp/pip-req-build-9tgg6dst\n  Running command git clone --filter=blob:none --quiet https://github.com/indic-transliteration/indic_transliteration_py/ /tmp/pip-req-build-9tgg6dst\n  Resolved https://github.com/indic-transliteration/indic_transliteration_py/ to commit 322e88533190a874743d72853e3991cbd0e1c076\n  Running command git submodule update --init --recursive -q\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting backports.functools_lru_cache (from indic_transliteration==2.3.79)\n  Downloading backports.functools_lru_cache-2.0.0-py2.py3-none-any.whl.metadata (3.5 kB)\nRequirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from indic_transliteration==2.3.79) (2025.11.3)\nRequirement already satisfied: typer in /usr/local/lib/python3.12/dist-packages (from indic_transliteration==2.3.79) (0.20.0)\nRequirement already satisfied: toml in /usr/local/lib/python3.12/dist-packages (from indic_transliteration==2.3.79) (0.10.2)\nCollecting roman (from indic_transliteration==2.3.79)\n  Downloading roman-5.2-py3-none-any.whl.metadata (4.3 kB)\nRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer->indic_transliteration==2.3.79) (8.3.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from typer->indic_transliteration==2.3.79) (4.15.0)\nRequirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer->indic_transliteration==2.3.79) (1.5.4)\nRequirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer->indic_transliteration==2.3.79) (14.2.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer->indic_transliteration==2.3.79) (4.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer->indic_transliteration==2.3.79) (2.19.2)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer->indic_transliteration==2.3.79) (0.1.2)\nDownloading backports.functools_lru_cache-2.0.0-py2.py3-none-any.whl (6.7 kB)\nDownloading roman-5.2-py3-none-any.whl (6.0 kB)\nBuilding wheels for collected packages: indic_transliteration\n  Building wheel for indic_transliteration (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for indic_transliteration: filename=indic_transliteration-2.3.79-py3-none-any.whl size=164285 sha256=76a3b44de90ae7548ea09d4fdeca985ab2c8ddd1336f0a70710e5dd2ad65779c\n  Stored in directory: /tmp/pip-ephem-wheel-cache-4oxn_sq_/wheels/ef/dd/4d/c621f9f27e4ec9e07c73a5614faa96a12a4cc1e94bf05f069b\nSuccessfully built indic_transliteration\nInstalling collected packages: roman, backports.functools_lru_cache, indic_transliteration\nSuccessfully installed backports.functools_lru_cache-2.0.0 indic_transliteration-2.3.79 roman-5.2\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"def text_preprocessing(text):\n    import re\n    pattern = re.compile('[@#\\/]\\S+')\n    text = pattern.sub(r'',text)\n\n    pattern = re.compile('\\d+')\n    text = pattern.sub(r'', text)\n\n    pattern = re.compile(r'https?:\\/\\/\\S+|www\\.\\S+|ftp:\\/\\/\\S+|mailto:\\S+|https?:')\n\n    # First remove URLs\n    text = pattern.sub('', text)\n\n    # Remove newline characters (\\n) and carriage returns (\\r)\n    text = text.replace('\\n', ' ').replace('\\r', '')\n\n    # Remove extra spaces (including multiple spaces)\n    text = re.sub(r'\\s+', ' ', text).strip()\n\n    import string\n    punc = string.punctuation\n\n    text = text.translate(str.maketrans('','',punc))\n\n    emoji_pattern = re.compile(\n        \"[\"\n        \"\\U0001F600-\\U0001F64F\"  # Emoticons\n        \"\\U0001F300-\\U0001F5FF\"  # Symbols & Pictographs\n        \"\\U0001F680-\\U0001F6FF\"  # Transport & Map Symbols\n        \"\\U0001F1E0-\\U0001F1FF\"  # Flags (iOS)\n        \"\\U00002700-\\U000027BF\"  # Dingbats\n        \"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n        \"\\U00002600-\\U000026FF\"  # Miscellaneous Symbols\n        \"\\U00002B50-\\U00002B55\"  # Stars and other symbols\n        \"]+\",\n        flags=re.UNICODE\n    )\n\n    text = emoji_pattern.sub(r'', text)\n\n    ml_stopwords = [\"ഒരു\", \"എന്ന്\", \"മറ്റും\", \"ഈ\", \"ഇത്\", \"എന്ന\", \"കൊണ്ട്\", \"എന്നത്\", \"പല\", \"ആണ്\",\n\"അല്ലെങ്കിൽ\", \"അവൻ\", \"ഞാൻ\", \"ഉള്ള\", \"ആ\", \"ഇവൻ\", \"എന്നാൽ\", \"ആദ്യം\", \"എന്ത്\", \"നിന്ന്\",\n\"ചില\", \"എന്റെ\", \"പോലെ\", \"വേണ്ടി\", \"വന്ന്\", \"ഇതിന്റെ\", \"അത്\", \"അവൾ\", \"തന്നെ\", \"പലരും\",\n\"എന്നും\", \"കൂടാതെ\", \"ശേഷം\", \"കொண்ட\", \"ഇരിക്കും\", \"തന്റെ\", \"ഉണ്ട്\", \"സമയം\", \"എപ്പോഴും\",\n\"അതിന്റെ\", \"തൻ\", \"പിന്നീട്\", \"അവർ\", \"വരെ\", \"നീ\", \"ആയ\", \"ഇരുന്നു\", \"ഉണ്ടായിരുന്നു\",\n\"വന്ന\", \"ഇരുന്ന\", \"വളരെ\", \"ഇവിടെ\", \"മേൽ\", \"ഒരു\", \"ഇവ\", \"ഈ\", \"കുറിച്ച്\", \"വരും\",\n\"മറ്റൊരു\", \"ഇരു\", \"ഇതിൽ\", \"പോലെ\", \"ഇപ്പോൾ\", \"അവന്റെ\", \"മാത്രം\", \"ഈ\", \"എന്നുള്ള\", \"മുകളിൽ\",\n\"ശേഷം\", \"ചേർന്ന\", \"എനിക്ക്\", \"ഇനിയും\", \"ആ ദിവസം\", \"ഒരേ\", \"വളരെയേറെ\", \"അവിടെ\",\n\"പലവിധ\", \"വിട്ട്\", \"വലിയ\", \"അதை\", \"കുറിച്ചുള്ള\", \"നിന്റെ\", \"കൂടുതൽ\", \"പേര്\", \"ഇതിനാൽ\",\n\"അവ\", \"അതേ\", \"എന്തുകൊണ്ട്\", \"രീതി\", \"ആർ\", \"എന്നതിനെ\", \"എല്ലാം\", \"മാത്രമേ\", \"ഇവിടെ\", \"അവിടെ\",\n\"സ്ഥലം\", \"സ്ഥലത്ത്\", \"അതിൽ\", \"നാം\", \"അതിനു\", \"അതുകൊണ്ട്\", \"മറ്റു\", \"ചെറിയ\", \"വിട്ടു\", \"ഏത്\",\n\"എന്നുവെച്ച്\", \"എന്നറിയപ്പെടുന്ന\", \"എങ്കിലും\", \"അടുത്ത\", \"ഇതിനെ\", \"ഇത്\", \"എടുക്കാൻ\", \"ഇതിന്\",\n\"അതുകൊണ്ട്\", \"ഒഴികെ\", \"പോലും\", \"വരെ\", \"കുറച്ച്\", \"എനിക്ക്\"]\n                                                                                              \n\n    \n    text_ls = text.split()\n    filtered_words = [word for word in text_ls if word not in ml_stopwords]\n    # Join the remaining words back into a string\n    text = \" \".join(filtered_words)\n\n    from indic_transliteration import sanscript\n    from indic_transliteration.sanscript import SchemeMap, SCHEMES, transliterate\n    \n    \n    text = transliterate(text, sanscript.HK, sanscript.TAMIL)\n\n\n    \n    return text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-28T13:15:12.501866Z","iopub.execute_input":"2026-02-28T13:15:12.502732Z","iopub.status.idle":"2026-02-28T13:15:12.513662Z","shell.execute_reply.started":"2026-02-28T13:15:12.502690Z","shell.execute_reply":"2026-02-28T13:15:12.512687Z"}},"outputs":[{"name":"stderr","text":"<>:3: SyntaxWarning: invalid escape sequence '\\/'\n<>:6: SyntaxWarning: invalid escape sequence '\\d'\n<>:3: SyntaxWarning: invalid escape sequence '\\/'\n<>:6: SyntaxWarning: invalid escape sequence '\\d'\n/tmp/ipykernel_55/4251860584.py:3: SyntaxWarning: invalid escape sequence '\\/'\n  pattern = re.compile('[@#\\/]\\S+')\n/tmp/ipykernel_55/4251860584.py:6: SyntaxWarning: invalid escape sequence '\\d'\n  pattern = re.compile('\\d+')\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"train=train.rename(columns={'ml_text_clean': 'text'})\ntest= test.rename(columns={'ml_text_clean' : 'text'})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-28T13:15:18.026575Z","iopub.execute_input":"2026-02-28T13:15:18.026881Z","iopub.status.idle":"2026-02-28T13:15:18.033302Z","shell.execute_reply.started":"2026-02-28T13:15:18.026854Z","shell.execute_reply":"2026-02-28T13:15:18.032507Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"train['text'] = train['text'].fillna('').apply(text_preprocessing)\ntest['text'] = test['text'].fillna('').apply(text_preprocessing)\ntrain['text'] = train['text'].apply(text_preprocessing)\ntest['text'] = test['text'].apply(text_preprocessing)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-28T13:15:24.125200Z","iopub.execute_input":"2026-02-28T13:15:24.126222Z","iopub.status.idle":"2026-02-28T13:15:25.906884Z","shell.execute_reply.started":"2026-02-28T13:15:24.126175Z","shell.execute_reply":"2026-02-28T13:15:25.906120Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-28T13:15:29.163281Z","iopub.execute_input":"2026-02-28T13:15:29.163863Z","iopub.status.idle":"2026-02-28T13:15:35.379868Z","shell.execute_reply.started":"2026-02-28T13:15:29.163834Z","shell.execute_reply":"2026-02-28T13:15:35.379114Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"   meme_id                                               text        Level 1  \\\n0        1  ടം ആജ്‌ ര്‍ പ പ ദ്ദ ടി ഗ്യ ഷ്‌ കാടോ ലാ പ പ ടം ...  TROLL/ OPPOSE   \n1       10  റും ര ല്‍ റ ന പ ന മി ടം ന്തര ഞന ്‌ പര്‍ ല്‌ ലി...  TROLL/ OPPOSE   \n2      101  വി ിജിിര്തിട രി രിത്തുതിള്തി സ്‌ ി കസ്യിലാണ്‌ ...  TROLL/ OPPOSE   \n3      102  ടട ധ്യ ഉ ണ്ണ ു ടം റ്റി നിം യ ചി റി പം പു ര്‍ ല...  TROLL/ OPPOSE   \n4      104  ആആ്ഞ്ഞ്ഞ്ഞ്തംത്ത്്തഴ് പയല്നുന്ന്ര ത്ഞ്ത്ൃതന്ത്...  TROLL/ OPPOSE   \n\n                     Level 2  label1  label2  \\\n0               Intersection       1       2   \n1  Against individual person       1       0   \n2              Against party       1       1   \n3  Against individual person       1       0   \n4  Against individual person       1       0   \n\n                                          Image_name  \n0  [[[20, 20, 20], [20, 20, 20], [20, 20, 20], [2...  \n1  [[[52, 57, 49], [56, 60, 51], [62, 64, 57], [7...  \n2  [[[28, 28, 28], [28, 28, 28], [28, 28, 28], [2...  \n3  [[[20, 20, 20], [20, 20, 20], [20, 20, 20], [2...  \n4  [[[55, 55, 55], [55, 55, 55], [55, 55, 55], [5...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>meme_id</th>\n      <th>text</th>\n      <th>Level 1</th>\n      <th>Level 2</th>\n      <th>label1</th>\n      <th>label2</th>\n      <th>Image_name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>ടം ആജ്‌ ര്‍ പ പ ദ്ദ ടി ഗ്യ ഷ്‌ കാടോ ലാ പ പ ടം ...</td>\n      <td>TROLL/ OPPOSE</td>\n      <td>Intersection</td>\n      <td>1</td>\n      <td>2</td>\n      <td>[[[20, 20, 20], [20, 20, 20], [20, 20, 20], [2...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>10</td>\n      <td>റും ര ല്‍ റ ന പ ന മി ടം ന്തര ഞന ്‌ പര്‍ ല്‌ ലി...</td>\n      <td>TROLL/ OPPOSE</td>\n      <td>Against individual person</td>\n      <td>1</td>\n      <td>0</td>\n      <td>[[[52, 57, 49], [56, 60, 51], [62, 64, 57], [7...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>101</td>\n      <td>വി ിജിിര്തിട രി രിത്തുതിള്തി സ്‌ ി കസ്യിലാണ്‌ ...</td>\n      <td>TROLL/ OPPOSE</td>\n      <td>Against party</td>\n      <td>1</td>\n      <td>1</td>\n      <td>[[[28, 28, 28], [28, 28, 28], [28, 28, 28], [2...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>102</td>\n      <td>ടട ധ്യ ഉ ണ്ണ ു ടം റ്റി നിം യ ചി റി പം പു ര്‍ ല...</td>\n      <td>TROLL/ OPPOSE</td>\n      <td>Against individual person</td>\n      <td>1</td>\n      <td>0</td>\n      <td>[[[20, 20, 20], [20, 20, 20], [20, 20, 20], [2...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>104</td>\n      <td>ആആ്ഞ്ഞ്ഞ്ഞ്തംത്ത്്തഴ് പയല്നുന്ന്ര ത്ഞ്ത്ൃതന്ത്...</td>\n      <td>TROLL/ OPPOSE</td>\n      <td>Against individual person</td>\n      <td>1</td>\n      <td>0</td>\n      <td>[[[55, 55, 55], [55, 55, 55], [55, 55, 55], [5...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"!pip install deep-translator","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-28T13:16:18.677752Z","iopub.execute_input":"2026-02-28T13:16:18.678147Z","iopub.status.idle":"2026-02-28T13:16:22.146926Z","shell.execute_reply.started":"2026-02-28T13:16:18.678111Z","shell.execute_reply":"2026-02-28T13:16:22.145981Z"}},"outputs":[{"name":"stdout","text":"Collecting deep-translator\n  Downloading deep_translator-1.11.4-py3-none-any.whl.metadata (30 kB)\nRequirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /usr/local/lib/python3.12/dist-packages (from deep-translator) (4.13.5)\nRequirement already satisfied: requests<3.0.0,>=2.23.0 in /usr/local/lib/python3.12/dist-packages (from deep-translator) (2.32.5)\nRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (2.8)\nRequirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (4.15.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2.6.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2026.1.4)\nDownloading deep_translator-1.11.4-py3-none-any.whl (42 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: deep-translator\nSuccessfully installed deep-translator-1.11.4\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"print(\"Distribution for Label1:\")\nprint(train['label1'].value_counts())\n\n# Check counts for label2 (Sub-category)\nprint(\"\\nDistribution for Label2:\")\nprint(train['label2'].value_counts())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-28T13:15:50.900480Z","iopub.execute_input":"2026-02-28T13:15:50.900875Z","iopub.status.idle":"2026-02-28T13:15:50.907224Z","shell.execute_reply.started":"2026-02-28T13:15:50.900844Z","shell.execute_reply":"2026-02-28T13:15:50.906443Z"}},"outputs":[{"name":"stdout","text":"Distribution for Label1:\nlabel1\n1    477\n0     23\nName: count, dtype: int64\n\nDistribution for Label2:\nlabel2\n0    327\n1    120\n2     53\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"from PIL import Image\nfrom torchvision import transforms\nfrom deep_translator import GoogleTranslator\nimport numpy as np\nimport pandas as pd\nimport time\n\naugmented_data = []\ncnt = 0\n\n# Image augmentation pipeline\nimg_augmentations = transforms.Compose([\n    transforms.ColorJitter(brightness=0.4, contrast=0.3),\n    transforms.RandomGrayscale(p=0.1),\n    transforms.RandomRotation(10), # Small rotation for variety\n])\n\n# Iterate through the Malayalam 'train' dataframe\nfor idx, row in train.iterrows():\n    # Updated Column Names: meme_id, Image_name (pixels), text\n    image_data = row['Image_name'] \n    text_content = row['text']\n    l1 = row['label1']\n    l2 = row['label2']\n    meme_id = row['meme_id']\n\n    # --- STRATEGY: Target the extreme minorities ---\n    # We augment if Label 1 is 0 OR Label 2 is 2 (Intersection)\n    if l1 == 0 or l2 == 2:\n        cnt += 1\n        if cnt % 5 == 0: print(f\"Augmenting Malayalam Minority {cnt}...\")\n\n        image_pil = Image.fromarray(np.uint8(image_data))\n        \n        # We will create 4 variations for each minority row to bridge the gap\n        # Languages: English, Tamil, Hindi, Kannada (Dravidian neighbor)\n        target_langs = ['en', 'ta', 'hi', 'kn']\n        \n        for lang in target_langs:\n            # 1. Image Transform\n            img_aug = np.array(img_augmentations(image_pil))\n\n            # 2. Back-Translation (Malayalam -> Lang -> Malayalam)\n            try:\n                translated = GoogleTranslator(source='ml', target=lang).translate(text_content)\n                time.sleep(0.5) # Safety to avoid 429 Error\n                text_aug = GoogleTranslator(source=lang, target='ml').translate(translated)\n            except:\n                text_aug = text_content # Fallback to original text\n\n            augmented_data.append({\n                'meme_id': f\"{meme_id}_aug_{lang}\",\n                'text': text_aug,\n                'label1': l1,\n                'label2': l2,\n                'Image_name': img_aug\n            })\n\n# Combine and check\naug_df = pd.DataFrame(augmented_data)\ntrain = pd.concat([train, aug_df], ignore_index=True)\n\nprint(\"\\n--- After Augmentation Distribution ---\")\nprint(\"Label 1:\\n\", train['label1'].value_counts())\nprint(\"Label 2:\\n\", train['label2'].value_counts())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-28T13:16:32.193506Z","iopub.execute_input":"2026-02-28T13:16:32.194302Z","iopub.status.idle":"2026-02-28T13:28:20.229901Z","shell.execute_reply.started":"2026-02-28T13:16:32.194248Z","shell.execute_reply":"2026-02-28T13:28:20.229156Z"}},"outputs":[{"name":"stdout","text":"Augmenting Malayalam Minority 5...\nAugmenting Malayalam Minority 10...\nAugmenting Malayalam Minority 15...\nAugmenting Malayalam Minority 20...\nAugmenting Malayalam Minority 25...\nAugmenting Malayalam Minority 30...\nAugmenting Malayalam Minority 35...\nAugmenting Malayalam Minority 40...\nAugmenting Malayalam Minority 45...\nAugmenting Malayalam Minority 50...\nAugmenting Malayalam Minority 55...\nAugmenting Malayalam Minority 60...\nAugmenting Malayalam Minority 65...\nAugmenting Malayalam Minority 70...\nAugmenting Malayalam Minority 75...\n\n--- After Augmentation Distribution ---\nLabel 1:\n label1\n1    685\n0    115\nName: count, dtype: int64\nLabel 2:\n label2\n0    375\n2    265\n1    160\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"# 1. Isolate the \"Support\" memes that are NOT yet over-augmented\n# Focus on those where label2 is 1 or 2 to keep the whole system balanced\ntarget_rows = train[(train['label1'] == 0) & (train['label2'] != 0)].copy()\n\n# 2. If we don't have enough of those, take any label1 == 0\nif len(target_rows) < 50:\n    target_rows = train[train['label1'] == 0].copy()\n\nmore_balanced_data = []\n# Use 4 variations per row to add ~400+ samples\nfinal_langs = ['en', 'ta', 'kn', 'te'] \n\nprint(f\"Expanding {len(target_rows)} Support memes...\")\n\nfor idx, row in target_rows.iterrows():\n    img_pil = Image.fromarray(np.uint8(row['Image_name'])).convert(\"RGB\")\n    \n    for lang in final_langs:\n        try:\n            # Back-translate\n            trans = GoogleTranslator(source='ml', target=lang).translate(row['text'])\n            time.sleep(0.4)\n            text_aug = GoogleTranslator(source=lang, target='ml').translate(trans)\n            \n            more_balanced_data.append({\n                'meme_id': f\"{row['meme_id']}_final_{lang}\",\n                'text': text_aug,\n                'label1': 0,\n                'label2': row['label2'],\n                'Image_name': np.array(img_augmentations(img_pil))\n            })\n        except:\n            continue\n\n# 3. Merge and Check\ntrain = pd.concat([train, pd.DataFrame(more_balanced_data)], ignore_index=True)\n\nprint(\"\\n--- FINAL DISTRIBUTION ---\")\nprint(\"Label 1:\\n\", train['label1'].value_counts())\nprint(\"Label 2:\\n\", train['label2'].value_counts())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-28T13:28:27.016515Z","iopub.execute_input":"2026-02-28T13:28:27.017100Z","iopub.status.idle":"2026-02-28T13:36:00.037868Z","shell.execute_reply.started":"2026-02-28T13:28:27.017069Z","shell.execute_reply":"2026-02-28T13:36:00.037123Z"}},"outputs":[{"name":"stdout","text":"Expanding 55 Support memes...\n\n--- FINAL DISTRIBUTION ---\nLabel 1:\n label1\n1    685\n0    335\nName: count, dtype: int64\nLabel 2:\n label2\n0    375\n1    360\n2    285\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"# 1. Target a balanced count (around 700 per class)\ntarget_count = 700\n\n# 2. Downsample Label 1 = 0\ndf_l1_0 = train[train['label1'] == 0]\nif len(df_l1_0) > target_count:\n    df_l1_0 = df_l1_0.sample(n=target_count, random_state=42)\n\n# 3. Keep all of Label 1 = 1 (since it has 685)\ndf_l1_1 = train[train['label1'] == 1]\n\n# 4. Combine into a final balanced training set\ntrain_balanced = pd.concat([df_l1_0, df_l1_1]).sample(frac=1, random_state=42).reset_index(drop=True)\n\nprint(\"--- FINAL OPTIMIZED DISTRIBUTION ---\")\nprint(train_balanced['label1'].value_counts())\nprint(train_balanced['label2'].value_counts())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-28T13:36:12.147514Z","iopub.execute_input":"2026-02-28T13:36:12.148396Z","iopub.status.idle":"2026-02-28T13:36:12.159363Z","shell.execute_reply.started":"2026-02-28T13:36:12.148362Z","shell.execute_reply":"2026-02-28T13:36:12.158733Z"}},"outputs":[{"name":"stdout","text":"--- FINAL OPTIMIZED DISTRIBUTION ---\nlabel1\n1    685\n0    335\nName: count, dtype: int64\nlabel2\n0    375\n1    360\n2    285\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"print(\"Distribution for Label1:\")\nprint(train_balanced['label1'].value_counts())\nprint(train_balanced['label2'].value_counts())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-28T13:36:18.286744Z","iopub.execute_input":"2026-02-28T13:36:18.287457Z","iopub.status.idle":"2026-02-28T13:36:18.292532Z","shell.execute_reply.started":"2026-02-28T13:36:18.287425Z","shell.execute_reply":"2026-02-28T13:36:18.291788Z"}},"outputs":[{"name":"stdout","text":"Distribution for Label1:\nlabel1\n1    685\n0    335\nName: count, dtype: int64\nlabel2\n0    375\n1    360\n2    285\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Create a combined key for stratification to keep both labels balanced in the split\nstratify_key = train_balanced[\"label1\"].astype(str) + \"_\" + train_balanced[\"label2\"].astype(str)\n\n# Split the balanced dataframe (85% for training, 15% for validation)\ndf_train, df_val = train_test_split(\n    train_balanced, \n    test_size=0.15, \n    random_state=42, \n    stratify=stratify_key\n)\n\nprint(f\"Training samples: {len(df_train)}\")\nprint(f\"Validation samples: {len(df_val)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-28T13:36:21.890314Z","iopub.execute_input":"2026-02-28T13:36:21.891059Z","iopub.status.idle":"2026-02-28T13:36:21.901990Z","shell.execute_reply.started":"2026-02-28T13:36:21.891024Z","shell.execute_reply":"2026-02-28T13:36:21.901280Z"}},"outputs":[{"name":"stdout","text":"Training samples: 867\nValidation samples: 153\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport torch\nfrom transformers import CLIPProcessor, CLIPModel, AutoTokenizer, AutoModel\nimport easyocr\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import Input, Dense, Dropout, Concatenate, BatchNormalization\nfrom tensorflow.keras.models import Model\nfrom sklearn.model_selection import train_test_split","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-24T07:22:34.448787Z","iopub.execute_input":"2026-02-24T07:22:34.449368Z","iopub.status.idle":"2026-02-24T07:22:34.453779Z","shell.execute_reply.started":"2026-02-24T07:22:34.449337Z","shell.execute_reply":"2026-02-24T07:22:34.453081Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"train_balanced=train_balanced.dropna()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-28T13:36:38.910375Z","iopub.execute_input":"2026-02-28T13:36:38.910681Z","iopub.status.idle":"2026-02-28T13:36:38.915854Z","shell.execute_reply.started":"2026-02-28T13:36:38.910636Z","shell.execute_reply":"2026-02-28T13:36:38.915236Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"import torch\nfrom transformers import BertTokenizer, BertModel, ViTModel, ViTFeatureExtractor\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\nimport torch.optim as optim\nfrom PIL import Image\n\nclass MultiModalDataset(Dataset):\n    def __init__(self, df, tokenizer, feature_extractor):\n        self.df = df.reset_index(drop=True)\n        self.tokenizer = tokenizer\n        self.feature_extractor = feature_extractor\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        text = str(self.df.loc[idx, 'text'])\n        image_arr = self.df.loc[idx, 'Image_name'] \n        l1 = self.df.loc[idx, 'label1']\n        l2 = self.df.loc[idx, 'label2']\n\n        encoding = self.tokenizer(text, padding='max_length', truncation=True, max_length=128, return_tensors='pt')\n        \n        # Image processing\n        image = Image.fromarray(image_arr.astype('uint8')).convert(\"RGB\")\n        pixel_values = self.feature_extractor(images=image, return_tensors='pt')['pixel_values'].squeeze(0)\n\n        # Return DICTIONARY\n        return {\n            'input_ids': encoding['input_ids'].squeeze(0),\n            'attention_mask': encoding['attention_mask'].squeeze(0),\n            'pixel_values': pixel_values,\n            'label1': torch.tensor(l1, dtype=torch.long),\n            'label2': torch.tensor(l2, dtype=torch.long)\n        }   \n\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\nbert_model = BertModel.from_pretrained('bert-base-multilingual-cased')\n\nfeature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224')\nvit_model = ViTModel.from_pretrained('google/vit-base-patch16-224')\n\n\nclass MultiModal(nn.Module):\n    def __init__(self, bert_model, vit_model, num_classes_l1, num_classes_l2):\n        super(MultiModal, self).__init__()\n        self.bert_model = bert_model\n        self.vit_model = vit_model\n        \n        # Combined feature size is 768 (BERT) + 768 (ViT) = 1536\n        self.classifier_l1 = nn.Linear(1536, num_classes_l1)\n        self.classifier_l2 = nn.Linear(1536, num_classes_l2)\n        self.dropout = nn.Dropout(0.3)\n\n    def forward(self, input_ids, attention_mask, pixel_values):\n        # Text features from [CLS] token\n        bert_output = self.bert_model(input_ids=input_ids, attention_mask=attention_mask)\n        text_features = bert_output.pooler_output # Better than mean for BERT-base\n\n        # Vision features from [CLS] token\n        vit_output = self.vit_model(pixel_values=pixel_values)\n        vision_features = vit_output.pooler_output \n\n        # Fusion\n        combined = torch.cat((text_features, vision_features), dim=1)\n        combined = self.dropout(combined)\n        \n        logits_l1 = self.classifier_l1(combined)\n        logits_l2 = self.classifier_l2(combined)\n        \n        return logits_l1, logits_l2\n\n\ntrain_dataset = MultiModalDataset(df_train, tokenizer, feature_extractor)\ndev_dataset = MultiModalDataset(df_val, tokenizer, feature_extractor)\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\ndev_loader = DataLoader(dev_dataset, batch_size=16)\n\n\n# Initialize\nnum_l1 = 2 # Troll/Support\nnum_l2 = 3 # Person/Party\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = MultiModal(bert_model, vit_model, num_l1, num_l2).to(device)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.AdamW(model.parameters(), lr=1e-4) # AdamW is better for BERT/ViT\n\n\nfor epoch in range(10): # 10 is usually enough for fine-tuning\n  model.train()\n  total_loss = 0\n  for batch in train_loader:\n        # Now batch is a dictionary!\n        ids = batch['input_ids'].to(device)\n        mask = batch['attention_mask'].to(device)\n        pixels = batch['pixel_values'].to(device)\n        lab1 = batch['label1'].to(device)\n        lab2 = batch['label2'].to(device)\n\n        optimizer.zero_grad()\n        # Ensure your model forward function returns both out1 and out2\n        out1, out2 = model(ids, mask, pixels)\n        \n        loss = criterion(out1, lab1) + criterion(out2, lab2)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    \n  print(f\"Epoch {epoch+1} | Loss: {total_loss/len(train_loader):.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-28T13:36:43.738752Z","iopub.execute_input":"2026-02-28T13:36:43.739371Z","iopub.status.idle":"2026-02-28T13:46:49.628023Z","shell.execute_reply.started":"2026-02-28T13:36:43.739340Z","shell.execute_reply":"2026-02-28T13:46:49.627236Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"227d7b1b174b4e2a9213d75a8c301345"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"464bde0874f444fc8538af3d2adc71b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"275cddf265f34584881ac4b47ceee175"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2d4b9354afe41dca3831d3d4c76ec9b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e2c7ae524764cddae6cb83e6143685c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8386df899b22408aa76afc72598b700a"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/transformers/models/vit/feature_extraction_vit.py:30: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4aacbe7afa8a43f780f85628466a90ec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4c9693a1d0042daa00df45bceaa9e76"}},"metadata":{}},{"name":"stderr","text":"Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 | Loss: 1.0020\nEpoch 2 | Loss: 0.2694\nEpoch 3 | Loss: 0.0656\nEpoch 4 | Loss: 0.0189\nEpoch 5 | Loss: 0.0063\nEpoch 6 | Loss: 0.0042\nEpoch 7 | Loss: 0.0030\nEpoch 8 | Loss: 0.0024\nEpoch 9 | Loss: 0.0020\nEpoch 10 | Loss: 0.0017\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"model.eval()\ncorrect_l1 = 0\ncorrect_l2 = 0\ntotal = 0\n\nwith torch.no_grad():\n    # Use your new val_loader here\n    for batch in dev_loader:\n        # 1. Access directly by key (No batch[0] or batch[1])\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        pixel_values = batch['pixel_values'].to(device)\n        \n        # 2. Get true labels\n        labels_l1 = batch['label1'].to(device)\n        labels_l2 = batch['label2'].to(device)\n\n        # 3. Forward pass (returns two outputs)\n        outputs_l1, outputs_l2 = model(input_ids, attention_mask, pixel_values)\n        \n        # 4. Get predictions\n        _, predicted_l1 = torch.max(outputs_l1, 1)\n        _, predicted_l2 = torch.max(outputs_l2, 1)\n\n        # 5. Update counts\n        total += labels_l1.size(0)\n        correct_l1 += (predicted_l1 == labels_l1).sum().item()\n        correct_l2 += (predicted_l2 == labels_l2).sum().item()\n\n    print(f\"Level 1 Accuracy: {100 * correct_l1 / total:.2f}%\")\n    print(f\"Level 2 Accuracy: {100 * correct_l2 / total:.2f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-24T07:34:04.925690Z","iopub.execute_input":"2026-02-24T07:34:04.926004Z","iopub.status.idle":"2026-02-24T07:34:08.348901Z","shell.execute_reply.started":"2026-02-24T07:34:04.925976Z","shell.execute_reply":"2026-02-24T07:34:08.348260Z"}},"outputs":[{"name":"stdout","text":"Level 1 Accuracy: 99.35%\nLevel 2 Accuracy: 89.54%\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"from sklearn.metrics import classification_report, f1_score, precision_score, recall_score,confusion_matrix\nimport torch\nimport numpy as np\n\n# 1. Put model in evaluation mode\nmodel.eval()\n\n# Storage for ground truth and predictions\nl1_true, l1_pred = [], []\nl2_true, l2_pred = [], []\n\nprint(\"🚀 Starting Evaluation on Malayalam Validation Set...\")\n\nwith torch.no_grad():\n    for batch in dev_loader:\n        # Move inputs to device\n        ids = batch['input_ids'].to(device)\n        mask = batch['attention_mask'].to(device)\n        pix = batch['pixel_values'].to(device)\n        \n        # Get true labels\n        y1 = batch['label1'].to(device)\n        y2 = batch['label2'].to(device)\n\n        # Forward pass (XLM-R + CLIP)\n        out1, out2 = model(ids, mask, pix)\n        \n        # Get predicted class indices\n        _, p1 = torch.max(out1, 1)\n        _, p2 = torch.max(out2, 1)\n\n        # Store for report\n        l1_true.extend(y1.cpu().numpy())\n        l1_pred.extend(p1.cpu().numpy())\n        l2_true.extend(y2.cpu().numpy())\n        l2_pred.extend(p2.cpu().numpy())\n\n# 2. Define Category Names for clarity\n# Adjust these based on your specific Malayalam mapping\ntarget_names_l1 = ['SUPPORT', 'TROLL/ OPPOSE']\ntarget_names_l2 = ['person', 'party', 'Intersection']\n\n# 3. Generate and Print Reports\nprint(\"\\n\" + \"=\"*60)\nprint(\"📊 LEVEL 1: OVERALL SENTIMENT (Troll vs. Support)\")\nprint(\"=\"*60)\nprint(classification_report(l1_true, l1_pred, target_names=target_names_l1, digits=4))\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"📊 LEVEL 2: TARGET CLASSIFICATION (Person/Party/Intersection)\")\nprint(\"=\"*60)\n# Use labels=[0, 1, 2] to ensure all classes are included even if zero-support\nprint(classification_report(l2_true, l2_pred, target_names=target_names_l2, labels=[0, 1, 2], digits=4))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-28T13:48:06.544529Z","iopub.execute_input":"2026-02-28T13:48:06.545081Z","iopub.status.idle":"2026-02-28T13:48:10.282442Z","shell.execute_reply.started":"2026-02-28T13:48:06.545049Z","shell.execute_reply":"2026-02-28T13:48:10.281668Z"}},"outputs":[{"name":"stdout","text":"🚀 Starting Evaluation on Malayalam Validation Set...\n\n============================================================\n📊 LEVEL 1: OVERALL SENTIMENT (Troll vs. Support)\n============================================================\n               precision    recall  f1-score   support\n\n      SUPPORT     1.0000    0.9608    0.9800        51\nTROLL/ OPPOSE     0.9808    1.0000    0.9903       102\n\n     accuracy                         0.9869       153\n    macro avg     0.9904    0.9804    0.9851       153\n weighted avg     0.9872    0.9869    0.9869       153\n\n\n============================================================\n📊 LEVEL 2: TARGET CLASSIFICATION (Person/Party/Intersection)\n============================================================\n              precision    recall  f1-score   support\n\n      person     0.8060    0.9643    0.8780        56\n       party     0.9783    0.8333    0.9000        54\nIntersection     0.9500    0.8837    0.9157        43\n\n    accuracy                         0.8954       153\n   macro avg     0.9114    0.8938    0.8979       153\nweighted avg     0.9073    0.8954    0.8964       153\n\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"df_test_labeled=pd.read_excel('/kaggle/input/datasets/anindamajumder118060/malayalam-test-with-labels/Malayalam_Test_label (2).xlsx')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-28T13:53:38.308292Z","iopub.execute_input":"2026-02-28T13:53:38.308982Z","iopub.status.idle":"2026-02-28T13:53:38.335393Z","shell.execute_reply.started":"2026-02-28T13:53:38.308951Z","shell.execute_reply":"2026-02-28T13:53:38.334851Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"def test_Level1_MAP(label):\n    \n    if \"TROLL/OPPOSE\" in label:\n        return 1\n    elif \"Support/Praise\" in label:\n        return 0\n\n    else :\n        return 'NaN'\n\n\ndef test_map_level2(label):\n    \n\n    if \"person\" in label:\n        return 0\n    elif \"party\" in label:\n        return 1\n    elif \"Intersection\" in label:\n        return 2\n    else:\n        return -1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-28T13:58:21.477227Z","iopub.execute_input":"2026-02-28T13:58:21.477842Z","iopub.status.idle":"2026-02-28T13:58:21.482281Z","shell.execute_reply.started":"2026-02-28T13:58:21.477810Z","shell.execute_reply":"2026-02-28T13:58:21.481532Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"df_test_labeled['label1']=df_test_labeled['Level 1'].map(test_Level1_MAP)\ndf_test_labeled['label2']=df_test_labeled[\"Level 2\"].apply(test_map_level2)\n\nprint(\"Train examples:\", len(df_test_labeled))\n\ndf_test_labeled.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-28T13:58:32.458597Z","iopub.execute_input":"2026-02-28T13:58:32.458918Z","iopub.status.idle":"2026-02-28T13:58:32.468319Z","shell.execute_reply.started":"2026-02-28T13:58:32.458891Z","shell.execute_reply":"2026-02-28T13:58:32.467717Z"}},"outputs":[{"name":"stdout","text":"Train examples: 100\n","output_type":"stream"},{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"   Meme_id         Level 1                        Level 2  label1  label2\n0        2  Support/Praise  Support for individual person       0       0\n1       14    TROLL/OPPOSE      Against individual person       1       0\n2       15    TROLL/OPPOSE                  Against party       1       1\n3       21    TROLL/OPPOSE      Against individual person       1       0\n4       22    TROLL/OPPOSE      Against individual person       1       0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Meme_id</th>\n      <th>Level 1</th>\n      <th>Level 2</th>\n      <th>label1</th>\n      <th>label2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2</td>\n      <td>Support/Praise</td>\n      <td>Support for individual person</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>14</td>\n      <td>TROLL/OPPOSE</td>\n      <td>Against individual person</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>15</td>\n      <td>TROLL/OPPOSE</td>\n      <td>Against party</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>21</td>\n      <td>TROLL/OPPOSE</td>\n      <td>Against individual person</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>22</td>\n      <td>TROLL/OPPOSE</td>\n      <td>Against individual person</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":38},{"cell_type":"code","source":"import matplotlib.pyplot as plt \nimport seaborn as sns\nfrom matplotlib.colors import LinearSegmentedColormap\n\ncustom_cmap = LinearSegmentedColormap.from_list(\n    \"custom_pastel\",\n    [\"#B9E9E9\", \"#BFDFF3\", \"floralwhite\", \"#E6E6FA\"]\n)\n# Level 1 Confusion Matrix\ncm_l1 = confusion_matrix(l1_true, l1_pred)\n# Level 2 Confusion Matrix\ncm_l2 = confusion_matrix(l2_true, l2_pred)\n\ndef plot_confusion_matrix(cm, title, class_names):\n    plt.figure()\n    sns.heatmap(\n        cm,\n        annot=True,\n        fmt='d',\n        cmap=custom_cmap,\n        cbar=True\n    )\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"True\")\n    plt.title(title)\n    plt.xticks(ticks=np.arange(len(class_names)) + 0.5, labels=class_names)\n    plt.yticks(ticks=np.arange(len(class_names)) + 0.5, labels=class_names)\n    plt.show()\n\n\n# Level 1: Troll / Support\nplot_confusion_matrix(cm_l1, \"Level 1 Confusion Matrix(Malayalam)\", [\"Troll\", \"Support\"])\n\n# Level 2: Person / Party\nplot_confusion_matrix(cm_l2, \"Level 2 Confusion Matrix(Malayalam)\", [\"Person\", \"Party\", \"Intersection\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-24T07:39:34.268670Z","iopub.execute_input":"2026-02-24T07:39:34.269436Z","iopub.status.idle":"2026-02-24T07:39:34.529040Z","shell.execute_reply.started":"2026-02-24T07:39:34.269407Z","shell.execute_reply":"2026-02-24T07:39:34.528454Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 2 Axes>","image/png":"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\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 2 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":37},{"cell_type":"code","source":"# Check how many unique Tamil sentences are in your validation set\nunique_texts_val = df_val['text'].nunique()\ntotal_val = len(df_val)\n\nprint(f\"Total rows in Validation: {total_val}\")\nprint(f\"Unique Tamil sentences: {unique_texts_val}\")\n\nif unique_texts_val < total_val:\n    print(f\"⚠️ Potential Leakage: {total_val - unique_texts_val} rows have repeating text patterns.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-19T18:36:53.042420Z","iopub.execute_input":"2026-02-19T18:36:53.042684Z","iopub.status.idle":"2026-02-19T18:36:53.048925Z","shell.execute_reply.started":"2026-02-19T18:36:53.042660Z","shell.execute_reply":"2026-02-19T18:36:53.048086Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1. Get all unique text strings from the training set\ntrain_texts = set(df_train['text'].unique())\n\n# 2. Filter the validation set to ONLY include text that is NOT in the training set\n# This removes all augmented \"cousins\" of training data\nstrict_val_df = df_val[~df_val['text'].isin(train_texts)].copy()\n\nprint(f\"Rows in Strict Validation: {len(strict_val_df)}\")\n\nif len(strict_val_df) > 0:\n    # 3. Create a new loader for these 'unseen' memes\n    strict_val_dataset = MultiModalDataset(strict_val_df, tokenizer, feature_extractor)\n    strict_val_loader = DataLoader(strict_val_dataset, batch_size=16, shuffle=False)\n    \n    # 4. Run your evaluation loop again using 'strict_val_loader'\n    # (Use the code we used for the 100% report here)\nelse:\n    print(\"❌ Error: No unique text found in validation. All val rows are variations of training rows.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-19T18:36:53.049896Z","iopub.execute_input":"2026-02-19T18:36:53.050240Z","iopub.status.idle":"2026-02-19T18:36:53.073782Z","shell.execute_reply.started":"2026-02-19T18:36:53.050217Z","shell.execute_reply":"2026-02-19T18:36:53.073126Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import classification_report, f1_score\nimport numpy as np\n\n# 1. Create the Strict Dataset and Loader\nstrict_val_dataset = MultiModalDataset(strict_val_df, tokenizer, feature_extractor)\nstrict_val_loader = DataLoader(strict_val_dataset, batch_size=16, shuffle=False)\n\nmodel.eval()\nl1_true, l1_pred = [], []\nl2_true, l2_pred = [], []\n\nwith torch.no_grad():\n    for batch in strict_val_loader:\n        # Move to device\n        ids = batch['input_ids'].to(device)\n        mask = batch['attention_mask'].to(device)\n        pix = batch['pixel_values'].to(device)\n        \n        # Get labels\n        y1 = batch['label1'].to(device)\n        y2 = batch['label2'].to(device)\n\n        # Forward pass\n        out1, out2 = model(ids, mask, pix)\n        \n        # Get predictions\n        _, p1 = torch.max(out1, 1)\n        _, p2 = torch.max(out2, 1)\n\n        l1_true.extend(y1.cpu().numpy())\n        l1_pred.extend(p1.cpu().numpy())\n        l2_true.extend(y2.cpu().numpy())\n        l2_pred.extend(p2.cpu().numpy())\n\n# Final Report\nprint(\"🔥 --- STRICT VALIDATION RESULTS (0% LEAKAGE) --- 🔥\")\nprint(f\"Level 1 F1 (Macro): {f1_score(l1_true, l1_pred, average='macro'):.4f}\")\nprint(classification_report(l1_true, l1_pred, target_names=['Support', 'Troll']))\n\nprint(\"\\n\" + \"=\"*45 + \"\\n\")\nprint(f\"Level 2 F1 (Macro): {f1_score(l2_true, l2_pred, average='macro'):.4f}\")\nprint(classification_report(l2_true, l2_pred)) \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-19T18:36:53.074693Z","iopub.execute_input":"2026-02-19T18:36:53.074954Z","iopub.status.idle":"2026-02-19T18:36:56.174910Z","shell.execute_reply.started":"2026-02-19T18:36:53.074928Z","shell.execute_reply":"2026-02-19T18:36:56.174154Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test\ndrop_col=['Level 1', 'Level 2']\ntest.drop(columns=drop_col, axis=1, inplace=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-28T14:16:39.197914Z","iopub.execute_input":"2026-02-28T14:16:39.198775Z","iopub.status.idle":"2026-02-28T14:16:39.203545Z","shell.execute_reply.started":"2026-02-28T14:16:39.198726Z","shell.execute_reply":"2026-02-28T14:16:39.202852Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-28T14:16:50.053690Z","iopub.execute_input":"2026-02-28T14:16:50.054381Z","iopub.status.idle":"2026-02-28T14:17:02.367375Z","shell.execute_reply.started":"2026-02-28T14:16:50.054342Z","shell.execute_reply":"2026-02-28T14:17:02.366712Z"}},"outputs":[{"execution_count":49,"output_type":"execute_result","data":{"text/plain":"    meme_id                                               text  \\\n0       100  കില്‍ ത്ത്‌ ഗള്‍ കം ക ്ന റ ഥി വ ക്‌ ണാ ലം ടം ്...   \n1       103  ഹ്ന ടാം പി ടാ ടം ത്തത്‌ ക ത ന ക ടൂ ക ന്തു ഹു ട...   \n2       106  വ ്ു വ ി യ ക ലി സ്സ പ്ര ി ബ്ല യ നടന ഥ്‌ റ ലയ ന...   \n3       107  ന യിട്ടു ദിനം യി ാ ട്ട പ നി വിയ റ്‌ ടര രക സമദര...   \n4       122  യല്ല ലം ഴു്‌ ലാ ്്‌ റിക ഷ്‌ മി ഗി കു്‌ ലി ല്ല ...   \n..      ...                                                ...   \n95       65  പ്്ാ്റ്ര യ സ്‌ ന നു ന ക്്‌ക്്മ്തി കക്ഷത്ത്‌ ചി...   \n96       72  ₹ നട ി രം ള്‌ ഴ്‌ ം ി ലം പ്ല ഥ്ട്ബ്ങടയ ജി ടെ അ...   \n97       81  ലം ടി ക ക പ്ല ര്‍ ളു തി ം രിത്ത്തിത്ത്തിന്ന്ന്...   \n98       88  രാ നം തട്‌ ക മു ക ടാ ു ച്‌ ി ത്‌ അ നു ം ന ണു ട...   \n99       92  പപ ശ പച്്പ്വദുഡ്യ്യ്്സ്സ്‌ ക്‌ തത റ റ്‌ പ്പ ഴു...   \n\n                                           Image_name  \n0   [[[20, 20, 20], [20, 20, 20], [20, 20, 20], [2...  \n1   [[[255, 255, 255], [255, 255, 255], [255, 255,...  \n2   [[[69, 67, 64], [73, 64, 64], [73, 66, 66], [7...  \n3   [[[176, 115, 38], [175, 114, 38], [172, 111, 3...  \n4   [[[255, 255, 255], [255, 255, 255], [255, 255,...  \n..                                                ...  \n95  [[[255, 30, 34], [255, 31, 33], [255, 31, 32],...  \n96  [[[255, 255, 255], [255, 255, 255], [255, 255,...  \n97  [[[32, 39, 44], [33, 39, 44], [34, 38, 44], [3...  \n98  [[[189, 164, 144], [188, 165, 145], [190, 168,...  \n99  [[[67, 63, 98], [63, 60, 94], [62, 58, 93], [6...  \n\n[100 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>meme_id</th>\n      <th>text</th>\n      <th>Image_name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>100</td>\n      <td>കില്‍ ത്ത്‌ ഗള്‍ കം ക ്ന റ ഥി വ ക്‌ ണാ ലം ടം ്...</td>\n      <td>[[[20, 20, 20], [20, 20, 20], [20, 20, 20], [2...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>103</td>\n      <td>ഹ്ന ടാം പി ടാ ടം ത്തത്‌ ക ത ന ക ടൂ ക ന്തു ഹു ട...</td>\n      <td>[[[255, 255, 255], [255, 255, 255], [255, 255,...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>106</td>\n      <td>വ ്ു വ ി യ ക ലി സ്സ പ്ര ി ബ്ല യ നടന ഥ്‌ റ ലയ ന...</td>\n      <td>[[[69, 67, 64], [73, 64, 64], [73, 66, 66], [7...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>107</td>\n      <td>ന യിട്ടു ദിനം യി ാ ട്ട പ നി വിയ റ്‌ ടര രക സമദര...</td>\n      <td>[[[176, 115, 38], [175, 114, 38], [172, 111, 3...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>122</td>\n      <td>യല്ല ലം ഴു്‌ ലാ ്്‌ റിക ഷ്‌ മി ഗി കു്‌ ലി ല്ല ...</td>\n      <td>[[[255, 255, 255], [255, 255, 255], [255, 255,...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>65</td>\n      <td>പ്്ാ്റ്ര യ സ്‌ ന നു ന ക്്‌ക്്മ്തി കക്ഷത്ത്‌ ചി...</td>\n      <td>[[[255, 30, 34], [255, 31, 33], [255, 31, 32],...</td>\n    </tr>\n    <tr>\n      <th>96</th>\n      <td>72</td>\n      <td>₹ നട ി രം ള്‌ ഴ്‌ ം ി ലം പ്ല ഥ്ട്ബ്ങടയ ജി ടെ അ...</td>\n      <td>[[[255, 255, 255], [255, 255, 255], [255, 255,...</td>\n    </tr>\n    <tr>\n      <th>97</th>\n      <td>81</td>\n      <td>ലം ടി ക ക പ്ല ര്‍ ളു തി ം രിത്ത്തിത്ത്തിന്ന്ന്...</td>\n      <td>[[[32, 39, 44], [33, 39, 44], [34, 38, 44], [3...</td>\n    </tr>\n    <tr>\n      <th>98</th>\n      <td>88</td>\n      <td>രാ നം തട്‌ ക മു ക ടാ ു ച്‌ ി ത്‌ അ നു ം ന ണു ട...</td>\n      <td>[[[189, 164, 144], [188, 165, 145], [190, 168,...</td>\n    </tr>\n    <tr>\n      <th>99</th>\n      <td>92</td>\n      <td>പപ ശ പച്്പ്വദുഡ്യ്യ്്സ്സ്‌ ക്‌ തത റ റ്‌ പ്പ ഴു...</td>\n      <td>[[[67, 63, 98], [63, 60, 94], [62, 58, 93], [6...</td>\n    </tr>\n  </tbody>\n</table>\n<p>100 rows × 3 columns</p>\n</div>"},"metadata":{}}],"execution_count":49},{"cell_type":"code","source":"df_test_labeled=df_test_labeled.rename(columns={'Meme_id':'meme_id'})\ndf_test_labeled","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-28T14:12:09.568379Z","iopub.execute_input":"2026-02-28T14:12:09.569130Z","iopub.status.idle":"2026-02-28T14:12:09.578135Z","shell.execute_reply.started":"2026-02-28T14:12:09.569098Z","shell.execute_reply":"2026-02-28T14:12:09.577543Z"}},"outputs":[{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"    meme_id         Level 1                        Level 2  label1  label2\n0         2  Support/Praise  Support for individual person       0       0\n1        14    TROLL/OPPOSE      Against individual person       1       0\n2        15    TROLL/OPPOSE                  Against party       1       1\n3        21    TROLL/OPPOSE      Against individual person       1       0\n4        22    TROLL/OPPOSE      Against individual person       1       0\n..      ...             ...                            ...     ...     ...\n95      563    TROLL/OPPOSE                   Intersection       1       2\n96      584    TROLL/OPPOSE                  Against party       1       1\n97      586    TROLL/OPPOSE                  Against party       1       1\n98      595    TROLL/OPPOSE      Against individual person       1       0\n99      598    TROLL/OPPOSE      Against individual person       1       0\n\n[100 rows x 5 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>meme_id</th>\n      <th>Level 1</th>\n      <th>Level 2</th>\n      <th>label1</th>\n      <th>label2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2</td>\n      <td>Support/Praise</td>\n      <td>Support for individual person</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>14</td>\n      <td>TROLL/OPPOSE</td>\n      <td>Against individual person</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>15</td>\n      <td>TROLL/OPPOSE</td>\n      <td>Against party</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>21</td>\n      <td>TROLL/OPPOSE</td>\n      <td>Against individual person</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>22</td>\n      <td>TROLL/OPPOSE</td>\n      <td>Against individual person</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>563</td>\n      <td>TROLL/OPPOSE</td>\n      <td>Intersection</td>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>96</th>\n      <td>584</td>\n      <td>TROLL/OPPOSE</td>\n      <td>Against party</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>97</th>\n      <td>586</td>\n      <td>TROLL/OPPOSE</td>\n      <td>Against party</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>98</th>\n      <td>595</td>\n      <td>TROLL/OPPOSE</td>\n      <td>Against individual person</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>99</th>\n      <td>598</td>\n      <td>TROLL/OPPOSE</td>\n      <td>Against individual person</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>100 rows × 5 columns</p>\n</div>"},"metadata":{}}],"execution_count":43},{"cell_type":"code","source":"merged_test_labeled= pd.merge(test, df_test_labeled, on='meme_id')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-28T14:19:35.396434Z","iopub.execute_input":"2026-02-28T14:19:35.397144Z","iopub.status.idle":"2026-02-28T14:19:35.402998Z","shell.execute_reply.started":"2026-02-28T14:19:35.397107Z","shell.execute_reply":"2026-02-28T14:19:35.402333Z"}},"outputs":[],"execution_count":51},{"cell_type":"code","source":"merged_test_labeled","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-28T14:28:18.801380Z","iopub.execute_input":"2026-02-28T14:28:18.801988Z","iopub.status.idle":"2026-02-28T14:28:31.189927Z","shell.execute_reply.started":"2026-02-28T14:28:18.801958Z","shell.execute_reply":"2026-02-28T14:28:31.189240Z"}},"outputs":[{"execution_count":54,"output_type":"execute_result","data":{"text/plain":"    meme_id                                               text  \\\n0       100  കില്‍ ത്ത്‌ ഗള്‍ കം ക ്ന റ ഥി വ ക്‌ ണാ ലം ടം ്...   \n1       103  ഹ്ന ടാം പി ടാ ടം ത്തത്‌ ക ത ന ക ടൂ ക ന്തു ഹു ട...   \n2       106  വ ്ു വ ി യ ക ലി സ്സ പ്ര ി ബ്ല യ നടന ഥ്‌ റ ലയ ന...   \n3       107  ന യിട്ടു ദിനം യി ാ ട്ട പ നി വിയ റ്‌ ടര രക സമദര...   \n4       122  യല്ല ലം ഴു്‌ ലാ ്്‌ റിക ഷ്‌ മി ഗി കു്‌ ലി ല്ല ...   \n..      ...                                                ...   \n95       65  പ്്ാ്റ്ര യ സ്‌ ന നു ന ക്്‌ക്്മ്തി കക്ഷത്ത്‌ ചി...   \n96       72  ₹ നട ി രം ള്‌ ഴ്‌ ം ി ലം പ്ല ഥ്ട്ബ്ങടയ ജി ടെ അ...   \n97       81  ലം ടി ക ക പ്ല ര്‍ ളു തി ം രിത്ത്തിത്ത്തിന്ന്ന്...   \n98       88  രാ നം തട്‌ ക മു ക ടാ ു ച്‌ ി ത്‌ അ നു ം ന ണു ട...   \n99       92  പപ ശ പച്്പ്വദുഡ്യ്യ്്സ്സ്‌ ക്‌ തത റ റ്‌ പ്പ ഴു...   \n\n                                           Image_name         Level 1  \\\n0   [[[20, 20, 20], [20, 20, 20], [20, 20, 20], [2...    TROLL/OPPOSE   \n1   [[[255, 255, 255], [255, 255, 255], [255, 255,...    TROLL/OPPOSE   \n2   [[[69, 67, 64], [73, 64, 64], [73, 66, 66], [7...    TROLL/OPPOSE   \n3   [[[176, 115, 38], [175, 114, 38], [172, 111, 3...    TROLL/OPPOSE   \n4   [[[255, 255, 255], [255, 255, 255], [255, 255,...    TROLL/OPPOSE   \n..                                                ...             ...   \n95  [[[255, 30, 34], [255, 31, 33], [255, 31, 32],...    TROLL/OPPOSE   \n96  [[[255, 255, 255], [255, 255, 255], [255, 255,...    TROLL/OPPOSE   \n97  [[[32, 39, 44], [33, 39, 44], [34, 38, 44], [3...    TROLL/OPPOSE   \n98  [[[189, 164, 144], [188, 165, 145], [190, 168,...    TROLL/OPPOSE   \n99  [[[67, 63, 98], [63, 60, 94], [62, 58, 93], [6...  Support/Praise   \n\n                          Level 2  label1  label2  \n0                   Against party       1       1  \n1       Against individual person       1       0  \n2                   Against party       1       1  \n3       Against individual person       1       0  \n4                    Intersection       1       2  \n..                            ...     ...     ...  \n95                   Intersection       1       2  \n96                   Intersection       1       2  \n97      Against individual person       1       0  \n98                   Intersection       1       2  \n99  Support for individual person       0       0  \n\n[100 rows x 7 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>meme_id</th>\n      <th>text</th>\n      <th>Image_name</th>\n      <th>Level 1</th>\n      <th>Level 2</th>\n      <th>label1</th>\n      <th>label2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>100</td>\n      <td>കില്‍ ത്ത്‌ ഗള്‍ കം ക ്ന റ ഥി വ ക്‌ ണാ ലം ടം ്...</td>\n      <td>[[[20, 20, 20], [20, 20, 20], [20, 20, 20], [2...</td>\n      <td>TROLL/OPPOSE</td>\n      <td>Against party</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>103</td>\n      <td>ഹ്ന ടാം പി ടാ ടം ത്തത്‌ ക ത ന ക ടൂ ക ന്തു ഹു ട...</td>\n      <td>[[[255, 255, 255], [255, 255, 255], [255, 255,...</td>\n      <td>TROLL/OPPOSE</td>\n      <td>Against individual person</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>106</td>\n      <td>വ ്ു വ ി യ ക ലി സ്സ പ്ര ി ബ്ല യ നടന ഥ്‌ റ ലയ ന...</td>\n      <td>[[[69, 67, 64], [73, 64, 64], [73, 66, 66], [7...</td>\n      <td>TROLL/OPPOSE</td>\n      <td>Against party</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>107</td>\n      <td>ന യിട്ടു ദിനം യി ാ ട്ട പ നി വിയ റ്‌ ടര രക സമദര...</td>\n      <td>[[[176, 115, 38], [175, 114, 38], [172, 111, 3...</td>\n      <td>TROLL/OPPOSE</td>\n      <td>Against individual person</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>122</td>\n      <td>യല്ല ലം ഴു്‌ ലാ ്്‌ റിക ഷ്‌ മി ഗി കു്‌ ലി ല്ല ...</td>\n      <td>[[[255, 255, 255], [255, 255, 255], [255, 255,...</td>\n      <td>TROLL/OPPOSE</td>\n      <td>Intersection</td>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>65</td>\n      <td>പ്്ാ്റ്ര യ സ്‌ ന നു ന ക്്‌ക്്മ്തി കക്ഷത്ത്‌ ചി...</td>\n      <td>[[[255, 30, 34], [255, 31, 33], [255, 31, 32],...</td>\n      <td>TROLL/OPPOSE</td>\n      <td>Intersection</td>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>96</th>\n      <td>72</td>\n      <td>₹ നട ി രം ള്‌ ഴ്‌ ം ി ലം പ്ല ഥ്ട്ബ്ങടയ ജി ടെ അ...</td>\n      <td>[[[255, 255, 255], [255, 255, 255], [255, 255,...</td>\n      <td>TROLL/OPPOSE</td>\n      <td>Intersection</td>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>97</th>\n      <td>81</td>\n      <td>ലം ടി ക ക പ്ല ര്‍ ളു തി ം രിത്ത്തിത്ത്തിന്ന്ന്...</td>\n      <td>[[[32, 39, 44], [33, 39, 44], [34, 38, 44], [3...</td>\n      <td>TROLL/OPPOSE</td>\n      <td>Against individual person</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>98</th>\n      <td>88</td>\n      <td>രാ നം തട്‌ ക മു ക ടാ ു ച്‌ ി ത്‌ അ നു ം ന ണു ട...</td>\n      <td>[[[189, 164, 144], [188, 165, 145], [190, 168,...</td>\n      <td>TROLL/OPPOSE</td>\n      <td>Intersection</td>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>99</th>\n      <td>92</td>\n      <td>പപ ശ പച്്പ്വദുഡ്യ്യ്്സ്സ്‌ ക്‌ തത റ റ്‌ പ്പ ഴു...</td>\n      <td>[[[67, 63, 98], [63, 60, 94], [62, 58, 93], [6...</td>\n      <td>Support/Praise</td>\n      <td>Support for individual person</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>100 rows × 7 columns</p>\n</div>"},"metadata":{}}],"execution_count":54},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom PIL import Image\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\n\nclass Test_Labeled_Dataset(Dataset):\n    def __init__(self, df, tokenizer, feature_extractor):\n        self.df = df.reset_index(drop=True)\n        self.tokenizer = tokenizer\n        self.feature_extractor = feature_extractor\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        text = str(self.df.loc[idx, 'text'])\n        image_arr = self.df.loc[idx, 'Image_name'] \n        l1 = self.df.loc[idx, 'label1']\n        l2 = self.df.loc[idx, 'label2']\n\n        encoding = self.tokenizer(text, padding='max_length', truncation=True, max_length=128, return_tensors='pt')\n        \n        # Image processing\n        image = Image.fromarray(image_arr.astype('uint8')).convert(\"RGB\")\n        pixel_values = self.feature_extractor(images=image, return_tensors='pt')['pixel_values'].squeeze(0)\n\n        # Return DICTIONARY\n        return {\n            'input_ids': encoding['input_ids'].squeeze(0),\n            'attention_mask': encoding['attention_mask'].squeeze(0),\n            'pixel_values': pixel_values,\n            'label1': torch.tensor(l1, dtype=torch.long),\n            'label2': torch.tensor(l2, dtype=torch.long)\n        }   \n\n# 1. Initialize Dataset and Loader\ntest_dataset_labeled = Test_Labeled_Dataset(merged_test_labeled, tokenizer, feature_extractor)\ntest_loader_labeled = DataLoader(test_dataset_labeled, batch_size=16)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-28T14:42:38.625715Z","iopub.execute_input":"2026-02-28T14:42:38.626030Z","iopub.status.idle":"2026-02-28T14:42:38.634660Z","shell.execute_reply.started":"2026-02-28T14:42:38.626002Z","shell.execute_reply":"2026-02-28T14:42:38.633855Z"}},"outputs":[],"execution_count":60},{"cell_type":"code","source":"from sklearn.metrics import classification_report, f1_score, precision_score, recall_score,confusion_matrix\nimport torch\nimport numpy as np\n\n# 1. Put model in evaluation mode\nmodel.eval()\n\n# Storage for ground truth and predictions\nl1_true, l1_pred = [], []\nl2_true, l2_pred = [], []\n\nprint(\"🚀 Starting Evaluation on Malayalam test Set...\")\n\nwith torch.no_grad():\n    for batch in test_loader_labeled:\n        # Move inputs to device\n        ids = batch['input_ids'].to(device)\n        mask = batch['attention_mask'].to(device)\n        pix = batch['pixel_values'].to(device)\n        \n        # Get true labels\n        y1 = batch['label1'].to(device)\n        y2 = batch['label2'].to(device)\n\n        # Forward pass (XLM-R + CLIP)\n        out1, out2 = model(ids, mask, pix)\n        \n        # Get predicted class indices\n        _, p1 = torch.max(out1, 1)\n        _, p2 = torch.max(out2, 1)\n\n        # Store for report\n        l1_true.extend(y1.cpu().numpy())\n        l1_pred.extend(p1.cpu().numpy())\n        l2_true.extend(y2.cpu().numpy())\n        l2_pred.extend(p2.cpu().numpy())\n\n# 2. Define Category Names for clarity\n# Adjust these based on your specific Malayalam mapping\ntarget_names_l1 = ['Support/Praise', 'TROLL/OPPOSE']\ntarget_names_l2 = ['person', 'party', 'Intersection']\n\n# 3. Generate and Print Reports\nprint(\"\\n\" + \"=\"*60)\nprint(\"📊 LEVEL 1: OVERALL SENTIMENT for Test Data (Troll vs. Support)\")\nprint(\"=\"*60)\nprint(classification_report(l1_true, l1_pred, target_names=target_names_l1, digits=4))\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"📊 LEVEL 2: TARGET CLASSIFICATION for Test Data (Person/Party/Intersection)\")\nprint(\"=\"*60)\n# Use labels=[0, 1, 2] to ensure all classes are included even if zero-support\nprint(classification_report(l2_true, l2_pred, target_names=target_names_l2, labels=[0, 1, 2], digits=4))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-28T14:45:36.752323Z","iopub.execute_input":"2026-02-28T14:45:36.752785Z","iopub.status.idle":"2026-02-28T14:45:39.401455Z","shell.execute_reply.started":"2026-02-28T14:45:36.752756Z","shell.execute_reply":"2026-02-28T14:45:39.400062Z"}},"outputs":[{"name":"stdout","text":"🚀 Starting Evaluation on Malayalam test Set...\n\n============================================================\n📊 LEVEL 1: OVERALL SENTIMENT for Test Data (Troll vs. Support)\n============================================================\n                precision    recall  f1-score   support\n\nSupport/Praise     0.0000    0.0000    0.0000         4\n  TROLL/OPPOSE     0.9600    1.0000    0.9796        96\n\n      accuracy                         0.9600       100\n     macro avg     0.4800    0.5000    0.4898       100\n  weighted avg     0.9216    0.9600    0.9404       100\n\n\n============================================================\n📊 LEVEL 2: TARGET CLASSIFICATION for Test Data (Person/Party/Intersection)\n============================================================\n              precision    recall  f1-score   support\n\n      person     0.5862    0.9444    0.7234        54\n       party     0.6000    0.1935    0.2927        31\nIntersection     0.6667    0.1333    0.2222        15\n\n    accuracy                         0.5900       100\n   macro avg     0.6176    0.4238    0.4128       100\nweighted avg     0.6026    0.5900    0.5147       100\n\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n","output_type":"stream"}],"execution_count":62},{"cell_type":"code","source":"import matplotlib.pyplot as plt \nimport seaborn as sns\nfrom matplotlib.colors import LinearSegmentedColormap\n\ncustom_cmap = LinearSegmentedColormap.from_list(\n    \"custom_pastel\",\n    [\"#B9E9E9\", \"#BFDFF3\", \"floralwhite\", \"#E6E6FA\"]\n)\n# Level 1 Confusion Matrix\ncm_l1 = confusion_matrix(l1_true, l1_pred)\n# Level 2 Confusion Matrix\ncm_l2 = confusion_matrix(l2_true, l2_pred)\n\ndef plot_confusion_matrix(cm, title, class_names):\n    plt.figure()\n    sns.heatmap(\n        cm,\n        annot=True,\n        fmt='d',\n        cmap=custom_cmap,\n        cbar=True\n    )\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"True\")\n    plt.title(title)\n    plt.xticks(ticks=np.arange(len(class_names)) + 0.5, labels=class_names)\n    plt.yticks(ticks=np.arange(len(class_names)) + 0.5, labels=class_names)\n    plt.show()\n\n\n# Level 1: Troll / Support\nplot_confusion_matrix(cm_l1, \"Level 1 Confusion Matrix(Malayalam)\", [\"Support\", \"Troll\"])\n\n# Level 2: Person / Party\nplot_confusion_matrix(cm_l2, \"Level 2 Confusion Matrix(Malayalam)\", [\"Person\", \"Party\", \"Intersection\"])","metadata":{"trusted":true,"execution":{"execution_failed":"2026-02-28T15:38:59.286Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom PIL import Image\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\n\nclass TestDataset(Dataset):\n    def __init__(self, df, tokenizer, feature_extractor):\n        # Reset index to ensure safe .loc access\n        self.df = df.reset_index(drop=True)\n        self.tokenizer = tokenizer\n        self.feature_extractor = feature_extractor\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        # Use your test dataframe column names\n        text = str(self.df.loc[idx, 'text'])\n        image_arr = self.df.loc[idx, 'Image_name'] \n\n        encoding = self.tokenizer(\n            text, \n            padding='max_length', \n            truncation=True, \n            max_length=128, # Match training max_length\n            return_tensors='pt'\n        )\n\n        image_pil = Image.fromarray(image_arr.astype('uint8')).convert(\"RGB\")\n        pixel_values = self.feature_extractor(images=image_pil, return_tensors='pt')['pixel_values'].squeeze(0)\n\n        return {\n            'input_ids': encoding['input_ids'].squeeze(0),\n            'attention_mask': encoding['attention_mask'].squeeze(0),\n            'pixel_values': pixel_values\n        }\n\n# 1. Initialize Dataset and Loader\ntest_dataset = TestDataset(test, tokenizer, feature_extractor)\ntest_loader = DataLoader(test_dataset, batch_size=16)\n\n# 2. Evaluation Mode\nmodel.eval()\n\n# Lists to store predictions\nl1_preds = []\nl2_preds = []\n\nprint(\"Starting Inference on Test Data...\")\nwith torch.no_grad():\n    for batch in tqdm(test_loader):\n        # Access by keys to match updated Dataset\n        ids = batch['input_ids'].to(device)\n        mask = batch['attention_mask'].to(device)\n        pix = batch['pixel_values'].to(device)\n\n        # Get both Level 1 and Level 2 outputs\n        out1, out2 = model(ids, mask, pix)\n        \n        # Get class indices\n        _, pred1 = torch.max(out1, 1)\n        _, pred2 = torch.max(out2, 1)\n\n        l1_preds.extend(pred1.cpu().numpy())\n        l2_preds.extend(pred2.cpu().numpy())\n\n# 3. Save results to the test dataframe\ntest['label1_pred'] = l1_preds\ntest['label2_pred'] = l2_preds\n\n# 4. Export to CSV (assuming your test set has 'Image_id')\npredictions_df = pd.DataFrame({\n    'meme_id': test['meme_id'],\n    'label1_prediction': test['label1_pred'],\n    'label2_prediction': test['label2_pred']\n})\n\npredictions_df.to_csv('Malyalam_mbert_vit.csv', index=False)\nprint(\"Predictions saved\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-28T14:49:32.965323Z","iopub.execute_input":"2026-02-28T14:49:32.966379Z","iopub.status.idle":"2026-02-28T14:49:35.575502Z","shell.execute_reply.started":"2026-02-28T14:49:32.966347Z","shell.execute_reply":"2026-02-28T14:49:35.574747Z"}},"outputs":[{"name":"stdout","text":"Starting Inference on Test Data...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 7/7 [00:02<00:00,  2.71it/s]","output_type":"stream"},{"name":"stdout","text":"Predictions saved\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":64},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"prediction1= pd.read_csv('/kaggle/working/Malyalam_mbert_vit.csv')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-22T10:09:45.657633Z","iopub.execute_input":"2026-02-22T10:09:45.658448Z","iopub.status.idle":"2026-02-22T10:09:45.663386Z","shell.execute_reply.started":"2026-02-22T10:09:45.658374Z","shell.execute_reply":"2026-02-22T10:09:45.662764Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"def map_meme_labels(row):\n    l1 = row['label1_prediction']\n    l2 = row['label2_prediction']\n    \n    # Logic 1: Both are 0\n    if l1 == 0 and l2 == 0:\n        return \"SUPPORT\", \"Support for individual person\"\n    \n    # Logic 2: label1 is 0 and label2 is 1\n    elif l1 == 0 and l2 == 1:\n        return \"SUPPORT\", \"Support for party\"\n    \n    # Logic 3: label1 is 1 and label2 is 0\n    elif l1 == 1 and l2 == 0:\n        return \"TROLL/ OPPOSE\", \"Against individual person\"\n    \n    # Logic 4: Both are 1\n    elif l1 == 1 and l2 == 1:\n        return \"TROLL/OPPOSE\", \"Against party\"\n\n    elif l1==0 and l2==2 :\n        return \"SUPPORT\", \"Intersection\"\n\n    elif l1==1 and l2==2 :\n        return \"TROLL/OPPOSE\", \"Intersection\"\n    \n    # Fallback for any other combinations (optional)\n    return \"Unknown\", \"Unknown\"\n\n# 1. Apply the logic across the dataframe\n# This creates two new columns based on your rules\nprediction1[['Level 1', 'Level 2']] = predictions_df.apply(\n    lambda row: pd.Series(map_meme_labels(row)), axis=1\n)\n\n# 2. Add the Image_name column from your test data\n# Assuming 'Image_name_x' contains values like '000.jpg'\nprediction1\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-22T10:09:54.005115Z","iopub.execute_input":"2026-02-22T10:09:54.005429Z","iopub.status.idle":"2026-02-22T10:09:54.030027Z","shell.execute_reply.started":"2026-02-22T10:09:54.005387Z","shell.execute_reply":"2026-02-22T10:09:54.029464Z"}},"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"    meme_id  label1_prediction  label2_prediction        Level 1  \\\n0       100                  1                  0  TROLL/ OPPOSE   \n1       103                  1                  0  TROLL/ OPPOSE   \n2       106                  1                  0  TROLL/ OPPOSE   \n3       107                  1                  0  TROLL/ OPPOSE   \n4       122                  1                  0  TROLL/ OPPOSE   \n..      ...                ...                ...            ...   \n95       65                  1                  0  TROLL/ OPPOSE   \n96       72                  1                  0  TROLL/ OPPOSE   \n97       81                  1                  0  TROLL/ OPPOSE   \n98       88                  1                  0  TROLL/ OPPOSE   \n99       92                  1                  0  TROLL/ OPPOSE   \n\n                      Level 2  \n0   Against individual person  \n1   Against individual person  \n2   Against individual person  \n3   Against individual person  \n4   Against individual person  \n..                        ...  \n95  Against individual person  \n96  Against individual person  \n97  Against individual person  \n98  Against individual person  \n99  Against individual person  \n\n[100 rows x 5 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>meme_id</th>\n      <th>label1_prediction</th>\n      <th>label2_prediction</th>\n      <th>Level 1</th>\n      <th>Level 2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>100</td>\n      <td>1</td>\n      <td>0</td>\n      <td>TROLL/ OPPOSE</td>\n      <td>Against individual person</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>103</td>\n      <td>1</td>\n      <td>0</td>\n      <td>TROLL/ OPPOSE</td>\n      <td>Against individual person</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>106</td>\n      <td>1</td>\n      <td>0</td>\n      <td>TROLL/ OPPOSE</td>\n      <td>Against individual person</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>107</td>\n      <td>1</td>\n      <td>0</td>\n      <td>TROLL/ OPPOSE</td>\n      <td>Against individual person</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>122</td>\n      <td>1</td>\n      <td>0</td>\n      <td>TROLL/ OPPOSE</td>\n      <td>Against individual person</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>65</td>\n      <td>1</td>\n      <td>0</td>\n      <td>TROLL/ OPPOSE</td>\n      <td>Against individual person</td>\n    </tr>\n    <tr>\n      <th>96</th>\n      <td>72</td>\n      <td>1</td>\n      <td>0</td>\n      <td>TROLL/ OPPOSE</td>\n      <td>Against individual person</td>\n    </tr>\n    <tr>\n      <th>97</th>\n      <td>81</td>\n      <td>1</td>\n      <td>0</td>\n      <td>TROLL/ OPPOSE</td>\n      <td>Against individual person</td>\n    </tr>\n    <tr>\n      <th>98</th>\n      <td>88</td>\n      <td>1</td>\n      <td>0</td>\n      <td>TROLL/ OPPOSE</td>\n      <td>Against individual person</td>\n    </tr>\n    <tr>\n      <th>99</th>\n      <td>92</td>\n      <td>1</td>\n      <td>0</td>\n      <td>TROLL/ OPPOSE</td>\n      <td>Against individual person</td>\n    </tr>\n  </tbody>\n</table>\n<p>100 rows × 5 columns</p>\n</div>"},"metadata":{}}],"execution_count":37},{"cell_type":"markdown","source":"# ****CLIP+XLM-Roberta","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModel, CLIPModel, CLIPProcessor, ViTFeatureExtractor\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\nimport torch.optim as optim\nfrom PIL import Image\n\nclass MultiModalDataset(Dataset):\n    def __init__(self, df, tokenizer, feature_extractor):\n        self.df = df.reset_index(drop=True)\n        self.tokenizer = tokenizer\n        self.feature_extractor = feature_extractor\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        text = str(self.df.loc[idx, 'text'])\n        image_arr = self.df.loc[idx, 'Image_name'] \n        l1 = self.df.loc[idx, 'label1']\n        l2 = self.df.loc[idx, 'label2']\n\n        encoding = self.tokenizer(text, padding='max_length', truncation=True, max_length=128, return_tensors='pt')\n        \n        # Image processing\n        image = Image.fromarray(image_arr.astype('uint8')).convert(\"RGB\")\n        #pixel_values = self.feature_extractor(images=image, return_tensors='pt')['pixel_values'].squeeze(0)\n        pixel_values = clip_processor(images=image, return_tensors='pt')['pixel_values'].squeeze(0)\n        # Return DICTIONARY\n        return {\n            'input_ids': encoding['input_ids'].squeeze(0),\n            'attention_mask': encoding['attention_mask'].squeeze(0),\n            'pixel_values': pixel_values,\n            'label1': torch.tensor(l1, dtype=torch.long),\n            'label2': torch.tensor(l2, dtype=torch.long)\n        }   \n\n\ntokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base')\nroberta_model = AutoModel.from_pretrained('xlm-roberta-base')\n\nclip_processor = CLIPProcessor.from_pretrained('openai/clip-vit-base-patch32')\nclip_model = CLIPModel.from_pretrained('openai/clip-vit-base-patch32')\n#efficientnet_model.classifier = nn.Identity()  \n\nclass MultiModal(nn.Module):\n    def __init__(self, roberta_model, clip_model, num_classes_l1, num_classes_l2):\n        super(MultiModal, self).__init__()\n        self.roberta_model = roberta_model\n        # Use ONLY the vision part of CLIP\n        self.clip_vision_model = clip_model.vision_model \n        \n        # XLM-R base is 768, CLIP vision-base is 768 -> Total 1536\n        self.classifier_l1 = nn.Linear(1536, num_classes_l1)\n        self.classifier_l2 = nn.Linear(1536, num_classes_l2)\n        self.dropout = nn.Dropout(0.3)\n\n    def forward(self, input_ids, attention_mask, pixel_values):\n        # 1. Text features (XLM-R)\n        roberta_output = self.roberta_model(input_ids=input_ids, attention_mask=attention_mask)\n        # XLM-R doesn't always have a pooler_output like BERT, \n        # using the mean of hidden states or the first token [CLS/<s>] is safer\n        text_features = roberta_output.last_hidden_state[:, 0, :] \n\n        # 2. Vision features (CLIP Vision)\n        # We call .vision_model directly to avoid the text-input requirement\n        clip_output = self.clip_vision_model(pixel_values=pixel_values)\n        vision_features = clip_output.pooler_output \n\n        # 3. Fusion\n        combined = torch.cat((text_features, vision_features), dim=1)\n        combined = self.dropout(combined)\n        \n        logits_l1 = self.classifier_l1(combined)\n        logits_l2 = self.classifier_l2(combined)\n        \n        return logits_l1, logits_l2\n\n\ntrain_dataset = MultiModalDataset(df_train, tokenizer, feature_extractor)\ndev_dataset = MultiModalDataset(df_val, tokenizer, feature_extractor)\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\ndev_loader = DataLoader(dev_dataset, batch_size=16)\n\n\n# Initialize\nnum_l1 = 2 # Troll/Support\nnum_l2 = 3 # Person/Party\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel2 = MultiModal(roberta_model, clip_model, num_l1, num_l2).to(device)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.AdamW(model2.parameters(), lr=2e-5) # AdamW is better for BERT/ViT\n\n\nfor epoch in range(10): # 10 is usually enough for fine-tuning\n  model2.train()\n  total_loss = 0\n  for batch in train_loader:\n        # Now batch is a dictionary!\n        ids = batch['input_ids'].to(device)\n        mask = batch['attention_mask'].to(device)\n        pixels = batch['pixel_values'].to(device)\n        lab1 = batch['label1'].to(device)\n        lab2 = batch['label2'].to(device)\n\n        optimizer.zero_grad()\n        # Ensure your model forward function returns both out1 and out2\n        out1, out2 = model2(ids, mask, pixels)\n        \n        #loss = criterion(out1, lab1) + criterion(out2, lab2)\n        loss1 = criterion(out1, lab1)\n        loss2 = criterion(out2, lab2)\n        loss = (0.5 * loss1) + (1.5 * loss2)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    \n  print(f\"Epoch {epoch+1} | Loss: {total_loss/len(train_loader):.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-28T14:49:48.713389Z","iopub.execute_input":"2026-02-28T14:49:48.713923Z","iopub.status.idle":"2026-02-28T14:56:15.484221Z","shell.execute_reply.started":"2026-02-28T14:49:48.713893Z","shell.execute_reply":"2026-02-28T14:56:15.483424Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17e2fb96160d46b7be1ad4e95675aa5c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"312357f1896d4bdb9a5c87a7c068cb93"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c2ba60ebfee94b6987ba2aa9403be029"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dffb28133c3349c4ba2a7d97d53c75eb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"149f709d574c4128a23ad26a99ed015f"}},"metadata":{}},{"name":"stderr","text":"Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"affd69d234db47c79005e3bbd17bf627"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/592 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2520aa227afd469bbd7de32f639037d2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eba76759d3634e37a213264035c49b1d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ebfcf13b348047199d9e82cb54b4d222"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e8dc976f0d24105bc67396c08b1d80a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c14c16f563b14f17bb20608303c2c534"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b16afea9f326433390cf3444e4f30825"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/605M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e32343a877d4237bd37f8ba5d582212"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/605M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"160f4f12443f4bcda2c9443c00b9f44e"}},"metadata":{}},{"name":"stdout","text":"Epoch 1 | Loss: 1.3112\nEpoch 2 | Loss: 0.3417\nEpoch 3 | Loss: 0.0776\nEpoch 4 | Loss: 0.0240\nEpoch 5 | Loss: 0.0178\nEpoch 6 | Loss: 0.0520\nEpoch 7 | Loss: 0.1897\nEpoch 8 | Loss: 0.1805\nEpoch 9 | Loss: 0.1951\nEpoch 10 | Loss: 0.0404\n","output_type":"stream"}],"execution_count":65},{"cell_type":"code","source":"from sklearn.metrics import classification_report, f1_score, precision_score, recall_score\nimport torch\nimport numpy as np\n\n# 1. Put model in evaluation mode\nmodel2.eval()\n\n# Storage for ground truth and predictions\nl1_true, l1_pred = [], []\nl2_true, l2_pred = [], []\n\nprint(\"🚀 Starting Evaluation on Malayalam Validation Set...\")\n\nwith torch.no_grad():\n    for batch in dev_loader:\n        # Move inputs to device\n        ids = batch['input_ids'].to(device)\n        mask = batch['attention_mask'].to(device)\n        pix = batch['pixel_values'].to(device)\n        \n        # Get true labels\n        y1 = batch['label1'].to(device)\n        y2 = batch['label2'].to(device)\n\n        # Forward pass (XLM-R + CLIP)\n        out1, out2 = model2(ids, mask, pix)\n        \n        # Get predicted class indices\n        _, p1 = torch.max(out1, 1)\n        _, p2 = torch.max(out2, 1)\n\n        # Store for report\n        l1_true.extend(y1.cpu().numpy())\n        l1_pred.extend(p1.cpu().numpy())\n        l2_true.extend(y2.cpu().numpy())\n        l2_pred.extend(p2.cpu().numpy())\n\n# 2. Define Category Names for clarity\n# Adjust these based on your specific Malayalam mapping\ntarget_names_l1 = ['SUPPORT', 'TROLL/ OPPOSE']\ntarget_names_l2 = ['person', 'party', 'Intersection']\n\n# 3. Generate and Print Reports\nprint(\"\\n\" + \"=\"*60)\nprint(\"📊 LEVEL 1: OVERALL SENTIMENT (Troll vs. Support)\")\nprint(\"=\"*60)\nprint(classification_report(l1_true, l1_pred, target_names=target_names_l1, digits=4))\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"📊 LEVEL 2: TARGET CLASSIFICATION (Person/Party/Intersection)\")\nprint(\"=\"*60)\n# Use labels=[0, 1, 2] to ensure all classes are included even if zero-support\nprint(classification_report(l2_true, l2_pred, target_names=target_names_l2, labels=[0, 1, 2], digits=4))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-28T14:57:42.680809Z","iopub.execute_input":"2026-02-28T14:57:42.681539Z","iopub.status.idle":"2026-02-28T14:57:44.781813Z","shell.execute_reply.started":"2026-02-28T14:57:42.681506Z","shell.execute_reply":"2026-02-28T14:57:44.781052Z"}},"outputs":[{"name":"stdout","text":"🚀 Starting Evaluation on Malayalam Validation Set...\n\n============================================================\n📊 LEVEL 1: OVERALL SENTIMENT (Troll vs. Support)\n============================================================\n               precision    recall  f1-score   support\n\n      SUPPORT     0.9796    0.9412    0.9600        51\nTROLL/ OPPOSE     0.9712    0.9902    0.9806       102\n\n     accuracy                         0.9739       153\n    macro avg     0.9754    0.9657    0.9703       153\n weighted avg     0.9740    0.9739    0.9737       153\n\n\n============================================================\n📊 LEVEL 2: TARGET CLASSIFICATION (Person/Party/Intersection)\n============================================================\n              precision    recall  f1-score   support\n\n      person     0.7639    0.9821    0.8594        56\n       party     1.0000    0.7778    0.8750        54\nIntersection     0.9744    0.8837    0.9268        43\n\n    accuracy                         0.8824       153\n   macro avg     0.9127    0.8812    0.8871       153\nweighted avg     0.9064    0.8824    0.8838       153\n\n","output_type":"stream"}],"execution_count":66},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom PIL import Image\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\n\nclass Test_Labeled_Dataset(Dataset):\n    def __init__(self, df, tokenizer, feature_extractor):\n        self.df = df.reset_index(drop=True)\n        self.tokenizer = tokenizer\n        self.feature_extractor = feature_extractor\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        text = str(self.df.loc[idx, 'text'])\n        image_arr = self.df.loc[idx, 'Image_name'] \n        l1 = self.df.loc[idx, 'label1']\n        l2 = self.df.loc[idx, 'label2']\n\n        encoding = self.tokenizer(text, padding='max_length', truncation=True, max_length=128, return_tensors='pt')\n        \n        # Image processing\n        image = Image.fromarray(image_arr.astype('uint8')).convert(\"RGB\")\n        pixel_values = self.feature_extractor(images=image, return_tensors='pt')['pixel_values'].squeeze(0)\n\n        # Return DICTIONARY\n        return {\n            'input_ids': encoding['input_ids'].squeeze(0),\n            'attention_mask': encoding['attention_mask'].squeeze(0),\n            'pixel_values': pixel_values,\n            'label1': torch.tensor(l1, dtype=torch.long),\n            'label2': torch.tensor(l2, dtype=torch.long)\n        }   \n\n# 1. Initialize Dataset and Loader\ntest_dataset_labeled = Test_Labeled_Dataset(merged_test_labeled, tokenizer, feature_extractor)\ntest_loader_labeled = DataLoader(test_dataset_labeled, batch_size=16)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-28T14:58:02.024232Z","iopub.execute_input":"2026-02-28T14:58:02.024815Z","iopub.status.idle":"2026-02-28T14:58:02.033313Z","shell.execute_reply.started":"2026-02-28T14:58:02.024784Z","shell.execute_reply":"2026-02-28T14:58:02.032581Z"}},"outputs":[],"execution_count":67},{"cell_type":"code","source":"from sklearn.metrics import classification_report, f1_score, precision_score, recall_score,confusion_matrix\nimport torch\nimport numpy as np\n\n# 1. Put model in evaluation mode\nmodel2.eval()\n\n# Storage for ground truth and predictions\nl1_true, l1_pred = [], []\nl2_true, l2_pred = [], []\n\nprint(\"🚀 Starting Evaluation on Malayalam test Set...\")\n\nwith torch.no_grad():\n    for batch in test_loader_labeled:\n        # Move inputs to device\n        ids = batch['input_ids'].to(device)\n        mask = batch['attention_mask'].to(device)\n        pix = batch['pixel_values'].to(device)\n        \n        # Get true labels\n        y1 = batch['label1'].to(device)\n        y2 = batch['label2'].to(device)\n\n        # Forward pass (XLM-R + CLIP)\n        out1, out2 = model2(ids, mask, pix)\n        \n        # Get predicted class indices\n        _, p1 = torch.max(out1, 1)\n        _, p2 = torch.max(out2, 1)\n\n        # Store for report\n        l1_true.extend(y1.cpu().numpy())\n        l1_pred.extend(p1.cpu().numpy())\n        l2_true.extend(y2.cpu().numpy())\n        l2_pred.extend(p2.cpu().numpy())\n\n# 2. Define Category Names for clarity\n# Adjust these based on your specific Malayalam mapping\ntarget_names_l1 = ['Support/Praise', 'TROLL/OPPOSE']\ntarget_names_l2 = ['person', 'party', 'Intersection']\n\n# 3. Generate and Print Reports\nprint(\"\\n\" + \"=\"*60)\nprint(\"📊 LEVEL 1: OVERALL SENTIMENT for Test Data (Troll vs. Support)\")\nprint(\"=\"*60)\nprint(classification_report(l1_true, l1_pred, target_names=target_names_l1, digits=4))\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"📊 LEVEL 2: TARGET CLASSIFICATION for Test Data (Person/Party/Intersection)\")\nprint(\"=\"*60)\n# Use labels=[0, 1, 2] to ensure all classes are included even if zero-support\nprint(classification_report(l2_true, l2_pred, target_names=target_names_l2, labels=[0, 1, 2], digits=4))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-28T14:58:12.890463Z","iopub.execute_input":"2026-02-28T14:58:12.890885Z","iopub.status.idle":"2026-02-28T14:58:14.342235Z","shell.execute_reply.started":"2026-02-28T14:58:12.890855Z","shell.execute_reply":"2026-02-28T14:58:14.341511Z"}},"outputs":[{"name":"stdout","text":"🚀 Starting Evaluation on Malayalam test Set...\n\n============================================================\n📊 LEVEL 1: OVERALL SENTIMENT for Test Data (Troll vs. Support)\n============================================================\n                precision    recall  f1-score   support\n\nSupport/Praise     0.0000    0.0000    0.0000         4\n  TROLL/OPPOSE     0.9600    1.0000    0.9796        96\n\n      accuracy                         0.9600       100\n     macro avg     0.4800    0.5000    0.4898       100\n  weighted avg     0.9216    0.9600    0.9404       100\n\n\n============================================================\n📊 LEVEL 2: TARGET CLASSIFICATION for Test Data (Person/Party/Intersection)\n============================================================\n              precision    recall  f1-score   support\n\n      person     0.5909    0.9630    0.7324        54\n       party     0.5556    0.1613    0.2500        31\nIntersection     0.3333    0.0667    0.1111        15\n\n    accuracy                         0.5800       100\n   macro avg     0.4933    0.3970    0.3645       100\nweighted avg     0.5413    0.5800    0.4897       100\n\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n","output_type":"stream"}],"execution_count":68},{"cell_type":"markdown","source":"# # ****Indic+Resnet","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import notebook_login\nnotebook_login() ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-28T14:59:35.198040Z","iopub.execute_input":"2026-02-28T14:59:35.198571Z","iopub.status.idle":"2026-02-28T14:59:35.218137Z","shell.execute_reply.started":"2026-02-28T14:59:35.198534Z","shell.execute_reply":"2026-02-28T14:59:35.217327Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de13ae2f0174477a8c29235a8285a04d"}},"metadata":{}}],"execution_count":69},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModel\nfrom torchvision import models, transforms\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\nimport torch.optim as optim\nfrom PIL import Image\n\n\nimage_transform = transforms.Compose([\n    transforms.Resize((224, 224)),  \n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\nclass MultiModalDataset(Dataset):\n    def __init__(self, df, tokenizer, image_transform):\n        self.df = df.reset_index(drop=True)\n        self.tokenizer = tokenizer\n        self.image_transform = image_transform\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        text = str(self.df.loc[idx, 'text'])\n        image_arr = self.df.loc[idx, 'Image_name'] \n        l1 = self.df.loc[idx, 'label1']\n        l2 = self.df.loc[idx, 'label2']\n\n        encoding = self.tokenizer(\n            text, padding='max_length', truncation=True, max_length=128, return_tensors='pt'\n        )\n\n        image = Image.fromarray(image_arr.astype('uint8')).convert(\"RGB\")\n        image = self.image_transform(image)\n\n        return {\n            'input_ids': encoding['input_ids'].squeeze(0),\n            'attention_mask': encoding['attention_mask'].squeeze(0),\n            'pixel_values': image,\n            'label1': torch.tensor(l1, dtype=torch.long),\n            'label2': torch.tensor(l2, dtype=torch.long)\n        }\n\n\nclass MultiModal(nn.Module):\n    def __init__(self, indicbert_model, resnet_model, num_l1, num_l2):\n        super(MultiModal, self).__init__()\n        self.indicbert_model = indicbert_model\n        self.resnet_model = resnet_model\n        \n        # 768 (IndicBERT) + 2048 (ResNet50) = 2816\n        self.classifier_l1 = nn.Linear(2816, num_l1)\n        self.classifier_l2 = nn.Linear(2816, num_l2)\n        self.dropout = nn.Dropout(0.3)\n\n    def forward(self, input_ids, attention_mask, pixel_values):\n        # Text features (Mean Pooling)\n        text_out = self.indicbert_model(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n        text_features = torch.mean(text_out, dim=1)\n\n        # Image features\n        vision_features = self.resnet_model(pixel_values)\n\n        # Fusion\n        combined = torch.cat((text_features, vision_features), dim=1)\n        combined = self.dropout(combined)\n        \n        return self.classifier_l1(combined), self.classifier_l2(combined)\n\n\ntokenizer = AutoTokenizer.from_pretrained('ai4bharat/indic-bert')\nindicbert_model = AutoModel.from_pretrained('ai4bharat/indic-bert')\n\nresnet_model = models.resnet50(pretrained=True)\nresnet_model.fc = nn.Identity() \n\nmodel3 = MultiModal(indicbert_model, resnet_model, num_l1=2, num_l2=3).to(device)\noptimizer = optim.AdamW(model3.parameters(), lr=2e-5) # Use AdamW and lower LR for BERT\ncriterion = nn.CrossEntropyLoss()\n\n# Loaders\ntrain_loader = DataLoader(MultiModalDataset(df_train, tokenizer, image_transform), batch_size=16, shuffle=True)\ndev_loader = DataLoader(MultiModalDataset(df_val, tokenizer, image_transform), batch_size=16)\n\nfor epoch in range(10):\n    model3.train()\n    total_loss = 0\n    for batch in train_loader:\n        ids, mask = batch['input_ids'].to(device), batch['attention_mask'].to(device)\n        pix = batch['pixel_values'].to(device)\n        lab1, lab2 = batch['label1'].to(device), batch['label2'].to(device)\n\n        optimizer.zero_grad()\n        out1, out2 = model3(ids, mask, pix)\n        \n        # Combine losses\n        loss = criterion(out1, lab1) + criterion(out2, lab2)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n\n    print(f\"Epoch {epoch+1} | Loss: {total_loss/len(train_loader):.4f}\")  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-28T14:59:49.565577Z","iopub.execute_input":"2026-02-28T14:59:49.566160Z","iopub.status.idle":"2026-02-28T15:05:21.204556Z","shell.execute_reply.started":"2026-02-28T14:59:49.566130Z","shell.execute_reply":"2026-02-28T15:05:21.203936Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/507 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f981da5e3ca4963a00b8e4d9e36259c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/5.65M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"59cfadb055e644f892fa83e34af683aa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/135M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40bfa85ea3504d28838bca164369251f"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n","output_type":"stream"},{"name":"stdout","text":"Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n","output_type":"stream"},{"name":"stderr","text":" 11%|█         | 10.9M/97.8M [00:00<00:00, 114MB/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/135M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"57b9c4fef47f491299a3aac200df099c"}},"metadata":{}},{"name":"stderr","text":"100%|██████████| 97.8M/97.8M [00:00<00:00, 201MB/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 | Loss: 1.3038\nEpoch 2 | Loss: 0.7649\nEpoch 3 | Loss: 0.4638\nEpoch 4 | Loss: 0.2642\nEpoch 5 | Loss: 0.1396\nEpoch 6 | Loss: 0.0867\nEpoch 7 | Loss: 0.0569\nEpoch 8 | Loss: 0.0570\nEpoch 9 | Loss: 0.0435\nEpoch 10 | Loss: 0.0319\n","output_type":"stream"}],"execution_count":70},{"cell_type":"code","source":"from sklearn.metrics import classification_report, f1_score, precision_score, recall_score\nimport torch\nimport numpy as np\n\n# 1. Put model in evaluation mode\nmodel3.eval()\n\n# Storage for ground truth and predictions\nl1_true, l1_pred = [], []\nl2_true, l2_pred = [], []\n\nprint(\"🚀 Starting Evaluation on Malayalam Validation Set...\")\n\nwith torch.no_grad():\n    for batch in dev_loader:\n        # Move inputs to device\n        ids = batch['input_ids'].to(device)\n        mask = batch['attention_mask'].to(device)\n        pix = batch['pixel_values'].to(device)\n        \n        # Get true labels\n        y1 = batch['label1'].to(device)\n        y2 = batch['label2'].to(device)\n\n        # Forward pass (XLM-R + CLIP)\n        out1, out2 = model3(ids, mask, pix)\n        \n        # Get predicted class indices\n        _, p1 = torch.max(out1, 1)\n        _, p2 = torch.max(out2, 1)\n\n        # Store for report\n        l1_true.extend(y1.cpu().numpy())\n        l1_pred.extend(p1.cpu().numpy())\n        l2_true.extend(y2.cpu().numpy())\n        l2_pred.extend(p2.cpu().numpy())\n\n# 2. Define Category Names for clarity\n# Adjust these based on your specific Malayalam mapping\ntarget_names_l1 = ['SUPPORT', 'TROLL/ OPPOSE']\ntarget_names_l2 = ['person', 'party', 'Intersection']\n\n# 3. Generate and Print Reports\nprint(\"\\n\" + \"=\"*60)\nprint(\"📊 LEVEL 1: OVERALL SENTIMENT (Troll vs. Support)\")\nprint(\"=\"*60)\nprint(classification_report(l1_true, l1_pred, target_names=target_names_l1, digits=4))\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"📊 LEVEL 2: TARGET CLASSIFICATION (Person/Party/Intersection)\")\nprint(\"=\"*60)\n# Use labels=[0, 1, 2] to ensure all classes are included even if zero-support\nprint(classification_report(l2_true, l2_pred, target_names=target_names_l2, labels=[0, 1, 2], digits=4))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-28T15:09:24.002006Z","iopub.execute_input":"2026-02-28T15:09:24.002601Z","iopub.status.idle":"2026-02-28T15:09:25.995134Z","shell.execute_reply.started":"2026-02-28T15:09:24.002570Z","shell.execute_reply":"2026-02-28T15:09:25.994419Z"}},"outputs":[{"name":"stdout","text":"🚀 Starting Evaluation on Malayalam Validation Set...\n\n============================================================\n📊 LEVEL 1: OVERALL SENTIMENT (Troll vs. Support)\n============================================================\n               precision    recall  f1-score   support\n\n      SUPPORT     1.0000    0.9608    0.9800        51\nTROLL/ OPPOSE     0.9808    1.0000    0.9903       102\n\n     accuracy                         0.9869       153\n    macro avg     0.9904    0.9804    0.9851       153\n weighted avg     0.9872    0.9869    0.9869       153\n\n\n============================================================\n📊 LEVEL 2: TARGET CLASSIFICATION (Person/Party/Intersection)\n============================================================\n              precision    recall  f1-score   support\n\n      person     0.7538    0.8750    0.8099        56\n       party     0.8723    0.7593    0.8119        54\nIntersection     0.9756    0.9302    0.9524        43\n\n    accuracy                         0.8497       153\n   macro avg     0.8673    0.8548    0.8581       153\nweighted avg     0.8580    0.8497    0.8506       153\n\n","output_type":"stream"}],"execution_count":71},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom PIL import Image\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\n\nclass Test_Labeled_Dataset(Dataset):\n    def __init__(self, df, tokenizer, feature_extractor):\n        self.df = df.reset_index(drop=True)\n        self.tokenizer = tokenizer\n        self.feature_extractor = feature_extractor\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        text = str(self.df.loc[idx, 'text'])\n        image_arr = self.df.loc[idx, 'Image_name'] \n        l1 = self.df.loc[idx, 'label1']\n        l2 = self.df.loc[idx, 'label2']\n\n        encoding = self.tokenizer(text, padding='max_length', truncation=True, max_length=128, return_tensors='pt')\n        \n        # Image processing\n        image = Image.fromarray(image_arr.astype('uint8')).convert(\"RGB\")\n        pixel_values = self.feature_extractor(images=image, return_tensors='pt')['pixel_values'].squeeze(0)\n\n        # Return DICTIONARY\n        return {\n            'input_ids': encoding['input_ids'].squeeze(0),\n            'attention_mask': encoding['attention_mask'].squeeze(0),\n            'pixel_values': pixel_values,\n            'label1': torch.tensor(l1, dtype=torch.long),\n            'label2': torch.tensor(l2, dtype=torch.long)\n        }   \n\n# 1. Initialize Dataset and Loader\ntest_dataset_labeled = Test_Labeled_Dataset(merged_test_labeled, tokenizer, feature_extractor)\ntest_loader_labeled = DataLoader(test_dataset_labeled, batch_size=16)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-28T15:09:40.122115Z","iopub.execute_input":"2026-02-28T15:09:40.122806Z","iopub.status.idle":"2026-02-28T15:09:40.130582Z","shell.execute_reply.started":"2026-02-28T15:09:40.122776Z","shell.execute_reply":"2026-02-28T15:09:40.129962Z"}},"outputs":[],"execution_count":72},{"cell_type":"code","source":"from sklearn.metrics import classification_report, f1_score, precision_score, recall_score,confusion_matrix\nimport torch\nimport numpy as np\n\n# 1. Put model in evaluation mode\nmodel3.eval()\n\n# Storage for ground truth and predictions\nl1_true, l1_pred = [], []\nl2_true, l2_pred = [], []\n\nprint(\"🚀 Starting Evaluation on Malayalam test Set...\")\n\nwith torch.no_grad():\n    for batch in test_loader_labeled:\n        # Move inputs to device\n        ids = batch['input_ids'].to(device)\n        mask = batch['attention_mask'].to(device)\n        pix = batch['pixel_values'].to(device)\n        \n        # Get true labels\n        y1 = batch['label1'].to(device)\n        y2 = batch['label2'].to(device)\n\n        # Forward pass (XLM-R + CLIP)\n        out1, out2 = model3(ids, mask, pix)\n        \n        # Get predicted class indices\n        _, p1 = torch.max(out1, 1)\n        _, p2 = torch.max(out2, 1)\n\n        # Store for report\n        l1_true.extend(y1.cpu().numpy())\n        l1_pred.extend(p1.cpu().numpy())\n        l2_true.extend(y2.cpu().numpy())\n        l2_pred.extend(p2.cpu().numpy())\n\n# 2. Define Category Names for clarity\n# Adjust these based on your specific Malayalam mapping\ntarget_names_l1 = ['Support/Praise', 'TROLL/OPPOSE']\ntarget_names_l2 = ['person', 'party', 'Intersection']\n\n# 3. Generate and Print Reports\nprint(\"\\n\" + \"=\"*60)\nprint(\"📊 LEVEL 1: OVERALL SENTIMENT for Test Data (Troll vs. Support)\")\nprint(\"=\"*60)\nprint(classification_report(l1_true, l1_pred, target_names=target_names_l1, digits=4))\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"📊 LEVEL 2: TARGET CLASSIFICATION for Test Data (Person/Party/Intersection)\")\nprint(\"=\"*60)\n# Use labels=[0, 1, 2] to ensure all classes are included even if zero-support\nprint(classification_report(l2_true, l2_pred, target_names=target_names_l2, labels=[0, 1, 2], digits=4))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-28T15:09:45.590785Z","iopub.execute_input":"2026-02-28T15:09:45.591067Z","iopub.status.idle":"2026-02-28T15:09:47.027582Z","shell.execute_reply.started":"2026-02-28T15:09:45.591043Z","shell.execute_reply":"2026-02-28T15:09:47.026817Z"}},"outputs":[{"name":"stdout","text":"🚀 Starting Evaluation on Malayalam test Set...\n\n============================================================\n📊 LEVEL 1: OVERALL SENTIMENT for Test Data (Troll vs. Support)\n============================================================\n                precision    recall  f1-score   support\n\nSupport/Praise     0.0000    0.0000    0.0000         4\n  TROLL/OPPOSE     0.9600    1.0000    0.9796        96\n\n      accuracy                         0.9600       100\n     macro avg     0.4800    0.5000    0.4898       100\n  weighted avg     0.9216    0.9600    0.9404       100\n\n\n============================================================\n📊 LEVEL 2: TARGET CLASSIFICATION for Test Data (Person/Party/Intersection)\n============================================================\n              precision    recall  f1-score   support\n\n      person     0.5618    0.9259    0.6993        54\n       party     0.4286    0.0968    0.1579        31\nIntersection     0.2500    0.0667    0.1053        15\n\n    accuracy                         0.5400       100\n   macro avg     0.4135    0.3631    0.3208       100\nweighted avg     0.4737    0.5400    0.4424       100\n\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n","output_type":"stream"}],"execution_count":73}]}